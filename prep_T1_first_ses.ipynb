{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-17T11:33:00.534487Z",
     "start_time": "2025-11-17T11:33:00.530489Z"
    }
   },
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T12:16:01.239726Z",
     "start_time": "2025-11-17T12:16:01.228735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# The CSV file you uploaded\n",
    "INPUT_CSV_FILE = 'data/final_merged_fmri_t1_clinical (2).csv'\n",
    "\n",
    "# The new Excel file we will create\n",
    "OUTPUT_EXCEL_FILE = 'data/T1_first_session_data.xlsx'\n",
    "\n",
    "# The one column you want to include in EVERY sheet\n",
    "# Based on your request, this is 'Subject_Code'\n",
    "KEY_COLUMN = 'Subject_Code'\n",
    "\n",
    "# VVV --- THIS IS THE PART YOU EDIT --- VVV\n",
    "#\n",
    "# Define the sheets you want to create.\n",
    "#\n",
    "# - The 'key' is the name you want for the sheet (e.g., 'Clinical_Scores').\n",
    "# - The 'value' is a list of all the *other* columns you want in that sheet.\n",
    "#\n",
    "# The KEY_COLUMN ('Subject_Code') will be added for you automatically.\n",
    "# I've added some examples based on your CSV's column names.\n",
    "#\n",
    "SHEETS_TO_CREATE = {\n",
    "    'global_parameters': [\n",
    "    \"brainsegvol\",\n",
    "    \"cerebralwhitemattervol\",\n",
    "    \"cortexvol\",\n",
    "    \"etiv\",\n",
    "    \"lhcortexvol\",\n",
    "    \"rhcortexvol\",\n",
    "    \"subcortgrayvol\",\n",
    "    \"totalgrayvol\",\n",
    "    \"meanthickness_lh\",\n",
    "    \"meanthickness_rh\",\n",
    "    \"whitesurfarea_lh\",\n",
    "    \"whitesurfarea_rh\"],\n",
    "\n",
    "    'subcortical_vol': [\n",
    "    \"3rd_ventricle\",\n",
    "    \"4th_ventricle\",\n",
    "    \"5th_ventricle\",\n",
    "    \"brain_stem\",\n",
    "    \"cc_anterior\",\n",
    "    \"cc_central\",\n",
    "    \"cc_mid_anterior\",\n",
    "    \"cc_mid_posterior\",\n",
    "    \"cc_posterior\",\n",
    "    \"csf\",\n",
    "    \"left_accumbens_area\",\n",
    "    \"left_amygdala\",\n",
    "    \"left_caudate\",\n",
    "    \"left_cerebellum_cortex\",\n",
    "    \"left_cerebellum_white_matter\",\n",
    "    \"left_choroid_plexus\",\n",
    "    \"left_hippocampus\",\n",
    "    \"left_inf_lat_vent\",\n",
    "    \"left_lateral_ventricle\",\n",
    "    \"left_non_wm_hypointensities\",\n",
    "    \"left_pallidum\",\n",
    "    \"left_putamen\",\n",
    "    \"left_thalamus\",\n",
    "    \"left_ventraldc\",\n",
    "    \"left_vessel\",\n",
    "    \"left_wm_hypointensities\",\n",
    "    \"non_wm_hypointensities\",\n",
    "    \"optic_chiasm\",\n",
    "    \"right_accumbens_area\",\n",
    "    \"right_amygdala\",\n",
    "    \"right_caudate\",\n",
    "    \"right_cerebellum_cortex\",\n",
    "    \"right_cerebellum_white_matter\",\n",
    "    \"right_choroid_plexus\",\n",
    "    \"right_hippocampus\",\n",
    "    \"right_inf_lat_vent\",\n",
    "    \"right_lateral_ventricle\",\n",
    "    \"right_non_wm_hypointensities\",\n",
    "    \"right_pallidum\",\n",
    "    \"right_putamen\",\n",
    "    \"right_thalamus\",\n",
    "    \"right_ventraldc\",\n",
    "    \"right_vessel\",\n",
    "    \"right_wm_hypointensities\",\n",
    "    \"wm_hypointensities\"\n",
    "],\n",
    "\n",
    "    \"cortical_vol_lr\": [\n",
    "    \"bankssts_lh\",\n",
    "    \"bankssts_rh\",\n",
    "    \"caudalanteriorcingulate_lh\",\n",
    "    \"caudalanteriorcingulate_rh\",\n",
    "    \"caudalmiddlefrontal_lh\",\n",
    "    \"caudalmiddlefrontal_rh\",\n",
    "    \"cuneus_lh\",\n",
    "    \"cuneus_rh\",\n",
    "    \"entorhinal_lh\",\n",
    "    \"entorhinal_rh\",\n",
    "    \"frontalpole_lh\",\n",
    "    \"frontalpole_rh\",\n",
    "    \"fusiform_lh\",\n",
    "    \"fusiform_rh\",\n",
    "    \"inferiorparietal_lh\",\n",
    "    \"inferiorparietal_rh\",\n",
    "    \"inferiortemporal_lh\",\n",
    "    \"inferiortemporal_rh\",\n",
    "    \"insula_lh\",\n",
    "    \"insula_rh\",\n",
    "    \"isthmuscingulate_lh\",\n",
    "    \"isthmuscingulate_rh\",\n",
    "    \"lateraloccipital_lh\",\n",
    "    \"lateraloccipital_rh\",\n",
    "    \"lateralorbitofrontal_lh\",\n",
    "    \"lateralorbitofrontal_rh\",\n",
    "    \"lingual_lh\",\n",
    "    \"lingual_rh\",\n",
    "    \"medialorbitofrontal_lh\",\n",
    "    \"medialorbitofrontal_rh\",\n",
    "    \"middletemporal_lh\",\n",
    "    \"middletemporal_rh\",\n",
    "    \"paracentral_lh\",\n",
    "    \"paracentral_rh\",\n",
    "    \"parahippocampal_lh\",\n",
    "    \"parahippocampal_rh\",\n",
    "    \"parsopercularis_lh\",\n",
    "    \"parsopercularis_rh\",\n",
    "    \"parsorbitalis_lh\",\n",
    "    \"parsorbitalis_rh\",\n",
    "    \"parstriangularis_lh\",\n",
    "    \"parstriangularis_rh\",\n",
    "    \"pericalcarine_lh\",\n",
    "    \"pericalcarine_rh\",\n",
    "    \"postcentral_lh\",\n",
    "    \"postcentral_rh\",\n",
    "    \"posteriorcingulate_lh\",\n",
    "    \"posteriorcingulate_rh\",\n",
    "    \"precentral_lh\",\n",
    "    \"precentral_rh\",\n",
    "    \"precuneus_lh\",\n",
    "    \"precuneus_rh\",\n",
    "    \"rostralanteriorcingulate_lh\",\n",
    "    \"rostralanteriorcingulate_rh\",\n",
    "    \"rostralmiddlefrontal_lh\",\n",
    "    \"rostralmiddlefrontal_rh\",\n",
    "    \"superiorfrontal_lh\",\n",
    "    \"superiorfrontal_rh\",\n",
    "    \"superiorparietal_lh\",\n",
    "    \"superiorparietal_rh\",\n",
    "    \"superiortemporal_lh\",\n",
    "    \"superiortemporal_rh\",\n",
    "    \"supramarginal_lh\",\n",
    "    \"supramarginal_rh\",\n",
    "    \"temporalpole_lh\",\n",
    "    \"temporalpole_rh\",\n",
    "    \"transversetemporal_lh\",\n",
    "    \"transversetemporal_rh\"\n",
    "]\n",
    ",\n",
    "    'cortical_lobes_lr': [\n",
    "    \"Cingulate_lh\",\n",
    "    \"Cingulate_rh\",\n",
    "    \"Frontal_lh\",\n",
    "    \"Frontal_rh\",\n",
    "    \"Insula_lh\",\n",
    "    \"Insula_rh\",\n",
    "    \"Occipital_lh\",\n",
    "    \"Occipital_rh\",\n",
    "    \"Parietal_lh\",\n",
    "    \"Parietal_rh\",\n",
    "    \"Temporal_lh\",\n",
    "    \"Temporal_rh\"\n",
    "],\n",
    "\n",
    "\n",
    "    'cortical_lobes_full_brain': [\n",
    "    \"cingulate\",\n",
    "    \"frontal\",\n",
    "    \"insula\",\n",
    "    \"occipital\",\n",
    "    \"parietal\",\n",
    "    \"temporal\"\n",
    "],\n",
    "    'cortical_networks_full_brain': [\n",
    "    \"DMN_GM\",\n",
    "    \"DA_GM\",\n",
    "    \"FPN_GM\",\n",
    "    \"LIM_GM\",\n",
    "    \"SMN_GM\",\n",
    "    \"VAN_GM\",\n",
    "    \"VIS_GM\"\n",
    "]\n",
    "\n",
    "}\n",
    "# ^^^ --- THIS IS THE PART YOU EDIT --- ^^^\n",
    "\n",
    "\n"
   ],
   "id": "4df746b29fcbe7a7",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T12:18:32.857244Z",
     "start_time": "2025-11-17T12:18:32.481973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Main function to read CSV and write to a multi-sheet Excel file.\n",
    "\"\"\"\n",
    "try:\n",
    "    # 1. Load the entire CSV into a pandas DataFrame\n",
    "    print(f\"Reading input file: {INPUT_CSV_FILE}...\")\n",
    "    main_df = pd.read_csv(INPUT_CSV_FILE)\n",
    "    print(\"CSV file loaded successfully.\")\n",
    "\n",
    "    # Build a case-insensitive mapping: lower_name -> original_name\n",
    "    col_map = {c.lower(): c for c in main_df.columns}\n",
    "\n",
    "    # 2. Check that the key column exists (case-insensitive)\n",
    "    key_lower = KEY_COLUMN.lower()\n",
    "    if key_lower not in col_map:\n",
    "        print(f\"Error: The key column '{KEY_COLUMN}' was not found in the CSV (case-insensitive search).\")\n",
    "        print(\"Please check the KEY_COLUMN variable.\")\n",
    "        print(f\"Available columns are: {main_df.columns.tolist()}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    key_actual = col_map[key_lower]  # actual column name in the DataFrame\n",
    "\n",
    "    # 3. Create an ExcelWriter object to handle writing to multiple sheets\n",
    "    with pd.ExcelWriter(OUTPUT_EXCEL_FILE, engine='openpyxl') as writer:\n",
    "        print(f\"Creating Excel file: {OUTPUT_EXCEL_FILE}...\")\n",
    "\n",
    "        # 4. Loop through your configuration\n",
    "        for sheet_name, column_list in SHEETS_TO_CREATE.items():\n",
    "\n",
    "            # 5. Build the final list of columns for this sheet\n",
    "            final_columns = []\n",
    "\n",
    "            # Check if key column is already in the list (case-insensitive)\n",
    "            column_list_lower = [c.lower() for c in column_list]\n",
    "            if key_lower not in column_list_lower:\n",
    "                final_columns.append(KEY_COLUMN)   # we'll resolve to real name below\n",
    "\n",
    "            final_columns.extend(column_list)\n",
    "\n",
    "            # 6. Check for missing columns and create the sheet DataFrame\n",
    "            valid_columns = []\n",
    "            seen_lowers = set()  # to avoid duplicates with different cases\n",
    "\n",
    "            for col in final_columns:\n",
    "                col_lower = col.lower()\n",
    "                if col_lower in seen_lowers:\n",
    "                    continue  # skip duplicates\n",
    "                if col_lower in col_map:\n",
    "                    valid_columns.append(col_map[col_lower])  # use actual name from CSV\n",
    "                    seen_lowers.add(col_lower)\n",
    "                else:\n",
    "                    print(f\"  - Warning: Column '{col}' (for sheet '{sheet_name}') \"\n",
    "                          f\"not found in CSV (case-insensitive). It will be skipped.\")\n",
    "\n",
    "            if not valid_columns:\n",
    "                print(f\"  - Error: No valid columns found for sheet '{sheet_name}'. \"\n",
    "                      f\"This sheet will be empty or skipped.\")\n",
    "                continue\n",
    "\n",
    "            # 7. Create the new DataFrame for this sheet\n",
    "            sheet_df = main_df[valid_columns]\n",
    "\n",
    "            # 8. Write this DataFrame to the Excel file\n",
    "            sheet_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "            print(f\"  -> Successfully created sheet: '{sheet_name}' with {len(valid_columns)} columns.\")\n",
    "\n",
    "    print(f\"\\nAll done! Your file '{OUTPUT_EXCEL_FILE}' has been created.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{INPUT_CSV_FILE}' was not found.\")\n",
    "    print(\"Please make sure the CSV file is in the same directory as this script.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n"
   ],
   "id": "9bb88a6c8a6961c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading input file: data/final_merged_fmri_t1_clinical (2).csv...\n",
      "CSV file loaded successfully.\n",
      "Creating Excel file: data/T1_first_session_data.xlsx...\n",
      "  - Warning: Column 'brainsegvol' (for sheet 'global_parameters') not found in CSV (case-insensitive). It will be skipped.\n",
      "  -> Successfully created sheet: 'global_parameters' with 12 columns.\n",
      "  - Warning: Column '3rd_ventricle' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column '4th_ventricle' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column '5th_ventricle' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'brain_stem' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'cc_anterior' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'cc_central' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'cc_mid_anterior' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'cc_mid_posterior' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'cc_posterior' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'left_accumbens_area' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'left_amygdala' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'left_caudate' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'left_cerebellum_cortex' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'left_cerebellum_white_matter' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'left_choroid_plexus' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'left_hippocampus' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'left_inf_lat_vent' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'left_lateral_ventricle' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'left_non_wm_hypointensities' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'left_pallidum' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'left_putamen' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'left_thalamus' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'left_ventraldc' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'left_vessel' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'left_wm_hypointensities' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'non_wm_hypointensities' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'optic_chiasm' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'right_accumbens_area' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'right_amygdala' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'right_caudate' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'right_cerebellum_cortex' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'right_cerebellum_white_matter' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'right_choroid_plexus' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'right_hippocampus' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'right_inf_lat_vent' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'right_lateral_ventricle' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'right_non_wm_hypointensities' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'right_pallidum' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'right_putamen' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'right_thalamus' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'right_ventraldc' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'right_vessel' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'right_wm_hypointensities' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  - Warning: Column 'wm_hypointensities' (for sheet 'subcortical_vol') not found in CSV (case-insensitive). It will be skipped.\n",
      "  -> Successfully created sheet: 'subcortical_vol' with 2 columns.\n",
      "  -> Successfully created sheet: 'cortical_vol_lr' with 69 columns.\n",
      "  -> Successfully created sheet: 'cortical_lobes_lr' with 13 columns.\n",
      "  -> Successfully created sheet: 'cortical_lobes_full_brain' with 7 columns.\n",
      "  - Warning: Column 'DA_GM' (for sheet 'cortical_networks_full_brain') not found in CSV (case-insensitive). It will be skipped.\n",
      "  -> Successfully created sheet: 'cortical_networks_full_brain' with 7 columns.\n",
      "\n",
      "All done! Your file 'data/T1_first_session_data.xlsx' has been created.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T09:22:18.397131Z",
     "start_time": "2025-11-07T09:22:18.392638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def collect_files(folder: Path, recursive: bool = False):\n",
    "    if recursive:\n",
    "        return [p for p in folder.rglob(\"*.xlsx\") if p.is_file()]\n",
    "    return [p for p in folder.iterdir() if p.is_file() and p.suffix.lower() == \".xlsx\"]\n"
   ],
   "id": "55c78a624065a74a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T09:22:18.418121Z",
     "start_time": "2025-11-07T09:22:18.413743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def list_excel_files(folder: Path, recursive: bool = False):\n",
    "    if recursive:\n",
    "        return sorted([p for p in folder.rglob(\"*.xlsx\") if p.is_file()])\n",
    "    return sorted([p for p in folder.iterdir() if p.is_file() and p.suffix.lower() == \".xlsx\"])\n",
    "\n"
   ],
   "id": "e05dd3622b1dc561",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T09:22:18.435991Z",
     "start_time": "2025-11-07T09:22:18.431197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pick_column(df: pd.DataFrame, candidates):\n",
    "    \"\"\"Return the first matching column name in df (case-insensitive), else None.\"\"\"\n",
    "    cols_map = {c.lower(): c for c in df.columns}\n",
    "    for c in candidates:\n",
    "        if c.lower() in cols_map:\n",
    "            return cols_map[c.lower()]\n",
    "    # try partial matches (e.g., \"grayvol\" inside \"grayvol (mm3)\")\n",
    "    for c in candidates:\n",
    "        for k, v in cols_map.items():\n",
    "            if c.lower() in k:\n",
    "                return v\n",
    "    return None\n"
   ],
   "id": "31d03ed7dd7ebe61",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T09:22:18.455601Z",
     "start_time": "2025-11-07T09:22:18.451080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def slugify(name: str) -> str:\n",
    "    \"\"\"lowercase + replace non-alnum with underscores + collapse repeats.\"\"\"\n",
    "    s = re.sub(r\"\\W+\", \"_\", str(name).strip().lower())\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s"
   ],
   "id": "676783dba8aeee02",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T09:22:18.473131Z",
     "start_time": "2025-11-07T09:22:18.467652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a stable, pretty column order:\n",
    "#   subject_code first, then all lh/rh ses1 followed by ses2 (alphabetical by struct)\n",
    "def col_sort_key(c):\n",
    "    # subject_code stays first\n",
    "    if c == \"subject_code\":\n",
    "        return (0, \"\", \"\", 0)\n",
    "    # parse pattern: <struct>_<hemi>_ses<1|2>\n",
    "    m = re.match(r\"^(.*)_(lh|rh)_ses([12])$\", c)\n",
    "    if m:\n",
    "        struct, hemi, ses = m.group(1), m.group(2), int(m.group(3))\n",
    "        return (1, struct, hemi, ses)\n",
    "    # anything unexpected goes to the end\n",
    "    return (2, c, \"\", 99)"
   ],
   "id": "5c6c0b609fdc9208",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T09:22:18.489417Z",
     "start_time": "2025-11-07T09:22:18.483746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def merge_two_cols(dframe, cols, how=\"sum\"):\n",
    "    # cols is a list of existing columns (1 or 2). We want:\n",
    "    # - NaN if both missing\n",
    "    # - sum/mean ignoring NaNs otherwise\n",
    "    cols = [c for c in cols if c is not None and c in dframe.columns]\n",
    "    if not cols:\n",
    "        return pd.Series(np.nan, index=dframe.index)\n",
    "    if how == \"mean\":\n",
    "        return dframe[cols].mean(axis=1)\n",
    "    # default sum\n",
    "    return dframe[cols].sum(axis=1, min_count=1)\n"
   ],
   "id": "6a08d1d12b64940a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T09:22:18.507996Z",
     "start_time": "2025-11-07T09:22:18.503147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def reduce_cols(dframe, cols, how=\"sum\"):\n",
    "    cols = [c for c in cols if c in dframe.columns]\n",
    "    if not cols:\n",
    "        return pd.Series(np.nan, index=dframe.index)\n",
    "    return dframe[cols].sum(axis=1, min_count=1)"
   ],
   "id": "67116c40bc109a7b",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T09:22:18.531168Z",
     "start_time": "2025-11-07T09:22:18.524995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 4) Order columns nicely: subject_code, then lobe alpha, ses1 before ses2, lh before rh\n",
    "def order_key_lobe(c):\n",
    "    if c == \"subject_code\":\n",
    "        return (0, \"\", 0, \"\")\n",
    "    m = re.match(r\"^(?P<lobe>.+)_(?P<hemi>lh|rh)_ses(?P<ses>\\d+)$\", c)\n",
    "    if m:\n",
    "        lobe = m.group(\"lobe\")\n",
    "        hemi = m.group(\"hemi\")\n",
    "        ses  = int(m.group(\"ses\"))\n",
    "        hemi_order = 0 if hemi == \"lh\" else 1\n",
    "        return (1, lobe, ses, hemi_order)\n",
    "    return (2, c, 99, 99)\n"
   ],
   "id": "7ead2c8a9b7d15a7",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T09:22:18.543756Z",
     "start_time": "2025-11-07T09:22:18.538716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _read_csv(path: Path, merge_key: str) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        print(f\"WARNING: file not found -> {path}\")\n",
    "        return pd.DataFrame(columns=[merge_key])\n",
    "    df = pd.read_csv(path)\n",
    "    # normalize key dtype for safe merging/sorting\n",
    "    if merge_key in df.columns:\n",
    "        df[merge_key] = df[merge_key].astype(str)\n",
    "    return df\n",
    "\n"
   ],
   "id": "160933e0e621478c",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T09:22:18.559442Z",
     "start_time": "2025-11-07T09:22:18.554891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _order_columns_subject_first(df: pd.DataFrame, key: str = \"subject_code\") -> pd.DataFrame:\n",
    "    cols = list(df.columns)\n",
    "    if key in cols:\n",
    "        return df[[key] + [c for c in cols if c != key]]\n",
    "    return df"
   ],
   "id": "8f58dbcb089ecddc",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T09:22:18.574797Z",
     "start_time": "2025-11-07T09:22:18.569284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _sort_consistent(df: pd.DataFrame, key: str = \"subject_code\") -> pd.DataFrame:\n",
    "    \"\"\"Alphabetical by key; if numbers exist, use them as a secondary natural sort.\"\"\"\n",
    "    if key not in df.columns:\n",
    "        return df\n",
    "    key_series = df[key].astype(str)\n",
    "    # Extract first number if present; fall back to NaN\n",
    "    num = key_series.str.extract(r\"(\\d+)\")[0].astype(float)\n",
    "    # Primary: alphabetical by key; Secondary: numeric ascending (NaN last)\n",
    "    return (df.assign(_num=num)\n",
    "              .sort_values(by=[key, \"_num\"], kind=\"mergesort\")\n",
    "              .drop(columns=\"_num\"))\n"
   ],
   "id": "b46bdc5d167286b8",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T09:22:18.588832Z",
     "start_time": "2025-11-07T09:22:18.583339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _autosize_columns(writer: pd.ExcelWriter, df: pd.DataFrame, sheet_name: str, max_width: int = 60):\n",
    "    \"\"\"Best-effort autosize; harmless no-op if engine doesn't support.\"\"\"\n",
    "    try:\n",
    "        ws = writer.sheets[sheet_name]\n",
    "        for idx, col in enumerate(df.columns, 1):\n",
    "            s = df[col].astype(str)\n",
    "            longest = max([len(str(col))] + [len(x) for x in s.tolist()] + [10])\n",
    "            ws.column_dimensions[ws.cell(row=1, column=idx).column_letter].width = min(longest + 2, max_width)\n",
    "    except Exception:\n",
    "        pass"
   ],
   "id": "35b75b6006f850c0",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T09:22:18.609838Z",
     "start_time": "2025-11-07T09:22:18.605908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "records = []\n",
    "sheet_names_ref = None\n",
    "# --- CONFIG --- #\n",
    "FOLDER = Path(\"longitude_stats_without_normalizations\")  # change to your folder\n",
    "SUMMARY_CSV = Path(\"longitude_stats_without_normalizations/2_sess_T1_summary.csv\")  # change if you want\n",
    "RECURSIVE = False  # set True if files are in subfolders\n",
    "# --------------- #\n"
   ],
   "id": "f8574c0b5df94989",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T09:22:18.907762Z",
     "start_time": "2025-11-07T09:22:18.616383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Matches: 002_NT_ses-1_merged.xlsx  or  015_CT_ses-2_merged.xlsx  (case-insensitive)\n",
    "file_regex = re.compile(\n",
    "    r\"(?P<subject>\\d{3})_(?P<group>CT|NT)_ses-(?P<session>[12])_merged\\.xlsx$\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "for path in sorted(collect_files(FOLDER, RECURSIVE)):\n",
    "    m = file_regex.match(path.name)\n",
    "    if not m:\n",
    "        continue\n",
    "\n",
    "    subject = m.group(\"subject\")\n",
    "    group = m.group(\"group\").upper()\n",
    "    subject_code = f\"{group}{subject}\"\n",
    "\n",
    "    # Capture sheet names from the first valid workbook\n",
    "    if sheet_names_ref is None:\n",
    "        try:\n",
    "            xls = pd.ExcelFile(path)\n",
    "            sheet_names_ref = list(xls.sheet_names)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: failed to read sheet names from '{path}': {e}\")\n",
    "            sheet_names_ref = []\n"
   ],
   "id": "15b68716e117d83b",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T09:22:18.951058Z",
     "start_time": "2025-11-07T09:22:18.918840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# If we didn't find any matching files, still create a CSV with just the first two columns\n",
    "if sheet_names_ref is None:\n",
    "    sheet_names_ref = []\n",
    "\n",
    "# Build dataframe with empty columns for each sheet name\n",
    "columns = [\"subject_code\"] + sheet_names_ref\n",
    "df = pd.DataFrame(records, columns=columns)\n",
    "\n",
    "# Ensure those sheet columns are empty strings (not NaN)\n",
    "for s in sheet_names_ref:\n",
    "    df[s] = \"\"\n",
    "\n",
    "# Nice sorting: group (CT/NT) + numeric subject + session\n",
    "if not df.empty:\n",
    "    df[\"_grp\"] = df[\"subject_code\"].str.extract(r\"^(CT|NT)\")\n",
    "    df[\"_num\"] = df[\"subject_code\"].str.extract(r\"(\\d{3})\").astype(int)\n",
    "    df = df.sort_values(by=[\"_grp\", \"_num\"]).drop(columns=[\"_grp\", \"_num\"]).reset_index(drop=True)\n",
    "\n",
    "# Save CSV\n",
    "SUMMARY_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(SUMMARY_CSV, index=False)\n",
    "\n",
    "print(f\"CSV written to: {SUMMARY_CSV}\")\n",
    "display(df.head(10))"
   ],
   "id": "6a66b9ccec727c48",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV written to: longitude_stats_without_normalizations\\2_sess_T1_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  subject_code  session aparc_summary aparc_voxels aseg_summary aseg_voxels  \\\n",
       "0        CT003        1                                                       \n",
       "1        CT003        2                                                       \n",
       "2        CT007        1                                                       \n",
       "3        CT007        2                                                       \n",
       "4        CT010        1                                                       \n",
       "5        CT010        2                                                       \n",
       "6        CT012        1                                                       \n",
       "7        CT012        2                                                       \n",
       "8        CT013        1                                                       \n",
       "9        CT013        2                                                       \n",
       "\n",
       "  wmparc_summary wmparc_voxels  \n",
       "0                               \n",
       "1                               \n",
       "2                               \n",
       "3                               \n",
       "4                               \n",
       "5                               \n",
       "6                               \n",
       "7                               \n",
       "8                               \n",
       "9                               "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_code</th>\n",
       "      <th>session</th>\n",
       "      <th>aparc_summary</th>\n",
       "      <th>aparc_voxels</th>\n",
       "      <th>aseg_summary</th>\n",
       "      <th>aseg_voxels</th>\n",
       "      <th>wmparc_summary</th>\n",
       "      <th>wmparc_voxels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CT003</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CT003</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CT007</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CT007</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CT010</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CT010</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CT012</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CT012</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CT013</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CT013</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T09:22:22.109410Z",
     "start_time": "2025-11-07T09:22:18.975905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ---------- CONFIG ----------\n",
    "OUT_CSV = Path(\"longitude_stats_without_normalizations/2_sessions_aparc_voxels_lh_rh.csv\")  # output file\n",
    "SHEET_NAME = \"aparc_voxels\"  # sheet to read\n",
    "RECURSIVE = False            # set True if files are in subfolders\n",
    "# ----------------------------\n",
    "\n",
    "\n",
    "# Accumulate rows keyed by subject_code\n",
    "rows_by_subject = {}\n",
    "all_feature_cols = set()\n",
    "\n",
    "files = list_excel_files(FOLDER, RECURSIVE)\n",
    "for path in files:\n",
    "    m = file_regex.match(path.name)\n",
    "    if not m:\n",
    "        continue\n",
    "\n",
    "    subject = m.group(\"subject\")\n",
    "    group = m.group(\"group\").upper()\n",
    "    session = int(m.group(\"session\"))\n",
    "\n",
    "    subject_code = f\"{group}{subject}\"\n",
    "\n",
    "    try:\n",
    "        df = pd.read_excel(path, sheet_name=SHEET_NAME)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: couldn't read sheet '{SHEET_NAME}' in {path.name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Locate columns (robust to naming/case variants)\n",
    "    col_struct = pick_column(df, [\"StructName\"])\n",
    "    col_hemi   = pick_column(df, [\"Hemisphere\"])\n",
    "    col_gray   = pick_column(df, [\"GrayVol\"])\n",
    "\n",
    "    if not all([col_struct, col_hemi, col_gray]):\n",
    "        print(f\"Warning: missing needed columns in {path.name}. \"\n",
    "              f\"Found StructName = {col_struct}, Hemisphere = {col_hemi}, GrayVol = {col_gray}\")\n",
    "        continue\n",
    "\n",
    "    # Initialize subject row if needed\n",
    "    if subject_code not in rows_by_subject:\n",
    "        rows_by_subject[subject_code] = {\"subject_code\": subject_code}\n",
    "\n",
    "    # Build columns like insula_lh_ses1 with values from grayvol\n",
    "    for _, row in df.iterrows():\n",
    "        struct = slugify(row[col_struct])\n",
    "        hemi = slugify(row[col_hemi])  # expects 'lh'/'rh'\n",
    "        # Normalize common hemisphere values to 'lh'/'rh'\n",
    "        if hemi in (\"left\", \"l\"): hemi = \"lh\"\n",
    "        if hemi in (\"right\", \"r\"): hemi = \"rh\"\n",
    "\n",
    "        # skip rows missing essentials\n",
    "        if not struct or hemi not in (\"lh\", \"rh\"):\n",
    "            continue\n",
    "\n",
    "        col_name = f\"{struct}_{hemi}_ses{session}\"\n",
    "        val = row[col_gray]\n",
    "        # Coerce to numeric (keep NaN if not coercible)\n",
    "        try:\n",
    "            val = pd.to_numeric(val)\n",
    "        except Exception:\n",
    "            val = np.nan\n",
    "\n",
    "        rows_by_subject[subject_code][col_name] = val\n",
    "        all_feature_cols.add(col_name)\n",
    "    subject_code = f\"{group}{subject}\"      # e.g., CT002\n",
    "    mark_extraction_complete(subject_code, sheet_name=\"aparc_voxels\")# e.g., CT002 / NT015\n",
    "\n",
    "\n",
    "all_columns = [\"subject_code\"] + sorted(all_feature_cols, key=col_sort_key)\n",
    "\n",
    "# Build DataFrame\n",
    "df_out = pd.DataFrame(rows_by_subject.values())\n",
    "# Ensure all expected columns are present\n",
    "for c in all_columns:\n",
    "    if c not in df_out.columns:\n",
    "        df_out[c] = np.nan\n",
    "\n",
    "# Reorder and keep subject_code first\n",
    "df_out = df_out[all_columns]\n",
    "\n",
    "# Save\n",
    "OUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_out.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "print(f\"Saved: {OUT_CSV}\")\n",
    "display(df_out.head(5))"
   ],
   "id": "9f7b0a03850ec10b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: longitude_stats_without_normalizations\\2_sessions_aparc_voxels_lh_rh.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  subject_code  bankssts_lh_ses1  bankssts_lh_ses2  bankssts_rh_ses1  \\\n",
       "0        NT002              2670              2582              1960   \n",
       "1        CT003              2713              2673              2701   \n",
       "2        NT005              2788              2703              2196   \n",
       "3        CT007              1986              2015              2242   \n",
       "4        CT010              2542              2543              2294   \n",
       "\n",
       "   bankssts_rh_ses2  caudalanteriorcingulate_lh_ses1  \\\n",
       "0              1965                             1423   \n",
       "1              2613                             1627   \n",
       "2              2144                             1074   \n",
       "3              2177                             1744   \n",
       "4              2257                             1463   \n",
       "\n",
       "   caudalanteriorcingulate_lh_ses2  caudalanteriorcingulate_rh_ses1  \\\n",
       "0                             1397                             2060   \n",
       "1                             1683                             2014   \n",
       "2                             1025                             2215   \n",
       "3                             1785                             2951   \n",
       "4                             1418                             2369   \n",
       "\n",
       "   caudalanteriorcingulate_rh_ses2  caudalmiddlefrontal_lh_ses1  ...  \\\n",
       "0                             1979                         5850  ...   \n",
       "1                             2015                         7493  ...   \n",
       "2                             2176                         4278  ...   \n",
       "3                             3048                         5069  ...   \n",
       "4                             2361                         5473  ...   \n",
       "\n",
       "   supramarginal_rh_ses1  supramarginal_rh_ses2  temporalpole_lh_ses1  \\\n",
       "0                  10638                  10174                  2716   \n",
       "1                  10018                   9780                  3372   \n",
       "2                  10830                   9942                  2332   \n",
       "3                   9359                   9219                  2506   \n",
       "4                   7021                   7075                  2707   \n",
       "\n",
       "   temporalpole_lh_ses2  temporalpole_rh_ses1  temporalpole_rh_ses2  \\\n",
       "0                  2393                  2528                  2486   \n",
       "1                  3350                  3198                  3063   \n",
       "2                  2312                  2576                  2633   \n",
       "3                  2852                  2247                  2490   \n",
       "4                  2665                  2348                  2295   \n",
       "\n",
       "   transversetemporal_lh_ses1  transversetemporal_lh_ses2  \\\n",
       "0                        1047                        1018   \n",
       "1                        1012                         983   \n",
       "2                        1243                        1216   \n",
       "3                        1272                        1227   \n",
       "4                        1038                        1032   \n",
       "\n",
       "   transversetemporal_rh_ses1  transversetemporal_rh_ses2  \n",
       "0                         861                         823  \n",
       "1                         769                         757  \n",
       "2                         881                         850  \n",
       "3                         899                         929  \n",
       "4                         811                         800  \n",
       "\n",
       "[5 rows x 137 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_code</th>\n",
       "      <th>bankssts_lh_ses1</th>\n",
       "      <th>bankssts_lh_ses2</th>\n",
       "      <th>bankssts_rh_ses1</th>\n",
       "      <th>bankssts_rh_ses2</th>\n",
       "      <th>caudalanteriorcingulate_lh_ses1</th>\n",
       "      <th>caudalanteriorcingulate_lh_ses2</th>\n",
       "      <th>caudalanteriorcingulate_rh_ses1</th>\n",
       "      <th>caudalanteriorcingulate_rh_ses2</th>\n",
       "      <th>caudalmiddlefrontal_lh_ses1</th>\n",
       "      <th>...</th>\n",
       "      <th>supramarginal_rh_ses1</th>\n",
       "      <th>supramarginal_rh_ses2</th>\n",
       "      <th>temporalpole_lh_ses1</th>\n",
       "      <th>temporalpole_lh_ses2</th>\n",
       "      <th>temporalpole_rh_ses1</th>\n",
       "      <th>temporalpole_rh_ses2</th>\n",
       "      <th>transversetemporal_lh_ses1</th>\n",
       "      <th>transversetemporal_lh_ses2</th>\n",
       "      <th>transversetemporal_rh_ses1</th>\n",
       "      <th>transversetemporal_rh_ses2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NT002</td>\n",
       "      <td>2670</td>\n",
       "      <td>2582</td>\n",
       "      <td>1960</td>\n",
       "      <td>1965</td>\n",
       "      <td>1423</td>\n",
       "      <td>1397</td>\n",
       "      <td>2060</td>\n",
       "      <td>1979</td>\n",
       "      <td>5850</td>\n",
       "      <td>...</td>\n",
       "      <td>10638</td>\n",
       "      <td>10174</td>\n",
       "      <td>2716</td>\n",
       "      <td>2393</td>\n",
       "      <td>2528</td>\n",
       "      <td>2486</td>\n",
       "      <td>1047</td>\n",
       "      <td>1018</td>\n",
       "      <td>861</td>\n",
       "      <td>823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CT003</td>\n",
       "      <td>2713</td>\n",
       "      <td>2673</td>\n",
       "      <td>2701</td>\n",
       "      <td>2613</td>\n",
       "      <td>1627</td>\n",
       "      <td>1683</td>\n",
       "      <td>2014</td>\n",
       "      <td>2015</td>\n",
       "      <td>7493</td>\n",
       "      <td>...</td>\n",
       "      <td>10018</td>\n",
       "      <td>9780</td>\n",
       "      <td>3372</td>\n",
       "      <td>3350</td>\n",
       "      <td>3198</td>\n",
       "      <td>3063</td>\n",
       "      <td>1012</td>\n",
       "      <td>983</td>\n",
       "      <td>769</td>\n",
       "      <td>757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NT005</td>\n",
       "      <td>2788</td>\n",
       "      <td>2703</td>\n",
       "      <td>2196</td>\n",
       "      <td>2144</td>\n",
       "      <td>1074</td>\n",
       "      <td>1025</td>\n",
       "      <td>2215</td>\n",
       "      <td>2176</td>\n",
       "      <td>4278</td>\n",
       "      <td>...</td>\n",
       "      <td>10830</td>\n",
       "      <td>9942</td>\n",
       "      <td>2332</td>\n",
       "      <td>2312</td>\n",
       "      <td>2576</td>\n",
       "      <td>2633</td>\n",
       "      <td>1243</td>\n",
       "      <td>1216</td>\n",
       "      <td>881</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CT007</td>\n",
       "      <td>1986</td>\n",
       "      <td>2015</td>\n",
       "      <td>2242</td>\n",
       "      <td>2177</td>\n",
       "      <td>1744</td>\n",
       "      <td>1785</td>\n",
       "      <td>2951</td>\n",
       "      <td>3048</td>\n",
       "      <td>5069</td>\n",
       "      <td>...</td>\n",
       "      <td>9359</td>\n",
       "      <td>9219</td>\n",
       "      <td>2506</td>\n",
       "      <td>2852</td>\n",
       "      <td>2247</td>\n",
       "      <td>2490</td>\n",
       "      <td>1272</td>\n",
       "      <td>1227</td>\n",
       "      <td>899</td>\n",
       "      <td>929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CT010</td>\n",
       "      <td>2542</td>\n",
       "      <td>2543</td>\n",
       "      <td>2294</td>\n",
       "      <td>2257</td>\n",
       "      <td>1463</td>\n",
       "      <td>1418</td>\n",
       "      <td>2369</td>\n",
       "      <td>2361</td>\n",
       "      <td>5473</td>\n",
       "      <td>...</td>\n",
       "      <td>7021</td>\n",
       "      <td>7075</td>\n",
       "      <td>2707</td>\n",
       "      <td>2665</td>\n",
       "      <td>2348</td>\n",
       "      <td>2295</td>\n",
       "      <td>1038</td>\n",
       "      <td>1032</td>\n",
       "      <td>811</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  137 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T09:22:25.290144Z",
     "start_time": "2025-11-07T09:22:22.134847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "FOLDER = Path(\"longitude_stats_without_normalizations\")  # change to your folder\n",
    "OUT_CSV = Path(\"longitude_stats_without_normalizations/2_sessions_aseg_data.csv\")  # output file\n",
    "SHEET_NAME = \"aseg_voxels\"  # <-- aseg sheet\n",
    "RECURSIVE = False            # set True if files are in subfolders\n",
    "# ----------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Accumulate rows keyed by subject_code\n",
    "rows_by_subject = {}\n",
    "all_feature_cols = set()\n",
    "\n",
    "files = list_excel_files(FOLDER, RECURSIVE)\n",
    "for path in files:\n",
    "    m = file_regex.match(path.name)\n",
    "    if not m:\n",
    "        continue\n",
    "\n",
    "    subject = m.group(\"subject\")\n",
    "    group = m.group(\"group\").upper()\n",
    "    session = int(m.group(\"session\"))\n",
    "\n",
    "    subject_code = f\"{group}{subject}\"  # e.g., CT002 / NT015\n",
    "    try:\n",
    "        df = pd.read_excel(path, sheet_name=SHEET_NAME)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: couldn't read sheet '{SHEET_NAME}' in {path.name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Locate columns (robust to naming/case variants)\n",
    "    col_struct = pick_column(df, [\"StructName\"])\n",
    "    col_gray   = pick_column(df, [\"Volume_mm3\"])\n",
    "\n",
    "    if not all([col_struct, col_gray]):\n",
    "        print(f\"Warning: missing needed columns in {path.name}. \"\n",
    "              f\"Found struct={col_struct}, vol={col_gray}\")\n",
    "        continue\n",
    "\n",
    "    # Initialize subject row if needed\n",
    "    if subject_code not in rows_by_subject:\n",
    "        rows_by_subject[subject_code] = {\"subject_code\": subject_code}\n",
    "\n",
    "    # Build columns like <struct>_ses1 with values from grayvol\n",
    "    for _, row in df.iterrows():\n",
    "        struct = slugify(row[col_struct])\n",
    "        if not struct:\n",
    "            continue\n",
    "\n",
    "        col_name = f\"{struct}_ses{session}\"\n",
    "        val = row[col_gray]\n",
    "        try:\n",
    "            val = pd.to_numeric(val)\n",
    "        except Exception:\n",
    "            val = np.nan\n",
    "\n",
    "        rows_by_subject[subject_code][col_name] = val\n",
    "        all_feature_cols.add(col_name)\n",
    "    subject_code = f\"{group}{subject}\"      # e.g., CT002\n",
    "    session = int(m.group(\"session\"))       # 1 or 2\n",
    "    mark_extraction_complete(subject_code, session, sheet_name=\"aseg_voxels\")# e.g., CT002 / NT015\n",
    "\n",
    "\n",
    "all_columns = [\"subject_code\"] + sorted(all_feature_cols, key=col_sort_key)\n",
    "\n",
    "# Build DataFrame\n",
    "df_out = pd.DataFrame(rows_by_subject.values())\n",
    "for c in all_columns:\n",
    "    if c not in df_out.columns:\n",
    "        df_out[c] = np.nan\n",
    "df_out = df_out[all_columns]\n",
    "\n",
    "# Save\n",
    "OUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_out.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "print(f\"Saved: {OUT_CSV}\")\n",
    "display(df_out.head(5))\n"
   ],
   "id": "b48bc609c7367e87",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: longitude_stats_without_normalizations\\2_sessions_aseg_data.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  subject_code  3rd_ventricle_ses1  3rd_ventricle_ses2  4th_ventricle_ses1  \\\n",
       "0        NT002               922.3               869.1              1157.7   \n",
       "1        CT003               867.7               894.9              1899.4   \n",
       "2        NT005               847.5               859.8              1535.5   \n",
       "3        CT007               792.9               813.4              2136.5   \n",
       "4        CT010              1063.8              1008.2              2666.3   \n",
       "\n",
       "   4th_ventricle_ses2  5th_ventricle_ses1  5th_ventricle_ses2  \\\n",
       "0              1066.4                 0.0                 0.0   \n",
       "1              1816.2                 4.5                 2.7   \n",
       "2              1714.9                26.1                19.2   \n",
       "3              2081.8                10.6                 9.7   \n",
       "4              2634.2                 0.0                 0.0   \n",
       "\n",
       "   brain_stem_ses1  brain_stem_ses2  cc_anterior_ses1  ...  \\\n",
       "0          19800.5          20048.8             672.9  ...   \n",
       "1          20241.3          20001.1             829.5  ...   \n",
       "2          20201.7          19714.8            1036.0  ...   \n",
       "3          22519.3          22830.1            1030.1  ...   \n",
       "4          22630.1          22641.8            1047.6  ...   \n",
       "\n",
       "   right_thalamus_ses1  right_thalamus_ses2  right_ventraldc_ses1  \\\n",
       "0               7168.8               7047.9                4098.9   \n",
       "1               7814.8               7764.3                3942.3   \n",
       "2               7178.5               6962.1                3766.0   \n",
       "3               8271.4               8394.8                4074.8   \n",
       "4               8394.3               8186.3                4193.2   \n",
       "\n",
       "   right_ventraldc_ses2  right_vessel_ses1  right_vessel_ses2  \\\n",
       "0                4175.0               69.3               69.5   \n",
       "1                3987.4              131.3              122.0   \n",
       "2                3540.1               63.0               82.7   \n",
       "3                4190.7               82.8              100.4   \n",
       "4                4172.2               79.4               70.1   \n",
       "\n",
       "   right_wm_hypointensities_ses1  right_wm_hypointensities_ses2  \\\n",
       "0                            0.0                            0.0   \n",
       "1                            0.0                            0.0   \n",
       "2                            0.0                            0.0   \n",
       "3                            0.0                            0.0   \n",
       "4                            0.0                            0.0   \n",
       "\n",
       "   wm_hypointensities_ses1  wm_hypointensities_ses2  \n",
       "0                    667.7                    609.2  \n",
       "1                    944.5                    852.4  \n",
       "2                    360.6                    305.4  \n",
       "3                   1351.4                    975.4  \n",
       "4                    311.7                    377.4  \n",
       "\n",
       "[5 rows x 91 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_code</th>\n",
       "      <th>3rd_ventricle_ses1</th>\n",
       "      <th>3rd_ventricle_ses2</th>\n",
       "      <th>4th_ventricle_ses1</th>\n",
       "      <th>4th_ventricle_ses2</th>\n",
       "      <th>5th_ventricle_ses1</th>\n",
       "      <th>5th_ventricle_ses2</th>\n",
       "      <th>brain_stem_ses1</th>\n",
       "      <th>brain_stem_ses2</th>\n",
       "      <th>cc_anterior_ses1</th>\n",
       "      <th>...</th>\n",
       "      <th>right_thalamus_ses1</th>\n",
       "      <th>right_thalamus_ses2</th>\n",
       "      <th>right_ventraldc_ses1</th>\n",
       "      <th>right_ventraldc_ses2</th>\n",
       "      <th>right_vessel_ses1</th>\n",
       "      <th>right_vessel_ses2</th>\n",
       "      <th>right_wm_hypointensities_ses1</th>\n",
       "      <th>right_wm_hypointensities_ses2</th>\n",
       "      <th>wm_hypointensities_ses1</th>\n",
       "      <th>wm_hypointensities_ses2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NT002</td>\n",
       "      <td>922.3</td>\n",
       "      <td>869.1</td>\n",
       "      <td>1157.7</td>\n",
       "      <td>1066.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19800.5</td>\n",
       "      <td>20048.8</td>\n",
       "      <td>672.9</td>\n",
       "      <td>...</td>\n",
       "      <td>7168.8</td>\n",
       "      <td>7047.9</td>\n",
       "      <td>4098.9</td>\n",
       "      <td>4175.0</td>\n",
       "      <td>69.3</td>\n",
       "      <td>69.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>667.7</td>\n",
       "      <td>609.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CT003</td>\n",
       "      <td>867.7</td>\n",
       "      <td>894.9</td>\n",
       "      <td>1899.4</td>\n",
       "      <td>1816.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2.7</td>\n",
       "      <td>20241.3</td>\n",
       "      <td>20001.1</td>\n",
       "      <td>829.5</td>\n",
       "      <td>...</td>\n",
       "      <td>7814.8</td>\n",
       "      <td>7764.3</td>\n",
       "      <td>3942.3</td>\n",
       "      <td>3987.4</td>\n",
       "      <td>131.3</td>\n",
       "      <td>122.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>944.5</td>\n",
       "      <td>852.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NT005</td>\n",
       "      <td>847.5</td>\n",
       "      <td>859.8</td>\n",
       "      <td>1535.5</td>\n",
       "      <td>1714.9</td>\n",
       "      <td>26.1</td>\n",
       "      <td>19.2</td>\n",
       "      <td>20201.7</td>\n",
       "      <td>19714.8</td>\n",
       "      <td>1036.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7178.5</td>\n",
       "      <td>6962.1</td>\n",
       "      <td>3766.0</td>\n",
       "      <td>3540.1</td>\n",
       "      <td>63.0</td>\n",
       "      <td>82.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>360.6</td>\n",
       "      <td>305.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CT007</td>\n",
       "      <td>792.9</td>\n",
       "      <td>813.4</td>\n",
       "      <td>2136.5</td>\n",
       "      <td>2081.8</td>\n",
       "      <td>10.6</td>\n",
       "      <td>9.7</td>\n",
       "      <td>22519.3</td>\n",
       "      <td>22830.1</td>\n",
       "      <td>1030.1</td>\n",
       "      <td>...</td>\n",
       "      <td>8271.4</td>\n",
       "      <td>8394.8</td>\n",
       "      <td>4074.8</td>\n",
       "      <td>4190.7</td>\n",
       "      <td>82.8</td>\n",
       "      <td>100.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1351.4</td>\n",
       "      <td>975.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CT010</td>\n",
       "      <td>1063.8</td>\n",
       "      <td>1008.2</td>\n",
       "      <td>2666.3</td>\n",
       "      <td>2634.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22630.1</td>\n",
       "      <td>22641.8</td>\n",
       "      <td>1047.6</td>\n",
       "      <td>...</td>\n",
       "      <td>8394.3</td>\n",
       "      <td>8186.3</td>\n",
       "      <td>4193.2</td>\n",
       "      <td>4172.2</td>\n",
       "      <td>79.4</td>\n",
       "      <td>70.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>311.7</td>\n",
       "      <td>377.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  91 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "sdewew\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "FOLDER = Path(\"longitude_stats_without_normalizations\")  # change to your folder\n",
    "OUT_CSV = Path(\"longitude_stats_without_normalizations/2_sessions_wmparc_data.csv\")  # output file\n",
    "SHEET_NAME = \"wmparc_voxels\"  # <-- aseg sheet\n",
    "RECURSIVE = False            # set True if files are in subfolders\n",
    "# ----------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Accumulate rows keyed by subject_code\n",
    "rows_by_subject = {}\n",
    "all_feature_cols = set()\n",
    "\n",
    "files = list_excel_files(FOLDER, RECURSIVE)\n",
    "for path in files:\n",
    "    m = file_regex.match(path.name)\n",
    "    if not m:\n",
    "        continue\n",
    "\n",
    "    subject = m.group(\"subject\")\n",
    "    group = m.group(\"group\").upper()\n",
    "    session = int(m.group(\"session\"))\n",
    "\n",
    "    subject_code = f\"{group}{subject}\"  # e.g., CT002 / NT015\n",
    "    try:\n",
    "        df = pd.read_excel(path, sheet_name=SHEET_NAME)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: couldn't read sheet '{SHEET_NAME}' in {path.name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Locate columns (robust to naming/case variants)\n",
    "    col_struct = pick_column(df, [\"StructName\"])\n",
    "    col_gray   = pick_column(df, [\"Volume_mm3\"])\n",
    "\n",
    "    if not all([col_struct, col_gray]):\n",
    "        print(f\"Warning: missing needed columns in {path.name}. \"\n",
    "              f\"Found struct={col_struct}, vol={col_gray}\")\n",
    "        continue\n",
    "\n",
    "    # Initialize subject row if needed\n",
    "    if subject_code not in rows_by_subject:\n",
    "        rows_by_subject[subject_code] = {\"subject_code\": subject_code}\n",
    "\n",
    "    # Build columns like <struct>_ses1 with values from grayvol\n",
    "    for _, row in df.iterrows():\n",
    "        struct = slugify(row[col_struct])\n",
    "        if not struct:\n",
    "            continue\n",
    "\n",
    "        col_name = f\"{struct}_ses{session}\"\n",
    "        val = row[col_gray]\n",
    "        try:\n",
    "            val = pd.to_numeric(val)\n",
    "        except Exception:\n",
    "            val = np.nan\n",
    "\n",
    "        rows_by_subject[subject_code][col_name] = val\n",
    "        all_feature_cols.add(col_name)\n",
    "    subject_code = f\"{group}{subject}\"      # e.g., CT002\n",
    "    session = int(m.group(\"session\"))       # 1 or 2\n",
    "    mark_extraction_complete(subject_code, session, sheet_name=\"wmparc_voxels\")# e.g., CT002 / NT015\n",
    "\n",
    "\n",
    "all_columns = [\"subject_code\"] + sorted(all_feature_cols, key=col_sort_key)\n",
    "\n",
    "# Build DataFrame\n",
    "df_out = pd.DataFrame(rows_by_subject.values())\n",
    "for c in all_columns:\n",
    "    if c not in df_out.columns:\n",
    "        df_out[c] = np.nan\n",
    "df_out = df_out[all_columns]\n",
    "\n",
    "# Save\n",
    "OUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_out.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "print(f\"Saved: {OUT_CSV}\")\n",
    "display(df_out.head(5))\n"
   ],
   "id": "643a85a61579b726"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T09:22:31.291078Z",
     "start_time": "2025-11-07T09:22:28.831767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Build per-subject table from aparc_summary, including hemisphere in column names.\n",
    "# Requires helpers already defined: file_regex, list_excel_files, slugify, pick_column, col_sort_key, mark_extraction_complete\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "FOLDER = Path(\"longitude_stats_without_normalizations\")\n",
    "OUT_CSV = Path(\"longitude_stats_without_normalizations/2_sessions_aparc_summary_data.csv\")\n",
    "SHEET_NAME = \"aparc_summary\"\n",
    "\n",
    "SHORTNAME_COLUMN_CANDIDATES = [\"ShortName\"]\n",
    "VALUE_COLUMN_CANDIDATES     = [\"Value\"]\n",
    "HEMI_COLUMN_CANDIDATES      = [\"Hemisphere\"]\n",
    "\n",
    "# Keep ONLY these ShortName rows (exact matches as they appear in the sheet):\n",
    "SHORTNAMES_TO_KEEP = [\"WhiteSurfArea\",\"MeanThickness\"]\n",
    "    # e.g. \"insula\", \"precentral\", \"bankssts\", ...\n",
    "\n",
    "\n",
    "# Optional renames for output columns (after slugify)\n",
    "RENAMES = {}\n",
    "# ----------------------------\n",
    "\n",
    "rows_by_subject = {}\n",
    "all_feature_cols = set()\n",
    "\n",
    "files = list_excel_files(FOLDER, recursive=False)\n",
    "for path in files:\n",
    "    m = file_regex.match(path.name)\n",
    "    if not m:\n",
    "        continue\n",
    "\n",
    "    subject = m.group(\"subject\")\n",
    "    group = m.group(\"group\").upper()\n",
    "    session = int(m.group(\"session\"))\n",
    "    subject_code = f\"{group}{subject}\"\n",
    "\n",
    "    try:\n",
    "        df = pd.read_excel(path, sheet_name=SHEET_NAME)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: couldn't read sheet '{SHEET_NAME}' in {path.name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    col_short = pick_column(df, SHORTNAME_COLUMN_CANDIDATES)\n",
    "    col_val   = pick_column(df, VALUE_COLUMN_CANDIDATES)\n",
    "    col_hemi  = pick_column(df, HEMI_COLUMN_CANDIDATES)\n",
    "\n",
    "    if not all([col_short, col_val, col_hemi]):\n",
    "        print(f\"Warning: missing needed columns in {path.name}. \"\n",
    "              f\"Found ShortName={col_short}, Value={col_val}, Hemisphere={col_hemi}\")\n",
    "        continue\n",
    "\n",
    "    if subject_code not in rows_by_subject:\n",
    "        rows_by_subject[subject_code] = {\"subject_code\": subject_code}\n",
    "\n",
    "    # Filter only desired ShortNames (exact match on the raw values)\n",
    "    sub = df[df[col_short].astype(str).isin(SHORTNAMES_TO_KEEP)].copy()\n",
    "\n",
    "    for _, r in sub.iterrows():\n",
    "        raw_short = str(r[col_short])\n",
    "        hemi_raw  = str(r[col_hemi]).strip().lower()\n",
    "\n",
    "        # normalize hemisphere to lh/rh\n",
    "        if hemi_raw in (\"left\", \"l\"):  hemi = \"lh\"\n",
    "        elif hemi_raw in (\"right\", \"r\"): hemi = \"rh\"\n",
    "        elif hemi_raw in (\"lh\", \"rh\"): hemi = hemi_raw\n",
    "        else:\n",
    "            # skip rows without a valid hemisphere\n",
    "            continue\n",
    "\n",
    "        key = slugify(raw_short)\n",
    "        key = RENAMES.get(key, key)\n",
    "\n",
    "        col_name = f\"{key}_{hemi}_ses{session}\"\n",
    "\n",
    "        # coerce Value to numeric\n",
    "        val = pd.to_numeric(r[col_val], errors=\"coerce\")\n",
    "\n",
    "        rows_by_subject[subject_code][col_name] = val\n",
    "        all_feature_cols.add(col_name)\n",
    "\n",
    "    # mark summary done for this subject/session/sheet\n",
    "    mark_extraction_complete(subject_code, session, sheet_name=SHEET_NAME)\n",
    "\n",
    "# finalize dataframe\n",
    "all_columns = [\"subject_code\"] + sorted(all_feature_cols, key=col_sort_key)\n",
    "df_out = pd.DataFrame(rows_by_subject.values())\n",
    "for c in all_columns:\n",
    "    if c not in df_out.columns:\n",
    "        df_out[c] = np.nan\n",
    "df_out = df_out[all_columns]\n",
    "\n",
    "OUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_out.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "print(f\"Saved: {OUT_CSV}\")\n",
    "display(df_out.head(10))\n"
   ],
   "id": "111881f9a7fe6603",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: longitude_stats_without_normalizations\\2_sessions_aparc_summary_data.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  subject_code  meanthickness_lh_ses1  meanthickness_lh_ses2  \\\n",
       "0        NT002                2.57432                2.49454   \n",
       "1        CT003                2.63823                2.62996   \n",
       "2        NT005                2.65105                2.58844   \n",
       "3        CT007                2.48926                2.54471   \n",
       "4        CT010                2.59687                2.55895   \n",
       "5        CT012                2.67354                2.65185   \n",
       "6        CT013                2.60889                2.61030   \n",
       "7        NT015                2.61509                2.60183   \n",
       "8        NT016                2.61350                2.58100   \n",
       "9        NT017                2.55446                2.51326   \n",
       "\n",
       "   meanthickness_rh_ses1  meanthickness_rh_ses2  whitesurfarea_lh_ses1  \\\n",
       "0                2.51656                2.44400                84456.7   \n",
       "1                2.65913                2.62355                90104.7   \n",
       "2                2.62182                2.58579                79621.9   \n",
       "3                2.51000                2.54911                87803.7   \n",
       "4                2.57176                2.53221                83175.4   \n",
       "5                2.66016                2.65112                83156.0   \n",
       "6                2.61542                2.58230                89042.4   \n",
       "7                2.61451                2.57076                78292.3   \n",
       "8                2.62777                2.61197                86003.8   \n",
       "9                2.59662                2.52521                86593.2   \n",
       "\n",
       "   whitesurfarea_lh_ses2  whitesurfarea_rh_ses1  whitesurfarea_rh_ses2  \n",
       "0                83960.6                85798.8                84895.4  \n",
       "1                89147.2                88973.8                88186.2  \n",
       "2                76358.0                78921.6                76241.0  \n",
       "3                87857.1                87253.2                86583.0  \n",
       "4                82461.1                82291.4                82149.6  \n",
       "5                82023.9                81731.3                81021.9  \n",
       "6                86343.2                88615.8                85778.4  \n",
       "7                78024.0                77973.0                78038.7  \n",
       "8                83855.6                85369.8                83788.8  \n",
       "9                82282.6                85102.8                80878.6  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_code</th>\n",
       "      <th>meanthickness_lh_ses1</th>\n",
       "      <th>meanthickness_lh_ses2</th>\n",
       "      <th>meanthickness_rh_ses1</th>\n",
       "      <th>meanthickness_rh_ses2</th>\n",
       "      <th>whitesurfarea_lh_ses1</th>\n",
       "      <th>whitesurfarea_lh_ses2</th>\n",
       "      <th>whitesurfarea_rh_ses1</th>\n",
       "      <th>whitesurfarea_rh_ses2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NT002</td>\n",
       "      <td>2.57432</td>\n",
       "      <td>2.49454</td>\n",
       "      <td>2.51656</td>\n",
       "      <td>2.44400</td>\n",
       "      <td>84456.7</td>\n",
       "      <td>83960.6</td>\n",
       "      <td>85798.8</td>\n",
       "      <td>84895.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CT003</td>\n",
       "      <td>2.63823</td>\n",
       "      <td>2.62996</td>\n",
       "      <td>2.65913</td>\n",
       "      <td>2.62355</td>\n",
       "      <td>90104.7</td>\n",
       "      <td>89147.2</td>\n",
       "      <td>88973.8</td>\n",
       "      <td>88186.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NT005</td>\n",
       "      <td>2.65105</td>\n",
       "      <td>2.58844</td>\n",
       "      <td>2.62182</td>\n",
       "      <td>2.58579</td>\n",
       "      <td>79621.9</td>\n",
       "      <td>76358.0</td>\n",
       "      <td>78921.6</td>\n",
       "      <td>76241.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CT007</td>\n",
       "      <td>2.48926</td>\n",
       "      <td>2.54471</td>\n",
       "      <td>2.51000</td>\n",
       "      <td>2.54911</td>\n",
       "      <td>87803.7</td>\n",
       "      <td>87857.1</td>\n",
       "      <td>87253.2</td>\n",
       "      <td>86583.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CT010</td>\n",
       "      <td>2.59687</td>\n",
       "      <td>2.55895</td>\n",
       "      <td>2.57176</td>\n",
       "      <td>2.53221</td>\n",
       "      <td>83175.4</td>\n",
       "      <td>82461.1</td>\n",
       "      <td>82291.4</td>\n",
       "      <td>82149.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CT012</td>\n",
       "      <td>2.67354</td>\n",
       "      <td>2.65185</td>\n",
       "      <td>2.66016</td>\n",
       "      <td>2.65112</td>\n",
       "      <td>83156.0</td>\n",
       "      <td>82023.9</td>\n",
       "      <td>81731.3</td>\n",
       "      <td>81021.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CT013</td>\n",
       "      <td>2.60889</td>\n",
       "      <td>2.61030</td>\n",
       "      <td>2.61542</td>\n",
       "      <td>2.58230</td>\n",
       "      <td>89042.4</td>\n",
       "      <td>86343.2</td>\n",
       "      <td>88615.8</td>\n",
       "      <td>85778.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NT015</td>\n",
       "      <td>2.61509</td>\n",
       "      <td>2.60183</td>\n",
       "      <td>2.61451</td>\n",
       "      <td>2.57076</td>\n",
       "      <td>78292.3</td>\n",
       "      <td>78024.0</td>\n",
       "      <td>77973.0</td>\n",
       "      <td>78038.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NT016</td>\n",
       "      <td>2.61350</td>\n",
       "      <td>2.58100</td>\n",
       "      <td>2.62777</td>\n",
       "      <td>2.61197</td>\n",
       "      <td>86003.8</td>\n",
       "      <td>83855.6</td>\n",
       "      <td>85369.8</td>\n",
       "      <td>83788.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NT017</td>\n",
       "      <td>2.55446</td>\n",
       "      <td>2.51326</td>\n",
       "      <td>2.59662</td>\n",
       "      <td>2.52521</td>\n",
       "      <td>86593.2</td>\n",
       "      <td>82282.6</td>\n",
       "      <td>85102.8</td>\n",
       "      <td>80878.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T09:22:33.765808Z",
     "start_time": "2025-11-07T09:22:31.310462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "FOLDER = Path(\"longitude_stats_without_normalizations\")\n",
    "OUT_CSV = Path(\"longitude_stats_without_normalizations/2_sessions_aseg_summary_data.csv\")\n",
    "SHEET_NAME = \"aseg_summary\"   # e.g., \"some_metrics_sheet\"\n",
    "\n",
    "SHORTNAME_COLUMN_CANDIDATES = [\"ShortName\"]\n",
    "VALUE_COLUMN_CANDIDATES     = [\"Value\"]\n",
    "\n",
    "# Keep ONLY these ShortName rows (exact matches as they appear in the sheet):\n",
    "SHORTNAMES_TO_KEEP = [\"eTIV\", \"BrainSegVol\",\"lhCortexVol\",\"rhCortexVol\",\"CortexVol\",\"CerebralWhiteMatterVol\",'SubCortGrayVol',\"TotalGrayVol\"]\n",
    "\n",
    "rows_by_subject = {}\n",
    "all_feature_cols = set()\n",
    "\n",
    "files = list_excel_files(FOLDER, recursive=False)\n",
    "for path in files:\n",
    "    m = file_regex.match(path.name)\n",
    "    if not m:\n",
    "        continue\n",
    "\n",
    "    subject = m.group(\"subject\")\n",
    "    group = m.group(\"group\").upper()\n",
    "    session = int(m.group(\"session\"))\n",
    "    subject_code = f\"{group}{subject}\"\n",
    "\n",
    "    try:\n",
    "        df = pd.read_excel(path, sheet_name=SHEET_NAME)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: couldn't read sheet '{SHEET_NAME}' in {path.name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    col_short = pick_column(df, SHORTNAME_COLUMN_CANDIDATES)\n",
    "    col_val   = pick_column(df, VALUE_COLUMN_CANDIDATES)\n",
    "    if not all([col_short, col_val]):\n",
    "        print(f\"Warning: missing needed columns in {path.name}. Found ShortName={col_short}, Value={col_val}\")\n",
    "        continue\n",
    "\n",
    "    if subject_code not in rows_by_subject:\n",
    "        rows_by_subject[subject_code] = {\"subject_code\": subject_code}\n",
    "\n",
    "    # Filter rows to the desired ShortNames\n",
    "    sub = df[df[col_short].astype(str).isin(SHORTNAMES_TO_KEEP)].copy()\n",
    "\n",
    "    for _, r in sub.iterrows():\n",
    "        raw_short = str(r[col_short])\n",
    "        key = slugify(raw_short)\n",
    "        key = RENAMES.get(key, key)  # optional rename\n",
    "        col_name = f\"{key}_ses{session}\"\n",
    "\n",
    "        # coerce Value to numeric\n",
    "        val = pd.to_numeric(r[col_val], errors=\"coerce\")\n",
    "\n",
    "        rows_by_subject[subject_code][col_name] = val\n",
    "        all_feature_cols.add(col_name)\n",
    "\n",
    "    # mark summary done for this subject/session/sheet\n",
    "    mark_extraction_complete(subject_code, session, sheet_name=SHEET_NAME)\n",
    "\n",
    "# finalize dataframe\n",
    "all_columns = [\"subject_code\"] + sorted(all_feature_cols, key=col_sort_key)\n",
    "df_out = pd.DataFrame(rows_by_subject.values())\n",
    "for c in all_columns:\n",
    "    if c not in df_out.columns:\n",
    "        df_out[c] = np.nan\n",
    "df_out = df_out[all_columns]\n",
    "\n",
    "OUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_out.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "print(f\"Saved: {OUT_CSV}\")\n",
    "display(df_out.head(10))\n"
   ],
   "id": "7bd1c7e2c9225a71",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: longitude_stats_without_normalizations\\2_sessions_aseg_summary_data.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  subject_code  brainsegvol_ses1  brainsegvol_ses2  \\\n",
       "0        NT002         1110228.0         1088457.0   \n",
       "1        CT003         1184745.0         1168965.0   \n",
       "2        NT005         1110667.0         1067155.0   \n",
       "3        CT007         1138151.0         1149703.0   \n",
       "4        CT010         1122091.0         1115262.0   \n",
       "5        CT012         1107827.0         1091554.0   \n",
       "6        CT013         1131618.0         1085518.0   \n",
       "7        NT015         1037379.0         1038595.0   \n",
       "8        NT016         1143170.0         1110370.0   \n",
       "9        NT017         1153453.0         1078295.0   \n",
       "\n",
       "   cerebralwhitemattervol_ses1  cerebralwhitemattervol_ses2  cortexvol_ses1  \\\n",
       "0                     419457.0                     416588.0   490608.236876   \n",
       "1                     458291.0                     456157.0   531916.401250   \n",
       "2                     423652.0                     406878.0   471502.137855   \n",
       "3                     421712.0                     425634.0   489541.405657   \n",
       "4                     408536.0                     412789.0   474689.564570   \n",
       "5                     391761.0                     389121.0   497944.071008   \n",
       "6                     400730.0                     382679.0   524760.488220   \n",
       "7                     377454.0                     386570.0   458988.573462   \n",
       "8                     405225.0                     394230.0   504111.022004   \n",
       "9                     436138.0                     408880.0   497094.953205   \n",
       "\n",
       "   cortexvol_ses2     etiv_ses1     etiv_ses2  lhcortexvol_ses1  \\\n",
       "0   471911.310674  1.422479e+06  1.422479e+06     245669.103788   \n",
       "1   519861.644142  1.503435e+06  1.503435e+06     266407.973214   \n",
       "2   443915.716838  1.415292e+06  1.415292e+06     237566.663194   \n",
       "3   496968.257306  1.431403e+06  1.431403e+06     243929.482988   \n",
       "4   466056.025773  1.439088e+06  1.439088e+06     239267.805546   \n",
       "5   486970.812681  1.453701e+06  1.453701e+06     251339.320238   \n",
       "6   498031.891560  1.419949e+06  1.419949e+06     262316.073827   \n",
       "7   453851.341338  1.331148e+06  1.331148e+06     229765.717092   \n",
       "8   484988.825810  1.267231e+06  1.267231e+06     252366.166819   \n",
       "9   450596.126581  1.456538e+06  1.456538e+06     248767.239049   \n",
       "\n",
       "   lhcortexvol_ses2  rhcortexvol_ses1  rhcortexvol_ses2  subcortgrayvol_ses1  \\\n",
       "0     237397.422763     244939.133088     234513.887911              58014.0   \n",
       "1     260860.911186     265508.428036     259000.732956              59295.0   \n",
       "2     222745.097278     233935.474661     221170.619560              56161.0   \n",
       "3     249904.719304     245611.922669     247063.538002              63658.0   \n",
       "4     234389.921100     235421.759024     231666.104673              59937.0   \n",
       "5     244323.770697     246604.750770     242647.041984              62131.0   \n",
       "6     250895.428303     262444.414393     247136.463257              62748.0   \n",
       "7     228380.735128     229222.856370     225470.606209              57936.0   \n",
       "8     241518.612710     251744.855185     243470.213100              59494.0   \n",
       "9     227148.873162     248327.714156     223447.253419              57825.0   \n",
       "\n",
       "   subcortgrayvol_ses2  totalgrayvol_ses1  totalgrayvol_ses2  \n",
       "0              57402.0      655161.236876      636146.310674  \n",
       "1              58931.0      689043.401250      675766.644142  \n",
       "2              54908.0      641680.137855      613641.716838  \n",
       "3              64002.0      673969.405657      681836.257306  \n",
       "4              59739.0      657339.564570      648058.025773  \n",
       "5              61648.0      668019.071008      654484.812681  \n",
       "6              60772.0      692313.488220      666431.891560  \n",
       "7              57469.0      625411.573462      617026.341338  \n",
       "8              57989.0      694629.022004      672629.825810  \n",
       "9              55245.0      664121.953205      616935.126581  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_code</th>\n",
       "      <th>brainsegvol_ses1</th>\n",
       "      <th>brainsegvol_ses2</th>\n",
       "      <th>cerebralwhitemattervol_ses1</th>\n",
       "      <th>cerebralwhitemattervol_ses2</th>\n",
       "      <th>cortexvol_ses1</th>\n",
       "      <th>cortexvol_ses2</th>\n",
       "      <th>etiv_ses1</th>\n",
       "      <th>etiv_ses2</th>\n",
       "      <th>lhcortexvol_ses1</th>\n",
       "      <th>lhcortexvol_ses2</th>\n",
       "      <th>rhcortexvol_ses1</th>\n",
       "      <th>rhcortexvol_ses2</th>\n",
       "      <th>subcortgrayvol_ses1</th>\n",
       "      <th>subcortgrayvol_ses2</th>\n",
       "      <th>totalgrayvol_ses1</th>\n",
       "      <th>totalgrayvol_ses2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NT002</td>\n",
       "      <td>1110228.0</td>\n",
       "      <td>1088457.0</td>\n",
       "      <td>419457.0</td>\n",
       "      <td>416588.0</td>\n",
       "      <td>490608.236876</td>\n",
       "      <td>471911.310674</td>\n",
       "      <td>1.422479e+06</td>\n",
       "      <td>1.422479e+06</td>\n",
       "      <td>245669.103788</td>\n",
       "      <td>237397.422763</td>\n",
       "      <td>244939.133088</td>\n",
       "      <td>234513.887911</td>\n",
       "      <td>58014.0</td>\n",
       "      <td>57402.0</td>\n",
       "      <td>655161.236876</td>\n",
       "      <td>636146.310674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CT003</td>\n",
       "      <td>1184745.0</td>\n",
       "      <td>1168965.0</td>\n",
       "      <td>458291.0</td>\n",
       "      <td>456157.0</td>\n",
       "      <td>531916.401250</td>\n",
       "      <td>519861.644142</td>\n",
       "      <td>1.503435e+06</td>\n",
       "      <td>1.503435e+06</td>\n",
       "      <td>266407.973214</td>\n",
       "      <td>260860.911186</td>\n",
       "      <td>265508.428036</td>\n",
       "      <td>259000.732956</td>\n",
       "      <td>59295.0</td>\n",
       "      <td>58931.0</td>\n",
       "      <td>689043.401250</td>\n",
       "      <td>675766.644142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NT005</td>\n",
       "      <td>1110667.0</td>\n",
       "      <td>1067155.0</td>\n",
       "      <td>423652.0</td>\n",
       "      <td>406878.0</td>\n",
       "      <td>471502.137855</td>\n",
       "      <td>443915.716838</td>\n",
       "      <td>1.415292e+06</td>\n",
       "      <td>1.415292e+06</td>\n",
       "      <td>237566.663194</td>\n",
       "      <td>222745.097278</td>\n",
       "      <td>233935.474661</td>\n",
       "      <td>221170.619560</td>\n",
       "      <td>56161.0</td>\n",
       "      <td>54908.0</td>\n",
       "      <td>641680.137855</td>\n",
       "      <td>613641.716838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CT007</td>\n",
       "      <td>1138151.0</td>\n",
       "      <td>1149703.0</td>\n",
       "      <td>421712.0</td>\n",
       "      <td>425634.0</td>\n",
       "      <td>489541.405657</td>\n",
       "      <td>496968.257306</td>\n",
       "      <td>1.431403e+06</td>\n",
       "      <td>1.431403e+06</td>\n",
       "      <td>243929.482988</td>\n",
       "      <td>249904.719304</td>\n",
       "      <td>245611.922669</td>\n",
       "      <td>247063.538002</td>\n",
       "      <td>63658.0</td>\n",
       "      <td>64002.0</td>\n",
       "      <td>673969.405657</td>\n",
       "      <td>681836.257306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CT010</td>\n",
       "      <td>1122091.0</td>\n",
       "      <td>1115262.0</td>\n",
       "      <td>408536.0</td>\n",
       "      <td>412789.0</td>\n",
       "      <td>474689.564570</td>\n",
       "      <td>466056.025773</td>\n",
       "      <td>1.439088e+06</td>\n",
       "      <td>1.439088e+06</td>\n",
       "      <td>239267.805546</td>\n",
       "      <td>234389.921100</td>\n",
       "      <td>235421.759024</td>\n",
       "      <td>231666.104673</td>\n",
       "      <td>59937.0</td>\n",
       "      <td>59739.0</td>\n",
       "      <td>657339.564570</td>\n",
       "      <td>648058.025773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CT012</td>\n",
       "      <td>1107827.0</td>\n",
       "      <td>1091554.0</td>\n",
       "      <td>391761.0</td>\n",
       "      <td>389121.0</td>\n",
       "      <td>497944.071008</td>\n",
       "      <td>486970.812681</td>\n",
       "      <td>1.453701e+06</td>\n",
       "      <td>1.453701e+06</td>\n",
       "      <td>251339.320238</td>\n",
       "      <td>244323.770697</td>\n",
       "      <td>246604.750770</td>\n",
       "      <td>242647.041984</td>\n",
       "      <td>62131.0</td>\n",
       "      <td>61648.0</td>\n",
       "      <td>668019.071008</td>\n",
       "      <td>654484.812681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CT013</td>\n",
       "      <td>1131618.0</td>\n",
       "      <td>1085518.0</td>\n",
       "      <td>400730.0</td>\n",
       "      <td>382679.0</td>\n",
       "      <td>524760.488220</td>\n",
       "      <td>498031.891560</td>\n",
       "      <td>1.419949e+06</td>\n",
       "      <td>1.419949e+06</td>\n",
       "      <td>262316.073827</td>\n",
       "      <td>250895.428303</td>\n",
       "      <td>262444.414393</td>\n",
       "      <td>247136.463257</td>\n",
       "      <td>62748.0</td>\n",
       "      <td>60772.0</td>\n",
       "      <td>692313.488220</td>\n",
       "      <td>666431.891560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NT015</td>\n",
       "      <td>1037379.0</td>\n",
       "      <td>1038595.0</td>\n",
       "      <td>377454.0</td>\n",
       "      <td>386570.0</td>\n",
       "      <td>458988.573462</td>\n",
       "      <td>453851.341338</td>\n",
       "      <td>1.331148e+06</td>\n",
       "      <td>1.331148e+06</td>\n",
       "      <td>229765.717092</td>\n",
       "      <td>228380.735128</td>\n",
       "      <td>229222.856370</td>\n",
       "      <td>225470.606209</td>\n",
       "      <td>57936.0</td>\n",
       "      <td>57469.0</td>\n",
       "      <td>625411.573462</td>\n",
       "      <td>617026.341338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NT016</td>\n",
       "      <td>1143170.0</td>\n",
       "      <td>1110370.0</td>\n",
       "      <td>405225.0</td>\n",
       "      <td>394230.0</td>\n",
       "      <td>504111.022004</td>\n",
       "      <td>484988.825810</td>\n",
       "      <td>1.267231e+06</td>\n",
       "      <td>1.267231e+06</td>\n",
       "      <td>252366.166819</td>\n",
       "      <td>241518.612710</td>\n",
       "      <td>251744.855185</td>\n",
       "      <td>243470.213100</td>\n",
       "      <td>59494.0</td>\n",
       "      <td>57989.0</td>\n",
       "      <td>694629.022004</td>\n",
       "      <td>672629.825810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NT017</td>\n",
       "      <td>1153453.0</td>\n",
       "      <td>1078295.0</td>\n",
       "      <td>436138.0</td>\n",
       "      <td>408880.0</td>\n",
       "      <td>497094.953205</td>\n",
       "      <td>450596.126581</td>\n",
       "      <td>1.456538e+06</td>\n",
       "      <td>1.456538e+06</td>\n",
       "      <td>248767.239049</td>\n",
       "      <td>227148.873162</td>\n",
       "      <td>248327.714156</td>\n",
       "      <td>223447.253419</td>\n",
       "      <td>57825.0</td>\n",
       "      <td>55245.0</td>\n",
       "      <td>664121.953205</td>\n",
       "      <td>616935.126581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T09:22:33.922654Z",
     "start_time": "2025-11-07T09:22:33.788766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "IN_CSV  = Path(\"longitude_stats_without_normalizations/2_sessions_aparc_voxels_lh_rh.csv\")  # or your lh/rh file\n",
    "OUT_CSV = Path(\"longitude_stats_without_normalizations/2_sessions_aparc_voxels_merge.csv\")\n",
    "AGGREGATION = \"sum\"   # options: \"sum\" (default) or \"mean\"\n",
    "# ----------------------------\n",
    "\n",
    "# Load\n",
    "df = pd.read_csv(IN_CSV)\n",
    "\n",
    "# Regex to catch columns like: insula_lh_ses1, bankssts_rh_ses2, etc.\n",
    "pat = re.compile(r\"^(?P<struct>.+)_(?P<hemi>lh|rh)_ses(?P<ses>[12])$\", re.IGNORECASE)\n",
    "\n",
    "# Map (struct, session) -> list of columns [lh_col, rh_col] (whichever exist)\n",
    "pairs = {}\n",
    "other_cols = []  # cols we keep as-is (e.g., subject_code)\n",
    "for col in df.columns:\n",
    "    m = pat.match(col)\n",
    "    if m:\n",
    "        struct = m.group(\"struct\")\n",
    "        hemi = m.group(\"hemi\").lower()\n",
    "        ses = int(m.group(\"ses\"))\n",
    "        key = (struct, ses)\n",
    "        pairs.setdefault(key, {\"lh\": None, \"rh\": None})\n",
    "        pairs[key][hemi] = col\n",
    "    else:\n",
    "        other_cols.append(col)\n",
    "\n",
    "# Build merged dataframe\n",
    "out = df[other_cols].copy()  # keep subject_code and any non-lh/rh columns\n",
    "\n",
    "# Create merged columns like \"<struct>_ses1\" / \"<struct>_ses2\"\n",
    "for (struct, ses), hemis in pairs.items():\n",
    "    new_col = f\"{struct}_ses{ses}\"\n",
    "    out[new_col] = merge_two_cols(df, [hemis.get(\"lh\"), hemis.get(\"rh\")], how=AGGREGATION)\n",
    "\n",
    "# Optional: drop any original lh/rh columns entirely (keep only merged + other)\n",
    "# If you want to *also* keep the original columns, comment the next two lines.\n",
    "lh_rh_cols = [c for c in df.columns if pat.match(c)]\n",
    "out = out[[c for c in out.columns if c not in lh_rh_cols]]\n",
    "\n",
    "\n",
    "\n",
    "ordered_cols = sorted(out.columns, key=col_sort_key)\n",
    "out = out[ordered_cols]\n",
    "\n",
    "# Save\n",
    "OUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "out.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "print(f\"Saved merged file: {OUT_CSV}\")\n",
    "display(out.head(10))\n"
   ],
   "id": "90d667b11de48e3f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged file: longitude_stats_without_normalizations\\2_sessions_aparc_voxels_merge.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  subject_code  bankssts_ses1  bankssts_ses2  caudalanteriorcingulate_ses1  \\\n",
       "0        NT002           4630           4547                          3483   \n",
       "1        CT003           5414           5286                          3641   \n",
       "2        NT005           4984           4847                          3289   \n",
       "3        CT007           4228           4192                          4695   \n",
       "4        CT010           4836           4800                          3832   \n",
       "5        CT012           5770           5692                          3867   \n",
       "6        CT013           5303           5376                          4242   \n",
       "7        NT015           4317           4277                          3245   \n",
       "8        NT016           6578           6296                          4624   \n",
       "9        NT017           6702           6470                          3428   \n",
       "\n",
       "   caudalanteriorcingulate_ses2  caudalmiddlefrontal_ses1  \\\n",
       "0                          3376                     12457   \n",
       "1                          3698                     13923   \n",
       "2                          3201                      9134   \n",
       "3                          4833                     11265   \n",
       "4                          3779                      9310   \n",
       "5                          3881                     11473   \n",
       "6                          4109                     12054   \n",
       "7                          3133                     12903   \n",
       "8                          4526                     11288   \n",
       "9                          3140                     14781   \n",
       "\n",
       "   caudalmiddlefrontal_ses2  cuneus_ses1  cuneus_ses2  entorhinal_ses1  ...  \\\n",
       "0                     11686         8233         8022             3885  ...   \n",
       "1                     13379         6541         6480             3679  ...   \n",
       "2                      8261         5644         5318             4049  ...   \n",
       "3                     11605         5775         5824             3691  ...   \n",
       "4                      9159         7094         6949             3205  ...   \n",
       "5                     11151         6220         6301             3977  ...   \n",
       "6                     11314         7149         6969             3552  ...   \n",
       "7                     12628         8121         8150             2851  ...   \n",
       "8                     10542         6731         6581             4024  ...   \n",
       "9                     12194         5846         5542             3674  ...   \n",
       "\n",
       "   superiorparietal_ses1  superiorparietal_ses2  superiortemporal_ses1  \\\n",
       "0                  29238                  28032                  24431   \n",
       "1                  32101                  31242                  24854   \n",
       "2                  28052                  25215                  23734   \n",
       "3                  25890                  26348                  28656   \n",
       "4                  24883                  24689                  23551   \n",
       "5                  22114                  21656                  27877   \n",
       "6                  29462                  25944                  26033   \n",
       "7                  23412                  23971                  23097   \n",
       "8                  32128                  30323                  25880   \n",
       "9                  22892                  20276                  25252   \n",
       "\n",
       "   superiortemporal_ses2  supramarginal_ses1  supramarginal_ses2  \\\n",
       "0                  23930               21061               20247   \n",
       "1                  24200               19754               19265   \n",
       "2                  22924               20774               19162   \n",
       "3                  28823               18711               18882   \n",
       "4                  23497               21469               21299   \n",
       "5                  27401               21436               20955   \n",
       "6                  24740               20920               19581   \n",
       "7                  23044               19913               19816   \n",
       "8                  24787               25579               24179   \n",
       "9                  23227               23837               20688   \n",
       "\n",
       "   temporalpole_ses1  temporalpole_ses2  transversetemporal_ses1  \\\n",
       "0               5244               4879                     1908   \n",
       "1               6570               6413                     1781   \n",
       "2               4908               4945                     2124   \n",
       "3               4753               5342                     2171   \n",
       "4               5055               4960                     1849   \n",
       "5               5881               6043                     2177   \n",
       "6               5584               5218                     2225   \n",
       "7               4607               4331                     1672   \n",
       "8               4570               4612                     2750   \n",
       "9               4752               4515                     2171   \n",
       "\n",
       "   transversetemporal_ses2  \n",
       "0                     1841  \n",
       "1                     1740  \n",
       "2                     2066  \n",
       "3                     2156  \n",
       "4                     1832  \n",
       "5                     2111  \n",
       "6                     2250  \n",
       "7                     1651  \n",
       "8                     2666  \n",
       "9                     2074  \n",
       "\n",
       "[10 rows x 69 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_code</th>\n",
       "      <th>bankssts_ses1</th>\n",
       "      <th>bankssts_ses2</th>\n",
       "      <th>caudalanteriorcingulate_ses1</th>\n",
       "      <th>caudalanteriorcingulate_ses2</th>\n",
       "      <th>caudalmiddlefrontal_ses1</th>\n",
       "      <th>caudalmiddlefrontal_ses2</th>\n",
       "      <th>cuneus_ses1</th>\n",
       "      <th>cuneus_ses2</th>\n",
       "      <th>entorhinal_ses1</th>\n",
       "      <th>...</th>\n",
       "      <th>superiorparietal_ses1</th>\n",
       "      <th>superiorparietal_ses2</th>\n",
       "      <th>superiortemporal_ses1</th>\n",
       "      <th>superiortemporal_ses2</th>\n",
       "      <th>supramarginal_ses1</th>\n",
       "      <th>supramarginal_ses2</th>\n",
       "      <th>temporalpole_ses1</th>\n",
       "      <th>temporalpole_ses2</th>\n",
       "      <th>transversetemporal_ses1</th>\n",
       "      <th>transversetemporal_ses2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NT002</td>\n",
       "      <td>4630</td>\n",
       "      <td>4547</td>\n",
       "      <td>3483</td>\n",
       "      <td>3376</td>\n",
       "      <td>12457</td>\n",
       "      <td>11686</td>\n",
       "      <td>8233</td>\n",
       "      <td>8022</td>\n",
       "      <td>3885</td>\n",
       "      <td>...</td>\n",
       "      <td>29238</td>\n",
       "      <td>28032</td>\n",
       "      <td>24431</td>\n",
       "      <td>23930</td>\n",
       "      <td>21061</td>\n",
       "      <td>20247</td>\n",
       "      <td>5244</td>\n",
       "      <td>4879</td>\n",
       "      <td>1908</td>\n",
       "      <td>1841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CT003</td>\n",
       "      <td>5414</td>\n",
       "      <td>5286</td>\n",
       "      <td>3641</td>\n",
       "      <td>3698</td>\n",
       "      <td>13923</td>\n",
       "      <td>13379</td>\n",
       "      <td>6541</td>\n",
       "      <td>6480</td>\n",
       "      <td>3679</td>\n",
       "      <td>...</td>\n",
       "      <td>32101</td>\n",
       "      <td>31242</td>\n",
       "      <td>24854</td>\n",
       "      <td>24200</td>\n",
       "      <td>19754</td>\n",
       "      <td>19265</td>\n",
       "      <td>6570</td>\n",
       "      <td>6413</td>\n",
       "      <td>1781</td>\n",
       "      <td>1740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NT005</td>\n",
       "      <td>4984</td>\n",
       "      <td>4847</td>\n",
       "      <td>3289</td>\n",
       "      <td>3201</td>\n",
       "      <td>9134</td>\n",
       "      <td>8261</td>\n",
       "      <td>5644</td>\n",
       "      <td>5318</td>\n",
       "      <td>4049</td>\n",
       "      <td>...</td>\n",
       "      <td>28052</td>\n",
       "      <td>25215</td>\n",
       "      <td>23734</td>\n",
       "      <td>22924</td>\n",
       "      <td>20774</td>\n",
       "      <td>19162</td>\n",
       "      <td>4908</td>\n",
       "      <td>4945</td>\n",
       "      <td>2124</td>\n",
       "      <td>2066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CT007</td>\n",
       "      <td>4228</td>\n",
       "      <td>4192</td>\n",
       "      <td>4695</td>\n",
       "      <td>4833</td>\n",
       "      <td>11265</td>\n",
       "      <td>11605</td>\n",
       "      <td>5775</td>\n",
       "      <td>5824</td>\n",
       "      <td>3691</td>\n",
       "      <td>...</td>\n",
       "      <td>25890</td>\n",
       "      <td>26348</td>\n",
       "      <td>28656</td>\n",
       "      <td>28823</td>\n",
       "      <td>18711</td>\n",
       "      <td>18882</td>\n",
       "      <td>4753</td>\n",
       "      <td>5342</td>\n",
       "      <td>2171</td>\n",
       "      <td>2156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CT010</td>\n",
       "      <td>4836</td>\n",
       "      <td>4800</td>\n",
       "      <td>3832</td>\n",
       "      <td>3779</td>\n",
       "      <td>9310</td>\n",
       "      <td>9159</td>\n",
       "      <td>7094</td>\n",
       "      <td>6949</td>\n",
       "      <td>3205</td>\n",
       "      <td>...</td>\n",
       "      <td>24883</td>\n",
       "      <td>24689</td>\n",
       "      <td>23551</td>\n",
       "      <td>23497</td>\n",
       "      <td>21469</td>\n",
       "      <td>21299</td>\n",
       "      <td>5055</td>\n",
       "      <td>4960</td>\n",
       "      <td>1849</td>\n",
       "      <td>1832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CT012</td>\n",
       "      <td>5770</td>\n",
       "      <td>5692</td>\n",
       "      <td>3867</td>\n",
       "      <td>3881</td>\n",
       "      <td>11473</td>\n",
       "      <td>11151</td>\n",
       "      <td>6220</td>\n",
       "      <td>6301</td>\n",
       "      <td>3977</td>\n",
       "      <td>...</td>\n",
       "      <td>22114</td>\n",
       "      <td>21656</td>\n",
       "      <td>27877</td>\n",
       "      <td>27401</td>\n",
       "      <td>21436</td>\n",
       "      <td>20955</td>\n",
       "      <td>5881</td>\n",
       "      <td>6043</td>\n",
       "      <td>2177</td>\n",
       "      <td>2111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CT013</td>\n",
       "      <td>5303</td>\n",
       "      <td>5376</td>\n",
       "      <td>4242</td>\n",
       "      <td>4109</td>\n",
       "      <td>12054</td>\n",
       "      <td>11314</td>\n",
       "      <td>7149</td>\n",
       "      <td>6969</td>\n",
       "      <td>3552</td>\n",
       "      <td>...</td>\n",
       "      <td>29462</td>\n",
       "      <td>25944</td>\n",
       "      <td>26033</td>\n",
       "      <td>24740</td>\n",
       "      <td>20920</td>\n",
       "      <td>19581</td>\n",
       "      <td>5584</td>\n",
       "      <td>5218</td>\n",
       "      <td>2225</td>\n",
       "      <td>2250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NT015</td>\n",
       "      <td>4317</td>\n",
       "      <td>4277</td>\n",
       "      <td>3245</td>\n",
       "      <td>3133</td>\n",
       "      <td>12903</td>\n",
       "      <td>12628</td>\n",
       "      <td>8121</td>\n",
       "      <td>8150</td>\n",
       "      <td>2851</td>\n",
       "      <td>...</td>\n",
       "      <td>23412</td>\n",
       "      <td>23971</td>\n",
       "      <td>23097</td>\n",
       "      <td>23044</td>\n",
       "      <td>19913</td>\n",
       "      <td>19816</td>\n",
       "      <td>4607</td>\n",
       "      <td>4331</td>\n",
       "      <td>1672</td>\n",
       "      <td>1651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NT016</td>\n",
       "      <td>6578</td>\n",
       "      <td>6296</td>\n",
       "      <td>4624</td>\n",
       "      <td>4526</td>\n",
       "      <td>11288</td>\n",
       "      <td>10542</td>\n",
       "      <td>6731</td>\n",
       "      <td>6581</td>\n",
       "      <td>4024</td>\n",
       "      <td>...</td>\n",
       "      <td>32128</td>\n",
       "      <td>30323</td>\n",
       "      <td>25880</td>\n",
       "      <td>24787</td>\n",
       "      <td>25579</td>\n",
       "      <td>24179</td>\n",
       "      <td>4570</td>\n",
       "      <td>4612</td>\n",
       "      <td>2750</td>\n",
       "      <td>2666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NT017</td>\n",
       "      <td>6702</td>\n",
       "      <td>6470</td>\n",
       "      <td>3428</td>\n",
       "      <td>3140</td>\n",
       "      <td>14781</td>\n",
       "      <td>12194</td>\n",
       "      <td>5846</td>\n",
       "      <td>5542</td>\n",
       "      <td>3674</td>\n",
       "      <td>...</td>\n",
       "      <td>22892</td>\n",
       "      <td>20276</td>\n",
       "      <td>25252</td>\n",
       "      <td>23227</td>\n",
       "      <td>23837</td>\n",
       "      <td>20688</td>\n",
       "      <td>4752</td>\n",
       "      <td>4515</td>\n",
       "      <td>2171</td>\n",
       "      <td>2074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  69 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T09:22:34.119510Z",
     "start_time": "2025-11-07T09:22:33.998249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "IN_CSV   = Path(\"longitude_stats_without_normalizations/2_sessions_aparc_voxels_lh_rh.csv\")\n",
    "MAP_XLSX = Path(\"longitude_stats_without_normalizations/mapping_lobes_networks.xlsx\")\n",
    "MAP_SHEET = \"aparc_voxels_normalized_ttests\"\n",
    "\n",
    "OUT_CSV = Path(\"longitude_stats_without_normalizations/2_sessions_aparc_lobes_lr.csv\")\n",
    "\n",
    "AGGREGATION = \"sum\"   # \"sum\" (default) or \"mean\"\n",
    "STRUCTNAME_COL_CANDIDATES = [\"Parameter\"]\n",
    "LOBEFILTER_COL_CANDIDATES = [\"Lobes\"]\n",
    "# --------------------------------\n",
    "\n",
    "pat = re.compile(r\"^(?P<struct>.+)_(?P<hemi>lh|rh)_ses(?P<ses>\\d+)$\", re.IGNORECASE)\n",
    "\n",
    "\n",
    "# --- 1) Load subject table ---\n",
    "df = pd.read_csv(IN_CSV)\n",
    "\n",
    "# Identify all columns that look like <struct>_(lh|rh)_ses<1|2>\n",
    "lr_cols = [c for c in df.columns if pat.match(c)]\n",
    "\n",
    "# Collect unique sessions present (e.g., {1,2})\n",
    "sessions_present = sorted({int(pat.match(c).group(\"ses\")) for c in lr_cols})\n",
    "\n",
    "# --- 2) Load mapping and build struct -> lobe mapping ---\n",
    "map_df = pd.read_excel(MAP_XLSX, sheet_name=MAP_SHEET)\n",
    "\n",
    "col_struct = pick_column(map_df, STRUCTNAME_COL_CANDIDATES)\n",
    "col_lobe   = pick_column(map_df, LOBEFILTER_COL_CANDIDATES)  # prefer lobefilter\n",
    "\n",
    "if not col_struct or not col_lobe:\n",
    "    raise ValueError(f\"Could not find mapping columns. StructName={col_struct}, LobeFilter={col_lobe}\")\n",
    "\n",
    "# Normalize mapping keys with slugify so they match the struct part in your IN_CSV columns\n",
    "# (Your IN_CSV struct segments are typically slugified already from earlier steps.)\n",
    "map_df = map_df[[col_struct, col_lobe]].dropna()\n",
    "map_df[col_struct] = map_df[col_struct].astype(str)\n",
    "map_df[col_lobe]   = map_df[col_lobe].astype(str)\n",
    "\n",
    "struct_to_lobe = {slugify(s): slugify(l) for s, l in zip(map_df[col_struct], map_df[col_lobe])}\n",
    "\n",
    "# --- 3) Build lobe-level columns by hemisphere & session ---\n",
    "# For each lobe, hemi, session  sum values across mapped struct columns\n",
    "out = df[[\"subject_code\"]].copy()\n",
    "\n",
    "# Find all struct keys present in the IN_CSV (to intersect with mapping)\n",
    "present_structs = {}\n",
    "for c in lr_cols:\n",
    "    m = pat.match(c)\n",
    "    present_structs.setdefault(m.group(\"struct\"), set()).add((m.group(\"hemi\").lower(), int(m.group(\"ses\"))))\n",
    "\n",
    "# All lobes that appear in the mapping for structs actually present in the data\n",
    "lobes_present = sorted({struct_to_lobe.get(s) for s in present_structs.keys() if struct_to_lobe.get(s)})\n",
    "\n",
    "# Build columns\n",
    "for lobe in lobes_present:\n",
    "    for ses in sessions_present:\n",
    "        for hemi in (\"lh\", \"rh\"):\n",
    "            # Collect all struct columns that map to this lobe & hemi & session\n",
    "            cols_for_group = []\n",
    "            for struct in present_structs.keys():\n",
    "                if struct_to_lobe.get(struct) != lobe:\n",
    "                    continue\n",
    "                col_name = f\"{struct}_{hemi}_ses{ses}\"\n",
    "                if col_name in df.columns:\n",
    "                    cols_for_group.append(col_name)\n",
    "\n",
    "            new_col = f\"{lobe}_{hemi}_ses{ses}\"\n",
    "            out[new_col] = reduce_cols(df, cols_for_group, how=AGGREGATION)\n",
    "\n",
    "\n",
    "out = out[sorted(out.columns, key=order_key_lobe)]\n",
    "\n",
    "# --- 5) Save ---\n",
    "OUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "out.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "print(f\"Saved lobe-level file: {OUT_CSV}\")\n",
    "display(out.head(10))\n"
   ],
   "id": "bc185d63f8c438f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved lobe-level file: longitude_stats_without_normalizations\\2_sessions_aparc_lobes_lr.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  subject_code  cingulate_lh_ses1  cingulate_rh_ses1  cingulate_lh_ses2  \\\n",
       "0        NT002               9706               9865               9481   \n",
       "1        CT003              10398              10116              10359   \n",
       "2        NT005               7537               9053               7162   \n",
       "3        CT007              10191              11449              10221   \n",
       "4        CT010               9747              10332               9666   \n",
       "5        CT012              10343              10517              10094   \n",
       "6        CT013              11247              10899              10874   \n",
       "7        NT015              10087               7667               9844   \n",
       "8        NT016               9797              11546               9602   \n",
       "9        NT017               9958               8556               9122   \n",
       "\n",
       "   cingulate_rh_ses2  frontal_lh_ses1  frontal_rh_ses1  frontal_lh_ses2  \\\n",
       "0               9582            89293            84816            85423   \n",
       "1              10018            93810            93950            91608   \n",
       "2               8678            88186            83363            80631   \n",
       "3              11814            92583            89581            95697   \n",
       "4              10275            83272            90581            80950   \n",
       "5              10342            91091            86217            87877   \n",
       "6              10164            94079            95972            89250   \n",
       "7               7592            78371            79233            77450   \n",
       "8              11334            86041            84369            81924   \n",
       "9               8020            89886           100880            78901   \n",
       "\n",
       "   frontal_rh_ses2  insula_lh_ses1  ...  occipital_lh_ses2  occipital_rh_ses2  \\\n",
       "0            79681            6489  ...              25788              27386   \n",
       "1            91864            6930  ...              25565              26985   \n",
       "2            77493            8207  ...              21904              22303   \n",
       "3            90855            7518  ...              25342              26156   \n",
       "4            87320            6532  ...              25880              27921   \n",
       "5            84407            7779  ...              24029              25579   \n",
       "6            89608            7357  ...              28401              28536   \n",
       "7            77730            6872  ...              27463              27919   \n",
       "8            81611            6917  ...              23785              27984   \n",
       "9            87083            7106  ...              21435              21994   \n",
       "\n",
       "   parietal_lh_ses1  parietal_rh_ses1  parietal_lh_ses2  parietal_rh_ses2  \\\n",
       "0             58614             59527             56719             57026   \n",
       "1             67272             69244             65684             67453   \n",
       "2             54673             57590             50169             52917   \n",
       "3             51499             56189             52942             56020   \n",
       "4             60122             46754             59095             47148   \n",
       "5             54042             53904             52373             53256   \n",
       "6             60409             62153             55750             56117   \n",
       "7             52742             53544             53269             53934   \n",
       "8             63597             61607             59910             58494   \n",
       "9             63865             54713             56443             47747   \n",
       "\n",
       "   temporal_lh_ses1  temporal_rh_ses1  temporal_lh_ses2  temporal_rh_ses2  \n",
       "0             54678             55487             53217             54179  \n",
       "1             61560             58386             60339             56304  \n",
       "2             56064             52803             54508             51899  \n",
       "3             56565             54945             57935             54860  \n",
       "4             52625             52191             51868             51920  \n",
       "5             63107             62111             61617             61001  \n",
       "6             60272             57124             59441             56059  \n",
       "7             54022             52978             53207             51489  \n",
       "8             60854             57609             58983             56324  \n",
       "9             56102             54743             54436             52294  \n",
       "\n",
       "[10 rows x 25 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_code</th>\n",
       "      <th>cingulate_lh_ses1</th>\n",
       "      <th>cingulate_rh_ses1</th>\n",
       "      <th>cingulate_lh_ses2</th>\n",
       "      <th>cingulate_rh_ses2</th>\n",
       "      <th>frontal_lh_ses1</th>\n",
       "      <th>frontal_rh_ses1</th>\n",
       "      <th>frontal_lh_ses2</th>\n",
       "      <th>frontal_rh_ses2</th>\n",
       "      <th>insula_lh_ses1</th>\n",
       "      <th>...</th>\n",
       "      <th>occipital_lh_ses2</th>\n",
       "      <th>occipital_rh_ses2</th>\n",
       "      <th>parietal_lh_ses1</th>\n",
       "      <th>parietal_rh_ses1</th>\n",
       "      <th>parietal_lh_ses2</th>\n",
       "      <th>parietal_rh_ses2</th>\n",
       "      <th>temporal_lh_ses1</th>\n",
       "      <th>temporal_rh_ses1</th>\n",
       "      <th>temporal_lh_ses2</th>\n",
       "      <th>temporal_rh_ses2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NT002</td>\n",
       "      <td>9706</td>\n",
       "      <td>9865</td>\n",
       "      <td>9481</td>\n",
       "      <td>9582</td>\n",
       "      <td>89293</td>\n",
       "      <td>84816</td>\n",
       "      <td>85423</td>\n",
       "      <td>79681</td>\n",
       "      <td>6489</td>\n",
       "      <td>...</td>\n",
       "      <td>25788</td>\n",
       "      <td>27386</td>\n",
       "      <td>58614</td>\n",
       "      <td>59527</td>\n",
       "      <td>56719</td>\n",
       "      <td>57026</td>\n",
       "      <td>54678</td>\n",
       "      <td>55487</td>\n",
       "      <td>53217</td>\n",
       "      <td>54179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CT003</td>\n",
       "      <td>10398</td>\n",
       "      <td>10116</td>\n",
       "      <td>10359</td>\n",
       "      <td>10018</td>\n",
       "      <td>93810</td>\n",
       "      <td>93950</td>\n",
       "      <td>91608</td>\n",
       "      <td>91864</td>\n",
       "      <td>6930</td>\n",
       "      <td>...</td>\n",
       "      <td>25565</td>\n",
       "      <td>26985</td>\n",
       "      <td>67272</td>\n",
       "      <td>69244</td>\n",
       "      <td>65684</td>\n",
       "      <td>67453</td>\n",
       "      <td>61560</td>\n",
       "      <td>58386</td>\n",
       "      <td>60339</td>\n",
       "      <td>56304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NT005</td>\n",
       "      <td>7537</td>\n",
       "      <td>9053</td>\n",
       "      <td>7162</td>\n",
       "      <td>8678</td>\n",
       "      <td>88186</td>\n",
       "      <td>83363</td>\n",
       "      <td>80631</td>\n",
       "      <td>77493</td>\n",
       "      <td>8207</td>\n",
       "      <td>...</td>\n",
       "      <td>21904</td>\n",
       "      <td>22303</td>\n",
       "      <td>54673</td>\n",
       "      <td>57590</td>\n",
       "      <td>50169</td>\n",
       "      <td>52917</td>\n",
       "      <td>56064</td>\n",
       "      <td>52803</td>\n",
       "      <td>54508</td>\n",
       "      <td>51899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CT007</td>\n",
       "      <td>10191</td>\n",
       "      <td>11449</td>\n",
       "      <td>10221</td>\n",
       "      <td>11814</td>\n",
       "      <td>92583</td>\n",
       "      <td>89581</td>\n",
       "      <td>95697</td>\n",
       "      <td>90855</td>\n",
       "      <td>7518</td>\n",
       "      <td>...</td>\n",
       "      <td>25342</td>\n",
       "      <td>26156</td>\n",
       "      <td>51499</td>\n",
       "      <td>56189</td>\n",
       "      <td>52942</td>\n",
       "      <td>56020</td>\n",
       "      <td>56565</td>\n",
       "      <td>54945</td>\n",
       "      <td>57935</td>\n",
       "      <td>54860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CT010</td>\n",
       "      <td>9747</td>\n",
       "      <td>10332</td>\n",
       "      <td>9666</td>\n",
       "      <td>10275</td>\n",
       "      <td>83272</td>\n",
       "      <td>90581</td>\n",
       "      <td>80950</td>\n",
       "      <td>87320</td>\n",
       "      <td>6532</td>\n",
       "      <td>...</td>\n",
       "      <td>25880</td>\n",
       "      <td>27921</td>\n",
       "      <td>60122</td>\n",
       "      <td>46754</td>\n",
       "      <td>59095</td>\n",
       "      <td>47148</td>\n",
       "      <td>52625</td>\n",
       "      <td>52191</td>\n",
       "      <td>51868</td>\n",
       "      <td>51920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CT012</td>\n",
       "      <td>10343</td>\n",
       "      <td>10517</td>\n",
       "      <td>10094</td>\n",
       "      <td>10342</td>\n",
       "      <td>91091</td>\n",
       "      <td>86217</td>\n",
       "      <td>87877</td>\n",
       "      <td>84407</td>\n",
       "      <td>7779</td>\n",
       "      <td>...</td>\n",
       "      <td>24029</td>\n",
       "      <td>25579</td>\n",
       "      <td>54042</td>\n",
       "      <td>53904</td>\n",
       "      <td>52373</td>\n",
       "      <td>53256</td>\n",
       "      <td>63107</td>\n",
       "      <td>62111</td>\n",
       "      <td>61617</td>\n",
       "      <td>61001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CT013</td>\n",
       "      <td>11247</td>\n",
       "      <td>10899</td>\n",
       "      <td>10874</td>\n",
       "      <td>10164</td>\n",
       "      <td>94079</td>\n",
       "      <td>95972</td>\n",
       "      <td>89250</td>\n",
       "      <td>89608</td>\n",
       "      <td>7357</td>\n",
       "      <td>...</td>\n",
       "      <td>28401</td>\n",
       "      <td>28536</td>\n",
       "      <td>60409</td>\n",
       "      <td>62153</td>\n",
       "      <td>55750</td>\n",
       "      <td>56117</td>\n",
       "      <td>60272</td>\n",
       "      <td>57124</td>\n",
       "      <td>59441</td>\n",
       "      <td>56059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NT015</td>\n",
       "      <td>10087</td>\n",
       "      <td>7667</td>\n",
       "      <td>9844</td>\n",
       "      <td>7592</td>\n",
       "      <td>78371</td>\n",
       "      <td>79233</td>\n",
       "      <td>77450</td>\n",
       "      <td>77730</td>\n",
       "      <td>6872</td>\n",
       "      <td>...</td>\n",
       "      <td>27463</td>\n",
       "      <td>27919</td>\n",
       "      <td>52742</td>\n",
       "      <td>53544</td>\n",
       "      <td>53269</td>\n",
       "      <td>53934</td>\n",
       "      <td>54022</td>\n",
       "      <td>52978</td>\n",
       "      <td>53207</td>\n",
       "      <td>51489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NT016</td>\n",
       "      <td>9797</td>\n",
       "      <td>11546</td>\n",
       "      <td>9602</td>\n",
       "      <td>11334</td>\n",
       "      <td>86041</td>\n",
       "      <td>84369</td>\n",
       "      <td>81924</td>\n",
       "      <td>81611</td>\n",
       "      <td>6917</td>\n",
       "      <td>...</td>\n",
       "      <td>23785</td>\n",
       "      <td>27984</td>\n",
       "      <td>63597</td>\n",
       "      <td>61607</td>\n",
       "      <td>59910</td>\n",
       "      <td>58494</td>\n",
       "      <td>60854</td>\n",
       "      <td>57609</td>\n",
       "      <td>58983</td>\n",
       "      <td>56324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NT017</td>\n",
       "      <td>9958</td>\n",
       "      <td>8556</td>\n",
       "      <td>9122</td>\n",
       "      <td>8020</td>\n",
       "      <td>89886</td>\n",
       "      <td>100880</td>\n",
       "      <td>78901</td>\n",
       "      <td>87083</td>\n",
       "      <td>7106</td>\n",
       "      <td>...</td>\n",
       "      <td>21435</td>\n",
       "      <td>21994</td>\n",
       "      <td>63865</td>\n",
       "      <td>54713</td>\n",
       "      <td>56443</td>\n",
       "      <td>47747</td>\n",
       "      <td>56102</td>\n",
       "      <td>54743</td>\n",
       "      <td>54436</td>\n",
       "      <td>52294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  25 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "417a5d153e60b492"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T09:22:34.410472Z",
     "start_time": "2025-11-07T09:22:34.274997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ---------- CONFIG ----------\n",
    "IN_CSV   = Path(\"longitude_stats_without_normalizations/2_sessions_aparc_voxels_merge.csv\")  # use your current file\n",
    "MAP_XLSX = Path(\"longitude_stats_without_normalizations/mapping_lobes_networks.xlsx\")\n",
    "MAP_SHEET = \"aparc_voxels_normalized_ttests\"\n",
    "\n",
    "OUT_CSV = Path(\"longitude_stats_without_normalizations/2_sessions_aparc_lobes_full_brain.csv\")\n",
    "\n",
    "AGGREGATION = \"sum\"   # \"sum\" (default) or \"mean\"\n",
    "STRUCTNAME_COL_CANDIDATES = [\"Parameter\"]  # mapping struct name column\n",
    "LOBEFILTER_COL_CANDIDATES = [\"Lobes\"]      # mapping lobe column\n",
    "# --------------------------------\n",
    "\n",
    "# Patterns (no helper re-definitions)\n",
    "pat_nohemi = re.compile(r\"^(?P<struct>.+)_ses(?P<ses>\\d+)$\", re.IGNORECASE)\n",
    "\n",
    "# 1) Load input table\n",
    "df = pd.read_csv(IN_CSV)\n",
    "\n",
    "# already merged per-structure (no hemisphere)\n",
    "nohemi_cols = [c for c in df.columns if pat_nohemi.match(c)]\n",
    "df_struct = df[[\"subject_code\"] + nohemi_cols].copy()\n",
    "\n",
    "# 3) Sessions present\n",
    "sessions_present = sorted({int(pat_nohemi.match(c).group(\"ses\")) for c in df_struct.columns if pat_nohemi.match(c)})\n",
    "\n",
    "# 4) Load mapping and build structlobe dict using your pick_column() + slugify()\n",
    "map_df = pd.read_excel(MAP_XLSX, sheet_name=MAP_SHEET)\n",
    "col_struct = pick_column(map_df, STRUCTNAME_COL_CANDIDATES)\n",
    "col_lobe   = pick_column(map_df, LOBEFILTER_COL_CANDIDATES)\n",
    "if not col_struct or not col_lobe:\n",
    "    raise ValueError(f\"Mapping columns not found. StructName={col_struct}, Lobes={col_lobe}\")\n",
    "\n",
    "map_df = map_df[[col_struct, col_lobe]].dropna()\n",
    "map_df[col_struct] = map_df[col_struct].astype(str)\n",
    "map_df[col_lobe]   = map_df[col_lobe].astype(str)\n",
    "struct_to_lobe = {slugify(s): slugify(l) for s, l in zip(map_df[col_struct], map_df[col_lobe])}\n",
    "\n",
    "# Which structs exist in the input?\n",
    "present_structs = {pat_nohemi.match(c).group(\"struct\") for c in df_struct.columns if pat_nohemi.match(c)}\n",
    "lobes_present = sorted({struct_to_lobe.get(s) for s in present_structs if struct_to_lobe.get(s)})\n",
    "\n",
    "# 5) Build lobe totals per session (no hemisphere)\n",
    "out = df_struct[[\"subject_code\"]].copy()\n",
    "for lobe in lobes_present:\n",
    "    for ses in sessions_present:\n",
    "        cols_for_group = []\n",
    "        for struct in present_structs:\n",
    "            if struct_to_lobe.get(struct) != lobe:\n",
    "                continue\n",
    "            col_name = f\"{struct}_ses{ses}\"\n",
    "            if col_name in df_struct.columns:\n",
    "                cols_for_group.append(col_name)\n",
    "        out[f\"{lobe}_ses{ses}\"] = reduce_cols(df_struct, cols_for_group, how=AGGREGATION)\n",
    "\n",
    "# 6) Order & save using your order_key_lobe()\n",
    "out = out[sorted(out.columns, key=order_key_lobe)]\n",
    "\n",
    "OUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "out.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "print(f\"Saved lobe-level (no hemisphere) file: {OUT_CSV}\")\n",
    "display(out.head(10))\n"
   ],
   "id": "257509e081ba44f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved lobe-level (no hemisphere) file: longitude_stats_without_normalizations\\2_sessions_aparc_lobes_full_brain.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  subject_code  cingulate_ses1  cingulate_ses2  frontal_ses1  frontal_ses2  \\\n",
       "0        NT002           19571           19063        174109        165104   \n",
       "1        CT003           20514           20377        187760        183472   \n",
       "2        NT005           16590           15840        171549        158124   \n",
       "3        CT007           21640           22035        182164        186552   \n",
       "4        CT010           20079           19941        173853        168270   \n",
       "5        CT012           20860           20436        177308        172284   \n",
       "6        CT013           22146           21038        190051        178858   \n",
       "7        NT015           17754           17436        157604        155180   \n",
       "8        NT016           21343           20936        170410        163535   \n",
       "9        NT017           18514           17142        190766        165984   \n",
       "\n",
       "   insula_ses1  insula_ses2  occipital_ses1  occipital_ses2  parietal_ses1  \\\n",
       "0        13009        12663           54898           53174         118141   \n",
       "1        13341        13115           53277           52550         136516   \n",
       "2        15975        15528           45661           44207         112263   \n",
       "3        14860        14758           51376           51498         107688   \n",
       "4        13295        13054           54790           53801         106876   \n",
       "5        15635        15424           50069           49608         107946   \n",
       "6        14546        14032           58386           56937         122562   \n",
       "7        13580        13352           56130           55382         106286   \n",
       "8        14350        14202           53424           51769         125204   \n",
       "9        13953        13094           44332           43429         118578   \n",
       "\n",
       "   parietal_ses2  temporal_ses1  temporal_ses2  \n",
       "0         113745         110165         107396  \n",
       "1         133137         119946         116643  \n",
       "2         103086         108867         106407  \n",
       "3         108962         111510         112795  \n",
       "4         106243         104816         103788  \n",
       "5         105629         125218         122618  \n",
       "6         111867         117396         115500  \n",
       "7         107203         107000         104696  \n",
       "8         118404         118463         115307  \n",
       "9         104190         110845         106730  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_code</th>\n",
       "      <th>cingulate_ses1</th>\n",
       "      <th>cingulate_ses2</th>\n",
       "      <th>frontal_ses1</th>\n",
       "      <th>frontal_ses2</th>\n",
       "      <th>insula_ses1</th>\n",
       "      <th>insula_ses2</th>\n",
       "      <th>occipital_ses1</th>\n",
       "      <th>occipital_ses2</th>\n",
       "      <th>parietal_ses1</th>\n",
       "      <th>parietal_ses2</th>\n",
       "      <th>temporal_ses1</th>\n",
       "      <th>temporal_ses2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NT002</td>\n",
       "      <td>19571</td>\n",
       "      <td>19063</td>\n",
       "      <td>174109</td>\n",
       "      <td>165104</td>\n",
       "      <td>13009</td>\n",
       "      <td>12663</td>\n",
       "      <td>54898</td>\n",
       "      <td>53174</td>\n",
       "      <td>118141</td>\n",
       "      <td>113745</td>\n",
       "      <td>110165</td>\n",
       "      <td>107396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CT003</td>\n",
       "      <td>20514</td>\n",
       "      <td>20377</td>\n",
       "      <td>187760</td>\n",
       "      <td>183472</td>\n",
       "      <td>13341</td>\n",
       "      <td>13115</td>\n",
       "      <td>53277</td>\n",
       "      <td>52550</td>\n",
       "      <td>136516</td>\n",
       "      <td>133137</td>\n",
       "      <td>119946</td>\n",
       "      <td>116643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NT005</td>\n",
       "      <td>16590</td>\n",
       "      <td>15840</td>\n",
       "      <td>171549</td>\n",
       "      <td>158124</td>\n",
       "      <td>15975</td>\n",
       "      <td>15528</td>\n",
       "      <td>45661</td>\n",
       "      <td>44207</td>\n",
       "      <td>112263</td>\n",
       "      <td>103086</td>\n",
       "      <td>108867</td>\n",
       "      <td>106407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CT007</td>\n",
       "      <td>21640</td>\n",
       "      <td>22035</td>\n",
       "      <td>182164</td>\n",
       "      <td>186552</td>\n",
       "      <td>14860</td>\n",
       "      <td>14758</td>\n",
       "      <td>51376</td>\n",
       "      <td>51498</td>\n",
       "      <td>107688</td>\n",
       "      <td>108962</td>\n",
       "      <td>111510</td>\n",
       "      <td>112795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CT010</td>\n",
       "      <td>20079</td>\n",
       "      <td>19941</td>\n",
       "      <td>173853</td>\n",
       "      <td>168270</td>\n",
       "      <td>13295</td>\n",
       "      <td>13054</td>\n",
       "      <td>54790</td>\n",
       "      <td>53801</td>\n",
       "      <td>106876</td>\n",
       "      <td>106243</td>\n",
       "      <td>104816</td>\n",
       "      <td>103788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CT012</td>\n",
       "      <td>20860</td>\n",
       "      <td>20436</td>\n",
       "      <td>177308</td>\n",
       "      <td>172284</td>\n",
       "      <td>15635</td>\n",
       "      <td>15424</td>\n",
       "      <td>50069</td>\n",
       "      <td>49608</td>\n",
       "      <td>107946</td>\n",
       "      <td>105629</td>\n",
       "      <td>125218</td>\n",
       "      <td>122618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CT013</td>\n",
       "      <td>22146</td>\n",
       "      <td>21038</td>\n",
       "      <td>190051</td>\n",
       "      <td>178858</td>\n",
       "      <td>14546</td>\n",
       "      <td>14032</td>\n",
       "      <td>58386</td>\n",
       "      <td>56937</td>\n",
       "      <td>122562</td>\n",
       "      <td>111867</td>\n",
       "      <td>117396</td>\n",
       "      <td>115500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NT015</td>\n",
       "      <td>17754</td>\n",
       "      <td>17436</td>\n",
       "      <td>157604</td>\n",
       "      <td>155180</td>\n",
       "      <td>13580</td>\n",
       "      <td>13352</td>\n",
       "      <td>56130</td>\n",
       "      <td>55382</td>\n",
       "      <td>106286</td>\n",
       "      <td>107203</td>\n",
       "      <td>107000</td>\n",
       "      <td>104696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NT016</td>\n",
       "      <td>21343</td>\n",
       "      <td>20936</td>\n",
       "      <td>170410</td>\n",
       "      <td>163535</td>\n",
       "      <td>14350</td>\n",
       "      <td>14202</td>\n",
       "      <td>53424</td>\n",
       "      <td>51769</td>\n",
       "      <td>125204</td>\n",
       "      <td>118404</td>\n",
       "      <td>118463</td>\n",
       "      <td>115307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NT017</td>\n",
       "      <td>18514</td>\n",
       "      <td>17142</td>\n",
       "      <td>190766</td>\n",
       "      <td>165984</td>\n",
       "      <td>13953</td>\n",
       "      <td>13094</td>\n",
       "      <td>44332</td>\n",
       "      <td>43429</td>\n",
       "      <td>118578</td>\n",
       "      <td>104190</td>\n",
       "      <td>110845</td>\n",
       "      <td>106730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T09:22:34.670399Z",
     "start_time": "2025-11-07T09:22:34.568459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ---------- CONFIG ----------\n",
    "IN_CSV   = Path(\"longitude_stats_without_normalizations/2_sessions_aparc_voxels_merge.csv\")  # use your current file\n",
    "MAP_XLSX = Path(\"longitude_stats_without_normalizations/mapping_lobes_networks.xlsx\")\n",
    "MAP_SHEET = \"aparc_voxels_normalized_ttests\"\n",
    "\n",
    "OUT_CSV = Path(\"longitude_stats_without_normalizations/2_sessions_aparc_nets_full_brain.csv\")\n",
    "\n",
    "AGGREGATION = \"sum\"   # \"sum\" (default) or \"mean\"\n",
    "STRUCTNAME_COL_CANDIDATES = [\"Parameter\"]  # mapping struct name column\n",
    "YEOFILTER_COL_CANDIDATES = [\"Yeo\"]      # mapping lobe column\n",
    "# --------------------------------\n",
    "\n",
    "# Patterns (no helper re-definitions)\n",
    "pat_nohemi = re.compile(r\"^(?P<struct>.+)_ses(?P<ses>\\d+)$\", re.IGNORECASE)\n",
    "\n",
    "# 1) Load input table\n",
    "df = pd.read_csv(IN_CSV)\n",
    "\n",
    "# already merged per-structure (no hemisphere)\n",
    "nohemi_cols = [c for c in df.columns if pat_nohemi.match(c)]\n",
    "df_struct = df[[\"subject_code\"] + nohemi_cols].copy()\n",
    "\n",
    "# 3) Sessions present\n",
    "sessions_present = sorted({int(pat_nohemi.match(c).group(\"ses\")) for c in df_struct.columns if pat_nohemi.match(c)})\n",
    "\n",
    "# 4) Load mapping and build structlobe dict using your pick_column() + slugify()\n",
    "map_df = pd.read_excel(MAP_XLSX, sheet_name=MAP_SHEET)\n",
    "col_struct = pick_column(map_df, STRUCTNAME_COL_CANDIDATES)\n",
    "col_net   = pick_column(map_df, YEOFILTER_COL_CANDIDATES)\n",
    "if not col_struct or not col_net:\n",
    "    raise ValueError(f\"Mapping columns not found. StructName={col_struct}, net={col_net}\")\n",
    "\n",
    "map_df = map_df[[col_struct, col_net]].dropna()\n",
    "map_df[col_struct] = map_df[col_struct].astype(str)\n",
    "map_df[col_net]   = map_df[col_net].astype(str)\n",
    "struct_to_net = {slugify(s): slugify(l) for s, l in zip(map_df[col_struct], map_df[col_net])}\n",
    "\n",
    "# Which structs exist in the input?\n",
    "present_structs = {pat_nohemi.match(c).group(\"struct\") for c in df_struct.columns if pat_nohemi.match(c)}\n",
    "networks_present = sorted({struct_to_net.get(s) for s in present_structs if struct_to_net.get(s)})\n",
    "\n",
    "# 5) Build lobe totals per session (no hemisphere)\n",
    "out = df_struct[[\"subject_code\"]].copy()\n",
    "for net in networks_present:\n",
    "    for ses in sessions_present:\n",
    "        cols_for_group = []\n",
    "        for struct in present_structs:\n",
    "            if struct_to_net.get(struct) != net:\n",
    "                continue\n",
    "            col_name = f\"{struct}_ses{ses}\"\n",
    "            if col_name in df_struct.columns:\n",
    "                cols_for_group.append(col_name)\n",
    "        out[f\"{net}_ses{ses}\"] = reduce_cols(df_struct, cols_for_group, how=AGGREGATION)\n",
    "\n",
    "\n",
    "# 6) Order & save using your order_key_lobe()\n",
    "out = out[sorted(out.columns, key=order_key_lobe)]\n",
    "\n",
    "# Rename network columns to your desired labels\n",
    "net_map = {\n",
    "    \"default_mode_ses1\"        : \"DMN_GM_ses1\",\n",
    "    \"default_mode_ses2\"        : \"DMN_GM_ses2\",\n",
    "    \"dorsal_attention_ses1\"    : \"DAN_GM_ses1\",\n",
    "    \"dorsal_attention_ses2\"    : \"DAN_GM_ses2\",   # <- fixed\n",
    "    \"frontoparietal_ses1\"      : \"FPN_GM_ses1\",\n",
    "    \"frontoparietal_ses2\"      : \"FPN_GM_ses2\",\n",
    "    \"limbic_ses1\"              : \"LIM_GM_ses1\",\n",
    "    \"limbic_ses2\"              : \"LIM_GM_ses2\",\n",
    "    \"somatomotor_ses1\"         : \"SMN_GM_ses1\",\n",
    "    \"somatomotor_ses2\"         : \"SMN_GM_ses2\",\n",
    "    \"ventral_attention_ses1\"   : \"VAN_GM_ses1\",\n",
    "    \"ventral_attention_ses2\"   : \"VAN_GM_ses2\",\n",
    "    \"visual_ses1\"              : \"VIS_GM_ses1\",\n",
    "    \"visual_ses2\"              : \"VIS_GM_ses2\",\n",
    "}\n",
    "\n",
    "# Correct signature:\n",
    "out = out.rename(columns=net_map)\n",
    "\n",
    "OUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "out.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "print(f\"Saved lobe-level (no hemisphere) file: {OUT_CSV}\")\n",
    "display(out.head(10))\n"
   ],
   "id": "4210121da90424b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved lobe-level (no hemisphere) file: longitude_stats_without_normalizations\\2_sessions_aparc_nets_full_brain.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  subject_code  DMN_GM_ses1  DMN_GM_ses2  DAN_GM_ses1  DAN_GM_ses2  \\\n",
       "0        NT002       145503       140448        29238        28032   \n",
       "1        CT003       163602       159262        32101        31242   \n",
       "2        NT005       140347       130680        28052        25215   \n",
       "3        CT007       145677       147333        25890        26348   \n",
       "4        CT010       145503       143330        24883        24689   \n",
       "5        CT012       152456       148166        22114        21656   \n",
       "6        CT013       161496       151958        29462        25944   \n",
       "7        NT015       136609       135068        23412        23971   \n",
       "8        NT016       151471       144659        32128        30323   \n",
       "9        NT017       156903       139231        22892        20276   \n",
       "\n",
       "   FPN_GM_ses1  FPN_GM_ses2  LIM_GM_ses1  LIM_GM_ses2  SMN_GM_ses1  \\\n",
       "0        45725        43292        61498        59040        79666   \n",
       "1        49248        47939        68644        67028        90636   \n",
       "2        41615        38412        60121        58524        78822   \n",
       "3        45649        47143        59708        61206        82243   \n",
       "4        45130        43090        55763        54546        71906   \n",
       "5        44610        43357        64223        62668        81108   \n",
       "6        46002        44532        65461        64313        82878   \n",
       "7        42163        41029        57144        55393        73140   \n",
       "8        42152        40649        62695        61649        78436   \n",
       "9        49380        43391        61987        59196        79467   \n",
       "\n",
       "   SMN_GM_ses2  VAN_GM_ses1  VAN_GM_ses2  VIS_GM_ses1  VIS_GM_ses2  \n",
       "0        76216        53484        51613        74779        72504  \n",
       "1        89068        52638        51529        74485        73226  \n",
       "2        73194        57560        54171        64388        62996  \n",
       "3        83858        58333        59006        71738        71706  \n",
       "4        70978        56568        55573        73956        72891  \n",
       "5        79692        59040        57829        73485        72631  \n",
       "6        75619        59105        56485        80683        79381  \n",
       "7        74000        51753        50963        74133        72825  \n",
       "8        74901        62139        59810        74173        72162  \n",
       "9        69356        62479        55983        63880        63136  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_code</th>\n",
       "      <th>DMN_GM_ses1</th>\n",
       "      <th>DMN_GM_ses2</th>\n",
       "      <th>DAN_GM_ses1</th>\n",
       "      <th>DAN_GM_ses2</th>\n",
       "      <th>FPN_GM_ses1</th>\n",
       "      <th>FPN_GM_ses2</th>\n",
       "      <th>LIM_GM_ses1</th>\n",
       "      <th>LIM_GM_ses2</th>\n",
       "      <th>SMN_GM_ses1</th>\n",
       "      <th>SMN_GM_ses2</th>\n",
       "      <th>VAN_GM_ses1</th>\n",
       "      <th>VAN_GM_ses2</th>\n",
       "      <th>VIS_GM_ses1</th>\n",
       "      <th>VIS_GM_ses2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NT002</td>\n",
       "      <td>145503</td>\n",
       "      <td>140448</td>\n",
       "      <td>29238</td>\n",
       "      <td>28032</td>\n",
       "      <td>45725</td>\n",
       "      <td>43292</td>\n",
       "      <td>61498</td>\n",
       "      <td>59040</td>\n",
       "      <td>79666</td>\n",
       "      <td>76216</td>\n",
       "      <td>53484</td>\n",
       "      <td>51613</td>\n",
       "      <td>74779</td>\n",
       "      <td>72504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CT003</td>\n",
       "      <td>163602</td>\n",
       "      <td>159262</td>\n",
       "      <td>32101</td>\n",
       "      <td>31242</td>\n",
       "      <td>49248</td>\n",
       "      <td>47939</td>\n",
       "      <td>68644</td>\n",
       "      <td>67028</td>\n",
       "      <td>90636</td>\n",
       "      <td>89068</td>\n",
       "      <td>52638</td>\n",
       "      <td>51529</td>\n",
       "      <td>74485</td>\n",
       "      <td>73226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NT005</td>\n",
       "      <td>140347</td>\n",
       "      <td>130680</td>\n",
       "      <td>28052</td>\n",
       "      <td>25215</td>\n",
       "      <td>41615</td>\n",
       "      <td>38412</td>\n",
       "      <td>60121</td>\n",
       "      <td>58524</td>\n",
       "      <td>78822</td>\n",
       "      <td>73194</td>\n",
       "      <td>57560</td>\n",
       "      <td>54171</td>\n",
       "      <td>64388</td>\n",
       "      <td>62996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CT007</td>\n",
       "      <td>145677</td>\n",
       "      <td>147333</td>\n",
       "      <td>25890</td>\n",
       "      <td>26348</td>\n",
       "      <td>45649</td>\n",
       "      <td>47143</td>\n",
       "      <td>59708</td>\n",
       "      <td>61206</td>\n",
       "      <td>82243</td>\n",
       "      <td>83858</td>\n",
       "      <td>58333</td>\n",
       "      <td>59006</td>\n",
       "      <td>71738</td>\n",
       "      <td>71706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CT010</td>\n",
       "      <td>145503</td>\n",
       "      <td>143330</td>\n",
       "      <td>24883</td>\n",
       "      <td>24689</td>\n",
       "      <td>45130</td>\n",
       "      <td>43090</td>\n",
       "      <td>55763</td>\n",
       "      <td>54546</td>\n",
       "      <td>71906</td>\n",
       "      <td>70978</td>\n",
       "      <td>56568</td>\n",
       "      <td>55573</td>\n",
       "      <td>73956</td>\n",
       "      <td>72891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CT012</td>\n",
       "      <td>152456</td>\n",
       "      <td>148166</td>\n",
       "      <td>22114</td>\n",
       "      <td>21656</td>\n",
       "      <td>44610</td>\n",
       "      <td>43357</td>\n",
       "      <td>64223</td>\n",
       "      <td>62668</td>\n",
       "      <td>81108</td>\n",
       "      <td>79692</td>\n",
       "      <td>59040</td>\n",
       "      <td>57829</td>\n",
       "      <td>73485</td>\n",
       "      <td>72631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CT013</td>\n",
       "      <td>161496</td>\n",
       "      <td>151958</td>\n",
       "      <td>29462</td>\n",
       "      <td>25944</td>\n",
       "      <td>46002</td>\n",
       "      <td>44532</td>\n",
       "      <td>65461</td>\n",
       "      <td>64313</td>\n",
       "      <td>82878</td>\n",
       "      <td>75619</td>\n",
       "      <td>59105</td>\n",
       "      <td>56485</td>\n",
       "      <td>80683</td>\n",
       "      <td>79381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NT015</td>\n",
       "      <td>136609</td>\n",
       "      <td>135068</td>\n",
       "      <td>23412</td>\n",
       "      <td>23971</td>\n",
       "      <td>42163</td>\n",
       "      <td>41029</td>\n",
       "      <td>57144</td>\n",
       "      <td>55393</td>\n",
       "      <td>73140</td>\n",
       "      <td>74000</td>\n",
       "      <td>51753</td>\n",
       "      <td>50963</td>\n",
       "      <td>74133</td>\n",
       "      <td>72825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NT016</td>\n",
       "      <td>151471</td>\n",
       "      <td>144659</td>\n",
       "      <td>32128</td>\n",
       "      <td>30323</td>\n",
       "      <td>42152</td>\n",
       "      <td>40649</td>\n",
       "      <td>62695</td>\n",
       "      <td>61649</td>\n",
       "      <td>78436</td>\n",
       "      <td>74901</td>\n",
       "      <td>62139</td>\n",
       "      <td>59810</td>\n",
       "      <td>74173</td>\n",
       "      <td>72162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NT017</td>\n",
       "      <td>156903</td>\n",
       "      <td>139231</td>\n",
       "      <td>22892</td>\n",
       "      <td>20276</td>\n",
       "      <td>49380</td>\n",
       "      <td>43391</td>\n",
       "      <td>61987</td>\n",
       "      <td>59196</td>\n",
       "      <td>79467</td>\n",
       "      <td>69356</td>\n",
       "      <td>62479</td>\n",
       "      <td>55983</td>\n",
       "      <td>63880</td>\n",
       "      <td>63136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T09:22:35.645726Z",
     "start_time": "2025-11-07T09:22:34.840845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "OUTPUT_XLSX = Path(\"longitude_stats_without_normalizations/2_sessions_T1_data.xlsx\")\n",
    "\n",
    "# The sheet that merges TWO CSVs into one\n",
    "MERGED_SHEET_NAME = \"global_parameters\"\n",
    "MERGED_CSVS = [\n",
    "    Path(\"longitude_stats_without_normalizations/2_sessions_aseg_summary_data.csv\"),\n",
    "    Path(\"longitude_stats_without_normalizations/2_sessions_aparc_summary_data.csv\"),\n",
    "]\n",
    "MERGE_KEY = \"subject_code\"\n",
    "MERGE_HOW = \"outer\"           # \"inner\", \"left\", \"right\", \"outer\"\n",
    "MERGE_SUFFIXES = (\"_x\", \"_y\") # for overlapping column names\n",
    "\n",
    "# Other individual sheets: sheet_name -> csv path\n",
    "OTHER_SHEETS = {\n",
    "    \"subcortical_vol\": Path(\"longitude_stats_without_normalizations/2_sessions_aseg_data.csv\"),\n",
    "    \"cortical_vol_lr\": Path(\"longitude_stats_without_normalizations/2_sessions_aparc_voxels_lh_rh.csv\"),\n",
    "    \"cortical_vol_full_brain\": Path(\"longitude_stats_without_normalizations/2_sessions_aparc_voxels_merge.csv\"),\n",
    "    \"cortical_lobes_lr_\": Path(\"longitude_stats_without_normalizations/2_sessions_aparc_lobes_lr.csv\"),\n",
    "    \"cortical_lobes_full_brain\": Path(\"longitude_stats_without_normalizations/2_sessions_aparc_lobes_full_brain.csv\"),  # <- check your path (removed .csv.csv)\n",
    "    \"cortical_networks_full_brain\": Path(\"longitude_stats_without_normalizations/2_sessions_aparc_nets_full_brain.csv\"),\n",
    "}\n",
    "# ===========================\n",
    "\n",
    "# ---------- helpers (local to this cell) ----------\n",
    "\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# 1) Merge the two CSVs into one sheet\n",
    "df_a = _read_csv(MERGED_CSVS[0], MERGE_KEY)\n",
    "df_b = _read_csv(MERGED_CSVS[1], MERGE_KEY)\n",
    "\n",
    "if df_a.empty and df_b.empty:\n",
    "    merged_df = pd.DataFrame(columns=[MERGE_KEY])\n",
    "else:\n",
    "    if MERGE_KEY not in df_a.columns or MERGE_KEY not in df_b.columns:\n",
    "        raise ValueError(f\"MERGE_KEY '{MERGE_KEY}' must be present in both files.\")\n",
    "    merged_df = df_a.merge(df_b, on=MERGE_KEY, how=MERGE_HOW, suffixes=MERGE_SUFFIXES)\n",
    "\n",
    "# Keep subject_code first, then sort consistently\n",
    "merged_df = _order_columns_subject_first(merged_df, MERGE_KEY)\n",
    "merged_df = _sort_consistent(merged_df, MERGE_KEY)\n",
    "\n",
    "# 2) Write everything to one Excel workbook (all sheets sorted the same way)\n",
    "with pd.ExcelWriter(OUTPUT_XLSX, engine=\"openpyxl\") as writer:\n",
    "    # merged sheet\n",
    "    merged_df.to_excel(writer, index=False, sheet_name=MERGED_SHEET_NAME[:31])\n",
    "    _autosize_columns(writer, merged_df, MERGED_SHEET_NAME[:31])\n",
    "\n",
    "    # other sheets (one CSV per sheet)\n",
    "    for sheet_name, csv_path in OTHER_SHEETS.items():\n",
    "        df = _read_csv(csv_path, MERGE_KEY)\n",
    "        df = _order_columns_subject_first(df, MERGE_KEY)\n",
    "        df = _sort_consistent(df, MERGE_KEY)\n",
    "        safe_name = sheet_name[:31]  # Excel sheet name limit is 31 chars\n",
    "        df.to_excel(writer, index=False, sheet_name=safe_name)\n",
    "        _autosize_columns(writer, df, safe_name)\n",
    "\n",
    "print(f\"Saved Excel workbook: {OUTPUT_XLSX}\")\n"
   ],
   "id": "d49d7871edaac3b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Excel workbook: longitude_stats_without_normalizations\\2_sessions_T1_data.xlsx\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ssSADDAWWD\n",
   "id": "8d0bcf1a035eb35"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## SADA\n",
    "\n"
   ],
   "id": "1b16ba15cff6b0fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "DEDWD\n",
   "id": "166d6e47152028b0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
