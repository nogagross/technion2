{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T10:05:46.936535Z",
     "start_time": "2025-11-10T10:05:46.930466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Re-run the full pipeline now that we've inspected the column names.\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "\n"
   ],
   "id": "7ef60f15cda09afd",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "5a1010521c9f23fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T10:05:47.033920Z",
     "start_time": "2025-11-10T10:05:47.028430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    def _clean(col: str) -> str:\n",
    "        if not isinstance(col, str):\n",
    "            return col\n",
    "        col = col.strip()\n",
    "        col = re.sub(r\"\\s+\", \" \", col)\n",
    "        col = col.replace(\" _\", \"_\").replace(\"_ \", \"_\")\n",
    "        return col\n",
    "    df = df.copy()\n",
    "    df.columns = [_clean(c) for c in df.columns]\n",
    "    return df"
   ],
   "id": "c28f4a902d782d80",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T10:05:47.104907Z",
     "start_time": "2025-11-10T10:05:47.097460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def normalize_name(name: str) -> str:\n",
    "    s = name.strip()\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)      # spaces -> _\n",
    "    s = s.replace(\"-\", \"_\")         # hyphens -> _\n",
    "    s = s.replace(\".\", \"_\")         # dots -> _\n",
    "    s = re.sub(r\"_+\", \"_\", s)       # collapse multiple _\n",
    "    s = s.lower()\n",
    "    # drop session suffixes\n",
    "    s = re.sub(r\"(_ses\\s*1|_ses\\s*2|_ses1|_ses2)$\", \"\", s)\n",
    "    # consistent lh/rh casing already lower\n",
    "    return s\n"
   ],
   "id": "f8be5c6496055231",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T10:05:47.129285Z",
     "start_time": "2025-11-10T10:05:47.122311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ------------- Helpers -------------\n",
    "def cohens_d(x: np.ndarray, y: np.ndarray) -> float:\n",
    "    # Cohen's d for independent samples (uses pooled SD)\n",
    "    nx, ny = len(x), len(y)\n",
    "    if nx < 2 or ny < 2:\n",
    "        return np.nan\n",
    "    vx, vy = x.var(ddof=1), y.var(ddof=1)\n",
    "    s = np.sqrt(((nx-1)*vx + (ny-1)*vy) / (nx + ny - 2))\n",
    "    if s == 0:\n",
    "        return 0.0\n",
    "    return (x.mean() - y.mean()) / s\n",
    "\n"
   ],
   "id": "3b89cd3a03b452dd",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T10:05:47.189736Z",
     "start_time": "2025-11-10T10:05:47.183345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def welch_ttest(x: np.ndarray, y: np.ndarray) -> Tuple[float, float]:\n",
    "    # Returns t-statistic and pvalue (Welch's t-test)\n",
    "    if len(x) < 2 or len(y) < 2:\n",
    "        return np.nan, np.nan\n",
    "    t, p = stats.ttest_ind(x, y, equal_var=False, nan_policy=\"omit\")\n",
    "    return t, p"
   ],
   "id": "503bf3520ee8dae",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T10:05:47.311027Z",
     "start_time": "2025-11-10T10:05:47.301912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "def fdr_bh(pvals: np.ndarray, alpha: float = 0.05) -> Tuple[np.ndarray, np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Benjamini-Hochberg FDR.\n",
    "    Returns: rejected(bool array), pvals_corrected, critical_value\n",
    "    \"\"\"\n",
    "    p = np.array(pvals, dtype=float)\n",
    "    n = np.sum(~np.isnan(p))\n",
    "    order = np.argsort(np.where(np.isnan(p), 1.1, p))  # NaNs at end\n",
    "    ranked_p = p[order]\n",
    "    crit = alpha * (np.arange(1, n+1) / n)\n",
    "    # only apply to non-NaN\n",
    "    ranked_p_nonan = ranked_p[~np.isnan(ranked_p)]\n",
    "    rejected = np.zeros_like(p, dtype=bool)\n",
    "    p_adj = np.full_like(p, np.nan, dtype=float)\n",
    "\n",
    "    # step-up\n",
    "    max_i = 0\n",
    "    for i in range(len(ranked_p_nonan)-1, -1, -1):\n",
    "        if ranked_p_nonan[i] <= crit[i]:\n",
    "            max_i = i + 1\n",
    "            break\n",
    "    # mark rejected\n",
    "    if max_i > 0:\n",
    "        rejected[order[:max_i]] = True\n",
    "\n",
    "    # compute adjusted p-values\n",
    "    # p_adj = min_{j>=i} (n/j * p_j)\n",
    "    adj_vals = np.empty_like(ranked_p_nonan)\n",
    "    min_val = 1.0\n",
    "    for i in range(len(ranked_p_nonan)-1, -1, -1):\n",
    "        val = (n/(i+1)) * ranked_p_nonan[i]\n",
    "        if val < min_val:\n",
    "            min_val = val\n",
    "        adj_vals[i] = min(min_val, 1.0)\n",
    "    # place back\n",
    "    p_adj_nonan = adj_vals\n",
    "    # fill in adjusted p for non-NaNs\n",
    "    p_adj_indices = order[:len(p_adj_nonan)]\n",
    "    p_adj[p_adj_indices] = p_adj_nonan\n",
    "\n",
    "    critical_value = crit[max_i-1] if max_i > 0 else 0.0\n",
    "    return rejected, p_adj, critical_value"
   ],
   "id": "f56363af47d1694b",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-10T10:05:47.441736Z",
     "start_time": "2025-11-10T10:05:47.374129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "BIG_FILE = Path(\"ttest_yeo/fmri_T1_clinical_merged_updated.csv\")\n",
    "CLUSTERS_FILE = Path(\"ttest_yeo/session1_clusters.csv\")\n",
    "OUTPUT_FILE = Path(\"ttest_yeo/bigfile_pruned_merged.csv\")\n",
    "\n",
    "\n",
    "\n",
    "big_df = pd.read_csv(BIG_FILE)\n",
    "clusters_df = pd.read_csv(CLUSTERS_FILE)\n",
    "\n",
    "big_df = normalize_columns(big_df)\n",
    "clusters_df = normalize_columns(clusters_df)\n",
    "\n",
    "columns_to_drop: List[str] = [\n",
    "    \"age_at_baseline\",\"b_ctq_total\",\"b_ctq_cutoff\",\"b_ctq_NEGLECT\",\"b_ctq_ABUSE\",\n",
    "    \"b_ctq_sexual_abuse\",\"b_ctq_physical_abuse\",\"b_ctq_emotional_abuse\",\"b_ctq_physical_neglect\",\n",
    "    \"b_ctq_emotional_neglect\",\"b_ctq_sexual_abuse_cutoff\",\"b_ctq_physical_abuse_cutoff\",\n",
    "    \"b_ctq_emotional_abuse_cutoff\",\"b_ctq_physical_neglect_cutoff\",\"b_ctq_emotional_neglect_cutoff\",\n",
    "    \"b_ctq_denial_score\",\"b_lec_0_to_16_total\",\"b_lec_interpersonal_events\",\"b_lec_non_interpersonal_events\",\n",
    "    \"b_strength_average\",\"b_PHQ_total\",\"b_GAD7_total\",\"b_social_support_total\",\"b_pcl_total\",\n",
    "    \"b_DERS_total\",\"b_DERS_Nonacceptance_Emotional_Responses\",\"b_DERS_Goal_Directed_Behavior\",\n",
    "    \"b_DERS_Impulse_Control\",\"b_DERS_Lack_Emotional_Awareness\",\"b_DERS_Emotion_Regulation_Strategies\",\n",
    "    \"b_DERS_Lack_Emotional_Clarity\",\"b_DES_average\",\"b_DES_Absorption\",\"b_DES_Amnesia\",\"b_DES_Depersonalization\",\n",
    "    \"b_LHQ_total\",\"b_PBI_mom_care\",\"b_PBI_mom_overprotection\",\"b_PBI_dad_care\",\"b_PBI_dad_overprotection\",\n",
    "    \"b_IRI_Perspective_Taking\",\"b_IRI_Empathic_Concern\",\"b_IRI_Personal_Distress\",\"b_IRI_Fantasy\",\n",
    "    \"T1_PHQ_total\",\"T2_PHQ_total\",\"T3_PHQ_total\",\"T1_GAD7_total\",\"T2_GAD7_total\",\"T3_GAD7_total\",\n",
    "    \"after_bits_PTSD_total\",\"after_bits_birth_symptoms\",\"after_bits_General_symptoms\",\"after_bits_Dissociatie_symptoms\",\n",
    "    \"after_bits_PTSD_criterion\",\"after_bits_Re_experiencing\",\"after_bits_Avoidance\",\"after_bits_Negative_Cognitions\",\n",
    "    \"after_bits_Hyperarousal\",\"after_MPAS_total\",\"after_MPAS_proximity\",\"after_MPAS_Acceptance\",\"after_MPAS_Tolerance\",\n",
    "    \"after_MPAS_Competence\",\"after_MPAS_Attachment\",\"after_MPAS_Hostility\",\"after_MPAS_Interaction\",\"after_CTQ_total\",\n",
    "    \"after_CTQ_cutoff\",\"after_CTQ_NEGLECT\",\"after_CTQ_ABUSE\",\"after_CTQ_sexual_abuse\",\"after_CTQ_physical_abuse\",\n",
    "    \"after_CTQ_emotional_abuse\",\"after_CTQ_physical_neglect\",\"after_CTQ_emotional_neglect\",\"after_CTQ_sexual_abuse_cutoff\",\n",
    "    \"after_CTQ_physical_abuse_cutoff\",\"after_CTQ_emotional_abuse_cutoff\",\"after_CTQ_physical_neglect_cutoff\",\n",
    "    \"after_CTQ_emotional_neglect_cutoff\",\"after_CTQ_denial_score\",\"after_DERS_total\",\n",
    "    \"after_DERS_Nonacceptance_Emotional_Responses\",\"after_DERS_Goal_Directed_Behavior\",\"after_DERS_Impulse_Control\",\n",
    "    \"after_DERS_Lack_Emotional_Awareness\",\"after_DERS_Emotion_Regulation_Strategies\",\"after_DERS_Lack_Emotional_Clarity\",\n",
    "    \"after_DES_total\",\"after_DES_Absorption\",\"after_DES_Amnesia\",\"after_DES_Depersonalization\",\n",
    "    \"VIS\",\"SMN\",\"DA\",\"VAN\",\"LIM\",\"FPN\",\"DMN\",\n",
    "    \"SMN-VIS\",\"DA-VIS\",\"VAN-VIS\",\"LIM-VIS\",\"FPN-VIS\",\"DMN-VIS\",\"DA-SMN\",\"VAN-SMN\",\"LIM-SMN\",\"FPN-SMN\",\"DMN-SMN\",\n",
    "    \"VAN-DA\",\"LIM-DAN\",\"FPN-DA\",\"DMN-DAN\",\"LIM-VAN\",\"FPN-VAN\",\"DMN-VAN\",\"FPN-LIM\",\"DMN-LIM\",\"DMN-FPN\",\n",
    "    \"VIS-Brain\",\"SMN-Brain\",\"DAN-Brain\",\"VAN-Brain\",\"LIM-Brain\",\"FPN-Brain\",\"DMN-Brain\",\n",
    "    \"seg_VIS\",\"seg_SMN\",\"seg_DAN\",\"seg_VAN\",\"seg_LIM\",\"seg_FPN\",\n",
    "    \"FD\",\"total_euler\",\"Clustering\",\"total_euler_before\",\"seg_DMN\"\n",
    "]\n",
    "\n",
    "# Normalize the drop list to match our normalization of columns\n",
    "columns_to_drop = list(pd.Index(columns_to_drop).map(lambda c: c.strip().replace(\" _\",\"_\").replace(\"_ \",\"_\")))\n",
    "\n",
    "present_to_drop = [c for c in columns_to_drop if c in big_df.columns]\n",
    "missing_to_drop = sorted(set(columns_to_drop) - set(present_to_drop))\n",
    "\n",
    "pruned_df = big_df.drop(columns=present_to_drop, errors=\"ignore\")\n",
    "\n",
    "# Use explicit columns we observed\n",
    "merge_key = \"Subject_Code\"\n",
    "cluster_col = \"kmeans_label\"\n",
    "\n",
    "merged_df = pruned_df.merge(\n",
    "    clusters_df[[merge_key, cluster_col]],\n",
    "    on=merge_key,\n",
    "    how=\"left\",\n",
    "    validate=\"m:1\"\n",
    ")\n",
    "\n",
    "# Save\n",
    "merged_df.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "# Show a compact summary and a preview\n",
    "summary = {\n",
    "    \"big_file_rows\": len(big_df),\n",
    "    \"big_file_cols_before\": len(big_df.columns),\n",
    "    \"dropped_columns_found\": len(present_to_drop),\n",
    "    \"dropped_columns_missing\": len(missing_to_drop),\n",
    "    \"big_file_cols_after_drop\": len(pruned_df.columns),\n",
    "    \"clusters_file_rows\": len(clusters_df),\n",
    "    \"clusters_file_cols\": len(clusters_df.columns),\n",
    "    \"merge_strategy\": \"key 'Subject_Code'\",\n",
    "    \"cluster_column_used\": cluster_col,\n",
    "    \"output_path\": str(OUTPUT_FILE),\n",
    "    \"output_rows\": len(merged_df),\n",
    "    \"output_cols\": len(merged_df.columns),\n",
    "    \"example_dropped_columns\": present_to_drop[:8],\n",
    "    \"example_missing_columns\": missing_to_drop[:8],\n",
    "}\n",
    "\n",
    "print(\"Summary:\", summary)\n",
    "print(merged_df.head(50))\n"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: {'big_file_rows': 117, 'big_file_cols_before': 267, 'dropped_columns_found': 138, 'dropped_columns_missing': 1, 'big_file_cols_after_drop': 129, 'clusters_file_rows': 115, 'clusters_file_cols': 2, 'merge_strategy': \"key 'Subject_Code'\", 'cluster_column_used': 'kmeans_label', 'output_path': 'ttest_yeo\\\\bigfile_pruned_merged.csv', 'output_rows': 117, 'output_cols': 130, 'example_dropped_columns': ['age_at_baseline', 'b_ctq_total', 'b_ctq_cutoff', 'b_ctq_NEGLECT', 'b_ctq_ABUSE', 'b_ctq_sexual_abuse', 'b_ctq_physical_abuse', 'b_ctq_emotional_abuse'], 'example_missing_columns': ['total_euler']}\n",
      "   Subject_Code         eTIV  Brain-Stem     CSF  Cingulate  Cingulate_lh  \\\n",
      "0         NT005  1440326.175     20336.4  1050.0      16031          7347   \n",
      "1         NT006  1567791.439     18653.6  1238.7      22877         12661   \n",
      "2         NT002  1410700.638     19351.4   846.2      19813          9761   \n",
      "3         NT003  1657960.938     23059.0  1091.7      24754         12000   \n",
      "4         NT007  1585405.714     17725.3   886.9      21564         11191   \n",
      "5         CT002  1563734.152     20198.9  1250.4      19677         10321   \n",
      "6         CT004  1564331.110     19347.2  1159.2      25123         12690   \n",
      "7         CT003  1500578.820     20283.0   961.8      20092         10279   \n",
      "8         NT010  1520452.463     21823.3   870.6      23741         12245   \n",
      "9         CT005  1320170.557     19999.1   783.1      17572          7822   \n",
      "10        NT013  1567441.815     18557.0   925.8      26592         13754   \n",
      "11        NT016  1277956.556     20738.3  1146.6      21021          9649   \n",
      "12        NT017  1492884.234     18643.8   921.3      18976         10531   \n",
      "13        NT020  1388022.096     17947.2   942.4      18965         10294   \n",
      "14        CT008  1507745.542     19432.5   763.3      16504          8785   \n",
      "15        NT019  1608704.817     20022.8  1014.9      20099         10175   \n",
      "16        CT007  1445751.389     22459.2  1009.4      21111         10137   \n",
      "17        NT015  1332513.825     20641.1   681.9      17589          9943   \n",
      "18        CT010  1447122.926     22405.1  1254.6      19751          9590   \n",
      "19        CT015  1523389.098     17787.8   868.7      26322         13586   \n",
      "20        CT013  1447602.868     19980.9  1101.0      22112         11215   \n",
      "21        CT012  1469667.400     20324.1   950.0      21030         10359   \n",
      "22        NT026  1296589.285     16540.3   734.8      18110          9104   \n",
      "23        NT029  1389670.672     19652.8   943.7      18944          9394   \n",
      "24        CT016  1495560.292     18385.2   796.0      19072          9742   \n",
      "25        NT028  1463122.368     20965.1  1273.2      23634         12422   \n",
      "26        NT032  1486055.171     21147.7  1229.8      18212          9142   \n",
      "27        NT035  1344752.493     18953.0   981.5      18365          9589   \n",
      "28        CT018  1449415.771     20238.4   879.5      18440          9849   \n",
      "29        NT036  1404362.831     20422.2   706.2      16916          8274   \n",
      "30        NT027  1501430.427     19618.3   891.7      20738         10421   \n",
      "31        NT033  1402738.953     18776.2  1017.6      19676         10874   \n",
      "32        CT020  1488916.000     21540.7  1071.7      19559          9774   \n",
      "33        NT031  1682579.568     22616.8  1024.0      22038         10534   \n",
      "34        NT025  1344094.401     17737.9   753.6      17716          9357   \n",
      "35        NT043  1583469.948     20588.1  1062.9      21388          9940   \n",
      "36        NT042  1531205.041     22956.9   736.0      20129         11583   \n",
      "37        NT038  1522764.226     19227.6  1186.5      22558         11356   \n",
      "38        NT040  1384093.712     19850.1   835.1      17957          8857   \n",
      "39        CT021  1504352.688     18323.4   997.7      20253          9780   \n",
      "40        NT048  1509689.722     20662.3  1094.9      23371         12581   \n",
      "41        NT049  1293345.802     16925.6   759.0      14860          8094   \n",
      "42        NT039  1409024.294     17537.1   980.3      18238          9192   \n",
      "43        CT023  1299119.958     19474.9  1048.8      18976          8934   \n",
      "44        NT055  1396995.319     20722.0  1109.2      19362          9834   \n",
      "45        NT056  1581884.892     21611.6   874.1      20719         11650   \n",
      "46        NT053  1589802.782     23191.1  1028.9      20467         10410   \n",
      "47        NT052  1612792.517     25202.1  1548.6      23296         12577   \n",
      "48        NT059  1605309.123     22636.9   902.3      21169          9973   \n",
      "49        CT017  1425369.679     19185.6  1020.6      20027          9592   \n",
      "\n",
      "    Cingulate_rh    CortexVol  DMN_GM  DAN_GM  ...  superiorparietal_rh  \\\n",
      "0           8684  458670.1565  136769   27225  ...                13876   \n",
      "1          10216  552172.7643  165192   28914  ...                14767   \n",
      "2          10052  479625.6700  143547   28123  ...                14705   \n",
      "3          12754  547404.5435  169448   30556  ...                15467   \n",
      "4          10373  573972.0240  173935   35641  ...                17361   \n",
      "5           9356  544619.1080  174423   33087  ...                16481   \n",
      "6          12433  554975.3921  172948   29889  ...                14536   \n",
      "7           9813  516867.6847  159872   30986  ...                15468   \n",
      "8          11496  521460.7690  169372   27596  ...                13267   \n",
      "9           9750  434623.3540  127745   23599  ...                11092   \n",
      "10         12838  557543.9986  168130   29253  ...                14927   \n",
      "11         11372  489491.9680  147137   30387  ...                14543   \n",
      "12          8445  493245.0585  154743   27022  ...                14215   \n",
      "13          8671  467738.2973  143547   22197  ...                10830   \n",
      "14          7719  476289.8879  143723   26563  ...                13438   \n",
      "15          9924  547473.5359  160620   28099  ...                13617   \n",
      "16         10974  471758.3582  142435   24517  ...                12564   \n",
      "17          7646  448756.9692  134090   22174  ...                11141   \n",
      "18         10161  463510.6878  144918   24280  ...                 9687   \n",
      "19         12736  540920.5197  165944   30170  ...                14650   \n",
      "20         10897  513230.3523  158513   28563  ...                14639   \n",
      "21         10671  491107.2088  150627   21960  ...                11306   \n",
      "22          9006  469593.8206  144586   25660  ...                13680   \n",
      "23          9550  465416.9232  143728   24856  ...                12792   \n",
      "24          9330  510926.4083  161499   29170  ...                14880   \n",
      "25         11212  534252.5913  159096   30203  ...                15907   \n",
      "26          9070  482901.5439  145931   30009  ...                15091   \n",
      "27          8776  447933.3843  139159   22897  ...                11967   \n",
      "28          8591  468342.0211  140986   27181  ...                13849   \n",
      "29          8642  459009.4706  134798   31176  ...                15193   \n",
      "30         10317  497418.4753  152743   29766  ...                14950   \n",
      "31          8802  464410.2794  139794   21103  ...                11557   \n",
      "32          9785  526393.9162  161824   28594  ...                13698   \n",
      "33         11504  554720.1681  166822   33396  ...                16855   \n",
      "34          8359  409000.0589  125798   24011  ...                11920   \n",
      "35         11448  534461.5996  166881   23411  ...                11033   \n",
      "36          8546  495790.1662  156332   28331  ...                14032   \n",
      "37         11202  536044.2257  161191   32050  ...                16207   \n",
      "38          9100  460023.3138  133270   28290  ...                15312   \n",
      "39         10473  502555.4729  152013   28319  ...                14691   \n",
      "40         10790  492798.7235  149562   26051  ...                13145   \n",
      "41          6766  426604.2665  128337   22185  ...                11221   \n",
      "42          9046  478512.9276  145601   20737  ...                 9894   \n",
      "43         10042  464319.5378  138024   24874  ...                12909   \n",
      "44          9528  455792.7219  138082   25916  ...                13203   \n",
      "45          9069  483016.9710  146650   23381  ...                11356   \n",
      "46         10057  522841.0305  155753   29862  ...                14550   \n",
      "47         10719  579888.2037  177757   29669  ...                14352   \n",
      "48         11196  476297.8463  149904   31656  ...                15913   \n",
      "49         10435  471910.3009  139081   28617  ...                13500   \n",
      "\n",
      "    superiortemporal_lh  superiortemporal_rh  supramarginal_lh  \\\n",
      "0                 12297                11343              9617   \n",
      "1                 16251                13864             14549   \n",
      "2                 12423                12143             10160   \n",
      "3                 15034                13084             14066   \n",
      "4                 13514                13650             14597   \n",
      "5                 15329                14010             14012   \n",
      "6                 14995                13529             11490   \n",
      "7                 12005                12525              9350   \n",
      "8                 13048                13487             12647   \n",
      "9                 12859                12023             10379   \n",
      "10                17567                15087             13104   \n",
      "11                13397                11844             14527   \n",
      "12                12584                12349             13257   \n",
      "13                14267                14084             10626   \n",
      "14                12528                11840             11322   \n",
      "15                15310                14127             13375   \n",
      "16                14181                13392              9010   \n",
      "17                11857                11117             10168   \n",
      "18                12144                11011             14061   \n",
      "19                12740                13502             10136   \n",
      "20                12909                12649              9635   \n",
      "21                15086                13262             11917   \n",
      "22                12771                11954             14199   \n",
      "23                12940                11756              9436   \n",
      "24                14990                12137             12048   \n",
      "25                16062                14483             15606   \n",
      "26                12647                10745             13051   \n",
      "27                13376                11457             12979   \n",
      "28                11154                11127             10043   \n",
      "29                11595                11838              7601   \n",
      "30                14442                12286             11231   \n",
      "31                12834                11321              9563   \n",
      "32                13715                13137             11108   \n",
      "33                14148                11820             11833   \n",
      "34                10599                10140              8639   \n",
      "35                13612                13827             13421   \n",
      "36                13490                11950             10296   \n",
      "37                14826                12490             16979   \n",
      "38                11976                11307             10937   \n",
      "39                14179                11747             11714   \n",
      "40                14207                12811             10295   \n",
      "41                11970                10742             10668   \n",
      "42                14795                12869              9986   \n",
      "43                13603                11805             11897   \n",
      "44                12083                10009             10470   \n",
      "45                13888                11087             12176   \n",
      "46                16918                13192             13371   \n",
      "47                16545                13660             13608   \n",
      "48                11415                10570             12542   \n",
      "49                12080                10654             11830   \n",
      "\n",
      "    supramarginal_rh  temporalpole_lh  temporalpole_rh  transversetemporal_lh  \\\n",
      "0              10700             1955             2082                   1253   \n",
      "1              12201             2190             2541                   1856   \n",
      "2              10382             2314             2215                    971   \n",
      "3              12609             2633             2607                   1289   \n",
      "4              16367             2762             2936                   1267   \n",
      "5              11521             1484             1787                   1362   \n",
      "6              10928             2404             2433                   1470   \n",
      "7               9664             3098             3175                   1042   \n",
      "8              10980             1202             1656                   1277   \n",
      "9              10633             2341             2682                   1139   \n",
      "10             12403             2304             2790                   1590   \n",
      "11             10794             2629             1919                   1596   \n",
      "12             11731             2313             2290                   1149   \n",
      "13             11599             2638             2337                   1095   \n",
      "14             11672             2694             2204                   1284   \n",
      "15             11887             2757             2143                   1738   \n",
      "16              9001             2283             2032                   1247   \n",
      "17              9128             2136             2344                    939   \n",
      "18              7681             2383             2052                   1067   \n",
      "19             13010             2146             2693                   1181   \n",
      "20             10462             3013             2523                   1317   \n",
      "21              8799             3208             2803                   1187   \n",
      "22              9046             2475             1797                   1254   \n",
      "23              9462             3052             2995                   1302   \n",
      "24              9453             1778             2087                   1242   \n",
      "25             14120             2875             2550                   1505   \n",
      "26             11148             2403             2077                   1133   \n",
      "27              8654             2687             2064                   1719   \n",
      "28             11581             2525             1942                   1326   \n",
      "29              8965             2329             2513                   1162   \n",
      "30             10232             2269             2126                   1125   \n",
      "31              9240             2538             2349                    998   \n",
      "32             10160             2070             2849                   1350   \n",
      "33             11335             3539             2454                   1302   \n",
      "34              8704             1983             1731                    934   \n",
      "35             11967             2342             2398                   1299   \n",
      "36              8048             1191             1740                   1319   \n",
      "37             11870             3240             2853                   1355   \n",
      "38             11410             2975             3218                   1222   \n",
      "39             10601             2869             2709                   1295   \n",
      "40              8468             3184             2503                   1069   \n",
      "41              8873             2369             2618                   1138   \n",
      "42              7098             2670             2993                   1120   \n",
      "43              9790             2891             2315                   1185   \n",
      "44              9576             2325             2609                   1649   \n",
      "45              8622             2762             2727                   1290   \n",
      "46             11969             2697             2416                   1688   \n",
      "47             10530             3505             3354                   1639   \n",
      "48             12032             1403             2301                   1216   \n",
      "49             10443             2422             2657                   1244   \n",
      "\n",
      "    transversetemporal_rh  kmeans_label  \n",
      "0                     907           0.0  \n",
      "1                    1237           0.0  \n",
      "2                     781           0.0  \n",
      "3                     883           0.0  \n",
      "4                    1142           0.0  \n",
      "5                     904           1.0  \n",
      "6                    1058           1.0  \n",
      "7                     776           0.0  \n",
      "8                    1189           1.0  \n",
      "9                     838           1.0  \n",
      "10                   1145           0.0  \n",
      "11                   1094           0.0  \n",
      "12                   1007           0.0  \n",
      "13                    878           1.0  \n",
      "14                    880           0.0  \n",
      "15                   1077           0.0  \n",
      "16                    855           0.0  \n",
      "17                    656           1.0  \n",
      "18                    799           0.0  \n",
      "19                    868           0.0  \n",
      "20                    911           0.0  \n",
      "21                    901           1.0  \n",
      "22                    765           0.0  \n",
      "23                    916           0.0  \n",
      "24                    957           0.0  \n",
      "25                   1044           1.0  \n",
      "26                    923           0.0  \n",
      "27                    739           1.0  \n",
      "28                    907           0.0  \n",
      "29                    965           1.0  \n",
      "30                   1052           1.0  \n",
      "31                    900           1.0  \n",
      "32                   1157           0.0  \n",
      "33                    995           1.0  \n",
      "34                    815           0.0  \n",
      "35                   1032           0.0  \n",
      "36                    822           1.0  \n",
      "37                   1010           0.0  \n",
      "38                    910           1.0  \n",
      "39                   1033           1.0  \n",
      "40                    999           0.0  \n",
      "41                    758           0.0  \n",
      "42                   1017           1.0  \n",
      "43                    901           1.0  \n",
      "44                    972           0.0  \n",
      "45                    867           0.0  \n",
      "46                   1034           0.0  \n",
      "47                   1139           0.0  \n",
      "48                    698           1.0  \n",
      "49                    945           0.0  \n",
      "\n",
      "[50 rows x 130 columns]\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T10:05:47.460357Z",
     "start_time": "2025-11-10T10:05:47.456824Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "1754f44220d7db37",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "3rd_ventricle\n",
    "4th_ventricle\n",
    "5th_ventricle\n",
    "brain_stem\n",
    "cc_anterior\n",
    "cc_central\n",
    "cc_mid_anterior\n",
    "cc_mid_posterior\n",
    "cc_posterior\n",
    "csf\n",
    "left_accumbens_area\n",
    "left_amygdala\n",
    "left_caudate\n",
    "left_cerebellum_cortex\n",
    "left_cerebellum_white_matter\n",
    "left_choroid_plexus\n",
    "left_hippocampus\n",
    "left_inf_lat_vent\n",
    "left_lateral_ventricle\n",
    "left_non_wm_hypointensities\n",
    "left_pallidum\n",
    "left_putamen\n",
    "left_thalamus\n",
    "left_ventraldc\n",
    "left_vessel\n",
    "left_wm_hypointensities\n",
    "non_wm_hypointensities\n",
    "optic_chiasm\n",
    "right_accumbens_area\n",
    "right_amygdala\n",
    "right_caudate\n",
    "right_cerebellum_cortex\n",
    "right_cerebellum_white_matter\n",
    "right_choroid_plexus\n",
    "right_hippocampus\n",
    "right_inf_lat_vent\n",
    "right_lateral_ventricle\n",
    "right_non_wm_hypointensities\n",
    "right_pallidum\n",
    "right_putamen\n",
    "right_thalamus\n",
    "right_ventraldc\n",
    "right_vessel\n",
    "right_wm_hypointensities\n",
    "wm_hypointensities\n"
   ],
   "id": "dd07f329e2ac99e4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T10:05:47.739113Z",
     "start_time": "2025-11-10T10:05:47.576938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Let's diagnose why your columns weren't being matched and fix it.\n",
    "# We will:\n",
    "# 1) Load the merged CSV\n",
    "# 2) Build a normalization that turns names like \"Left-Accumbens-area_ses1\" into \"left_accumbens_area\"\n",
    "#    (lowercase, replace '-' and spaces with '_', collapse repeats, drop _ses1/_ses2)\n",
    "# 3) Show a mapping of original -> normalized so you can see what's happening\n",
    "# 4) Provide a selector that matches your requested base names against these normalized names\n",
    "# 5) Re-run the subcortical t-tests/correlations using the robust mapping\n",
    "\n",
    "\n",
    "DATA_FILE = Path(\"ttest_yeo/bigfile_pruned_merged.csv\")\n",
    "GROUP_COL = \"kmeans_label\"\n",
    "\n",
    "# Your base names (underscored, no session suffix)\n",
    "base_features = [\n",
    "    \"3rd_ventricle\",\"4th_ventricle\",\"5th_ventricle\",\"brain_stem\",\"cc_anterior\",\"cc_central\",\n",
    "    \"cc_mid_anterior\",\"cc_mid_posterior\",\"cc_posterior\",\"csf\",\"left_accumbens_area\",\"left_amygdala\",\n",
    "    \"left_caudate\",\"left_cerebellum_cortex\",\"left_cerebellum_white_matter\",\"left_choroid_plexus\",\n",
    "    \"left_hippocampus\",\"left_inf_lat_vent\",\"left_lateral_ventricle\",\"left_non_wm_hypointensities\",\n",
    "    \"left_pallidum\",\"left_putamen\",\"left_thalamus\",\"left_ventraldc\",\"left_vessel\",\"left_wm_hypointensities\",\n",
    "    \"non_wm_hypointensities\",\"optic_chiasm\",\"right_accumbens_area\",\"right_amygdala\",\"right_caudate\",\n",
    "    \"right_cerebellum_cortex\",\"right_cerebellum_white_matter\",\"right_choroid_plexus\",\"right_hippocampus\",\n",
    "    \"right_inf_lat_vent\",\"right_lateral_ventricle\",\"right_non_wm_hypointensities\",\"right_pallidum\",\n",
    "    \"right_putamen\",\"right_thalamus\",\"right_ventraldc\",\"right_vessel\",\"right_wm_hypointensities\",\n",
    "    \"wm_hypointensities\"\n",
    "]\n",
    "\n",
    "def normalize_name(name: str) -> str:\n",
    "    s = name.strip()\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)      # spaces -> _\n",
    "    s = s.replace(\"-\", \"_\")         # hyphens -> _\n",
    "    s = s.replace(\".\", \"_\")         # dots -> _\n",
    "    s = re.sub(r\"_+\", \"_\", s)       # collapse multiple _\n",
    "    s = s.lower()\n",
    "    # drop session suffixes\n",
    "    s = re.sub(r\"(_ses\\s*1|_ses\\s*2|_ses1|_ses2)$\", \"\", s)\n",
    "    # consistent lh/rh casing already lower\n",
    "    return s\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "\n",
    "if GROUP_COL not in df.columns:\n",
    "    raise ValueError(f\"{GROUP_COL} not in file.\")\n",
    "\n",
    "# Build original->normalized and normalized->list(originals) maps\n",
    "orig_cols = df.columns.tolist()\n",
    "norm_cols = [normalize_name(c) for c in orig_cols]\n",
    "\n",
    "map_orig_to_norm: Dict[str, str] = dict(zip(orig_cols, norm_cols))\n",
    "\n",
    "# Build a \"best original\" picker for each normalized name:\n",
    "# - prefer originals ending with _ses1\n",
    "# - else originals ending with _ses2\n",
    "# - else the first seen\n",
    "pref: Dict[str, str] = {}\n",
    "for orig, norm in zip(orig_cols, norm_cols):\n",
    "    if norm not in pref:\n",
    "        pref[norm] = orig\n",
    "    # prefer *_ses1\n",
    "    if re.search(r\"_ses1$\", normalize_name(orig)):\n",
    "        pref[norm] = orig\n",
    "    # if current not ses1 and new is ses1, replace; if current neither and new is ses2, consider only if no ses1 seen\n",
    "    # (already covered by above)\n",
    "\n",
    "# Make a mapping table so you can inspect\n",
    "mapping_table = pd.DataFrame({\n",
    "    \"original\": orig_cols,\n",
    "    \"normalized\": norm_cols\n",
    "}).sort_values([\"normalized\",\"original\"])\n",
    "\n",
    "print(f\"Column name mapping (original -> normalized)  {mapping_table.head(200)}\")\n",
    "\n",
    "# Now map requested base names to selected original columns using normalized keys\n",
    "selected_originals: List[str] = []\n",
    "selection_reason: List[Tuple[str,str,str]] = []  # (base, original, reason)\n",
    "for base in base_features:\n",
    "    n = normalize_name(base)\n",
    "    if n in pref:\n",
    "        selected_originals.append(pref[n])\n",
    "        selection_reason.append((base, pref[n], \"matched_normalized\"))\n",
    "    else:\n",
    "        selection_reason.append((base, \"\", \"not_found\"))\n",
    "\n",
    "selection_df = pd.DataFrame(selection_reason, columns=[\"base_name\",\"selected_original\",\"reason\"])\n",
    "print(f\"Requested base names -> selected original column  {selection_df} \")\n",
    "\n",
    "# Helper stats functions\n",
    "def welch_ttest(x: np.ndarray, y: np.ndarray):\n",
    "    if len(x) < 2 or len(y) < 2:\n",
    "        return np.nan, np.nan\n",
    "    t, p = stats.ttest_ind(x, y, equal_var=False, nan_policy=\"omit\")\n",
    "    return float(t), float(p)\n",
    "\n",
    "def cohens_d(x: np.ndarray, y: np.ndarray) -> float:\n",
    "    nx, ny = len(x), len(y)\n",
    "    if nx < 2 or ny < 2:\n",
    "        return np.nan\n",
    "    vx, vy = x.var(ddof=1), y.var(ddof=1)\n",
    "    s = np.sqrt(((nx-1)*vx + (ny-1)*vy) / (nx + ny - 2))\n",
    "    if s == 0:\n",
    "        return 0.0\n",
    "    return float((x.mean() - y.mean()) / s)\n",
    "\n",
    "def fdr_bh(pvals: np.ndarray, alpha: float = 0.05):\n",
    "    p = np.array(pvals, dtype=float)\n",
    "    n = np.sum(~np.isnan(p))\n",
    "    if n == 0:\n",
    "        return np.array([False]*len(p)), np.array([np.nan]*len(p)), 0.0\n",
    "    order = np.argsort(np.where(np.isnan(p), 1.1, p))\n",
    "    ranked_p = p[order]\n",
    "    ranked_nonan = ranked_p[~np.isnan(ranked_p)]\n",
    "    crit = alpha * (np.arange(1, len(ranked_nonan)+1) / len(ranked_nonan))\n",
    "\n",
    "    rejected = np.zeros_like(p, dtype=bool)\n",
    "    p_adj = np.full_like(p, np.nan, dtype=float)\n",
    "\n",
    "    max_i = 0\n",
    "    for i in range(len(ranked_nonan)-1, -1, -1):\n",
    "        if ranked_nonan[i] <= crit[i]:\n",
    "            max_i = i + 1\n",
    "            break\n",
    "    if max_i > 0:\n",
    "        rejected[order[:max_i]] = True\n",
    "\n",
    "    adj = np.empty_like(ranked_nonan)\n",
    "    min_val = 1.0\n",
    "    for i in range(len(ranked_nonan)-1, -1, -1):\n",
    "        val = (len(ranked_nonan)/(i+1)) * ranked_nonan[i]\n",
    "        if val < min_val:\n",
    "            min_val = val\n",
    "        adj[i] = min(min_val, 1.0)\n",
    "\n",
    "    p_adj_indices = order[:len(adj)]\n",
    "    p_adj[p_adj_indices] = adj\n",
    "    crit_val = crit[max_i-1] if max_i > 0 else 0.0\n",
    "    return rejected, p_adj, crit_val\n",
    "\n",
    "# Proceed with tests if we found any selected columns\n",
    "if len(selected_originals) == 0:\n",
    "    print(\"None of the requested base names matched available columns after normalization.\")\n",
    "else:\n",
    "    # ensure binary groups\n",
    "    g = df[GROUP_COL]\n",
    "    if g.dtype.kind not in \"biu\":\n",
    "        try:\n",
    "            g = g.astype(int)\n",
    "        except Exception:\n",
    "            uniq = sorted(g.dropna().unique().tolist())\n",
    "            if len(uniq) != 2:\n",
    "                raise ValueError(f\"{GROUP_COL} must have 2 groups, found: {uniq}\")\n",
    "            g = g.map({uniq[0]:0, uniq[1]:1})\n",
    "    df[GROUP_COL] = g\n",
    "    uniq_g = sorted(df[GROUP_COL].dropna().unique().tolist())\n",
    "    g0, g1 = uniq_g[0], uniq_g[1]\n",
    "\n",
    "    # T-tests\n",
    "    rows = []\n",
    "    for col in selected_originals:\n",
    "        x = df.loc[df[GROUP_COL]==g0, col].astype(float).dropna().values\n",
    "        y = df.loc[df[GROUP_COL]==g1, col].astype(float).dropna().values\n",
    "        t, p = welch_ttest(x, y)\n",
    "        d = cohens_d(x, y)\n",
    "        rows.append({\n",
    "            \"feature_original\": col,\n",
    "            \"feature_normalized\": normalize_name(col),\n",
    "            f\"n_{g0}\": len(x), f\"mean_{g0}\": float(np.mean(x)) if len(x) else np.nan, f\"sd_{g0}\": float(np.std(x, ddof=1)) if len(x) else np.nan,\n",
    "            f\"n_{g1}\": len(y), f\"mean_{g1}\": float(np.mean(y)) if len(y) else np.nan, f\"sd_{g1}\": float(np.std(y, ddof=1)) if len(y) else np.nan,\n",
    "            \"t_stat\": t, \"p_value\": p, \"cohens_d\": d\n",
    "        })\n",
    "    ttest_df = pd.DataFrame(rows).sort_values(\"p_value\", na_position=\"last\")\n",
    "    rej, p_adj, crit = fdr_bh(ttest_df[\"p_value\"].values)\n",
    "    ttest_df[\"p_value_fdr_bh\"] = p_adj\n",
    "    ttest_df[\"significant_fdr_0.05\"] = rej\n",
    "\n",
    "    # Correlations\n",
    "    corr_rows = []\n",
    "    gg = df[GROUP_COL].astype(float)\n",
    "    for col in selected_originals:\n",
    "        xx = df[col].astype(float)\n",
    "        m = gg.notna() & xx.notna()\n",
    "        if m.sum() < 3:\n",
    "            r, p = (np.nan, np.nan)\n",
    "        else:\n",
    "            r, p = stats.pearsonr(gg[m], xx[m])\n",
    "        corr_rows.append({\"feature_original\": col, \"feature_normalized\": normalize_name(col), \"r_pointbiserial\": r, \"p_value\": p})\n",
    "    corr_df = pd.DataFrame(corr_rows).sort_values(\"p_value\", na_position=\"last\")\n",
    "    rej_c, p_adj_c, crit_c = fdr_bh(corr_df[\"p_value\"].values)\n",
    "    corr_df[\"p_value_fdr_bh\"] = p_adj_c\n",
    "    corr_df[\"significant_fdr_0.05\"] = rej_c\n",
    "\n",
    "    # Save and display\n",
    "    t_out = Path(\"ttest_yeo/ttest_subcortical_normalized.csv\")\n",
    "    c_out = Path(\"ttest_yeo/corr_subcortical_normalized.csv\")\n",
    "    map_out = Path(\"ttest_yeo/column_mapping_normalized.csv\")\n",
    "    ttest_df.to_csv(t_out, index=False)\n",
    "    corr_df.to_csv(c_out, index=False)\n",
    "    mapping_table.to_csv(map_out, index=False)\n",
    "\n",
    "    print(f\"T-tests (normalized mapping) {ttest_df}\")\n",
    "    print(f\"Correlations (normalized mapping) {corr_df}\")\n",
    "\n",
    "    print({\n",
    "        \"downloads\": {\n",
    "            \"ttest_results_csv\": str(t_out),\n",
    "            \"corr_results_csv\": str(c_out),\n",
    "            \"column_mapping_csv\": str(map_out)\n",
    "        },\n",
    "        \"tested_feature_count\": len(selected_originals)\n",
    "    })\n"
   ],
   "id": "100a0786076df825",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column name mapping (original -> normalized)                        original                  normalized\n",
      "56                 bankssts_lh                 bankssts_lh\n",
      "57                 bankssts_rh                 bankssts_rh\n",
      "2                   Brain-Stem                  brain_stem\n",
      "58  caudalanteriorcingulate_lh  caudalanteriorcingulate_lh\n",
      "59  caudalanteriorcingulate_rh  caudalanteriorcingulate_rh\n",
      "..                         ...                         ...\n",
      "51                      VAN_GM                      van_gm\n",
      "52                      VIS_GM                      vis_gm\n",
      "55               WhiteSurfArea               whitesurfarea\n",
      "53            WhiteSurfArea_lh            whitesurfarea_lh\n",
      "54            WhiteSurfArea_rh            whitesurfarea_rh\n",
      "\n",
      "[130 rows x 2 columns]\n",
      "Requested base names -> selected original column                          base_name        selected_original              reason\n",
      "0                   3rd_ventricle                                    not_found\n",
      "1                   4th_ventricle                                    not_found\n",
      "2                   5th_ventricle                                    not_found\n",
      "3                      brain_stem               Brain-Stem  matched_normalized\n",
      "4                     cc_anterior                                    not_found\n",
      "5                      cc_central                                    not_found\n",
      "6                 cc_mid_anterior                                    not_found\n",
      "7                cc_mid_posterior                                    not_found\n",
      "8                    cc_posterior                                    not_found\n",
      "9                             csf                      CSF  matched_normalized\n",
      "10            left_accumbens_area      Left-Accumbens-area  matched_normalized\n",
      "11                  left_amygdala            Left-Amygdala  matched_normalized\n",
      "12                   left_caudate             Left-Caudate  matched_normalized\n",
      "13         left_cerebellum_cortex   Left-Cerebellum-Cortex  matched_normalized\n",
      "14   left_cerebellum_white_matter                                    not_found\n",
      "15            left_choroid_plexus                                    not_found\n",
      "16               left_hippocampus         Left-Hippocampus  matched_normalized\n",
      "17              left_inf_lat_vent                                    not_found\n",
      "18         left_lateral_ventricle                                    not_found\n",
      "19    left_non_wm_hypointensities                                    not_found\n",
      "20                  left_pallidum            Left-Pallidum  matched_normalized\n",
      "21                   left_putamen             Left-Putamen  matched_normalized\n",
      "22                  left_thalamus            Left-Thalamus  matched_normalized\n",
      "23                 left_ventraldc           Left-VentralDC  matched_normalized\n",
      "24                    left_vessel                                    not_found\n",
      "25        left_wm_hypointensities                                    not_found\n",
      "26         non_wm_hypointensities                                    not_found\n",
      "27                   optic_chiasm                                    not_found\n",
      "28           right_accumbens_area     Right-Accumbens-area  matched_normalized\n",
      "29                 right_amygdala           Right-Amygdala  matched_normalized\n",
      "30                  right_caudate            Right-Caudate  matched_normalized\n",
      "31        right_cerebellum_cortex  Right-Cerebellum-Cortex  matched_normalized\n",
      "32  right_cerebellum_white_matter                                    not_found\n",
      "33           right_choroid_plexus                                    not_found\n",
      "34              right_hippocampus        Right-Hippocampus  matched_normalized\n",
      "35             right_inf_lat_vent                                    not_found\n",
      "36        right_lateral_ventricle                                    not_found\n",
      "37   right_non_wm_hypointensities                                    not_found\n",
      "38                 right_pallidum           Right-Pallidum  matched_normalized\n",
      "39                  right_putamen            Right-Putamen  matched_normalized\n",
      "40                 right_thalamus           Right-Thalamus  matched_normalized\n",
      "41                right_ventraldc          Right-VentralDC  matched_normalized\n",
      "42                   right_vessel                                    not_found\n",
      "43       right_wm_hypointensities                                    not_found\n",
      "44             wm_hypointensities                                    not_found \n",
      "T-tests (normalized mapping)            feature_original       feature_normalized  n_0.0      mean_0.0  \\\n",
      "15        Right-Hippocampus        right_hippocampus     51   4391.400000   \n",
      "12           Right-Amygdala           right_amygdala     51   1766.601961   \n",
      "8              Left-Putamen             left_putamen     51   4915.821569   \n",
      "6          Left-Hippocampus         left_hippocampus     51   4212.925490   \n",
      "9             Left-Thalamus            left_thalamus     51   7682.882353   \n",
      "3             Left-Amygdala            left_amygdala     51   1651.996078   \n",
      "1                       CSF                      csf     51    987.415686   \n",
      "7             Left-Pallidum            left_pallidum     51   2009.343137   \n",
      "16           Right-Pallidum           right_pallidum     51   1957.239216   \n",
      "17            Right-Putamen            right_putamen     51   5037.913725   \n",
      "2       Left-Accumbens-area      left_accumbens_area     51    477.452941   \n",
      "18           Right-Thalamus           right_thalamus     51   7497.694118   \n",
      "14  Right-Cerebellum-Cortex  right_cerebellum_cortex     51  54865.166667   \n",
      "11     Right-Accumbens-area     right_accumbens_area     51    610.027451   \n",
      "0                Brain-Stem               brain_stem     51  20369.239216   \n",
      "4              Left-Caudate             left_caudate     51   3572.707843   \n",
      "5    Left-Cerebellum-Cortex   left_cerebellum_cortex     51  55270.541176   \n",
      "13            Right-Caudate            right_caudate     51   3604.317647   \n",
      "10           Left-VentralDC           left_ventraldc     51   3943.158824   \n",
      "19          Right-VentralDC          right_ventraldc     51   3927.380392   \n",
      "\n",
      "         sd_0.0  n_1.0      mean_1.0       sd_1.0    t_stat   p_value  \\\n",
      "15   431.739991     64   4288.575000   332.329883  1.401796  0.164338   \n",
      "12   186.436980     64   1723.328125   169.439175  1.287244  0.200914   \n",
      "8    480.123880     64   4827.296875   396.050540  1.060282  0.291665   \n",
      "6    427.939956     64   4143.518750   327.601496  0.956289  0.341440   \n",
      "9    691.230841     64   7582.039062   569.025126  0.839550  0.403240   \n",
      "3    200.243362     64   1621.607812   201.327197  0.806550  0.421705   \n",
      "1    191.551754     64    963.917187   178.972332  0.672777  0.502583   \n",
      "7    174.099290     64   1987.581250   187.107223  0.644152  0.520814   \n",
      "16   176.861163     64   1934.837500   197.875453  0.640017  0.523475   \n",
      "17   478.870199     64   4991.942187   404.337402  0.547477  0.585297   \n",
      "2     83.183230     64    468.815625    89.352916  0.535227  0.593571   \n",
      "18   656.106898     64   7438.820312   634.585113  0.485041  0.628651   \n",
      "14  5015.699433     64  55222.295312  4843.044428 -0.385154  0.700898   \n",
      "11    89.691605     64    603.737500    98.195945  0.358171  0.720897   \n",
      "0   1866.879840     64  20475.504687  1625.848407 -0.320927  0.748937   \n",
      "4    399.393656     64   3556.251562   406.589930  0.217762  0.828025   \n",
      "5   5196.466849     64  55465.901563  4837.106636 -0.206495  0.836809   \n",
      "13   399.813713     64   3617.189063   409.523131 -0.169673  0.865584   \n",
      "10   283.086470     64   3949.559375   263.695006 -0.124152  0.901435   \n",
      "19   305.323508     64   3933.839063   301.327661 -0.113352  0.909965   \n",
      "\n",
      "    cohens_d  p_value_fdr_bh  significant_fdr_0.05  \n",
      "15  0.270919        0.909965                 False  \n",
      "12  0.244262        0.909965                 False  \n",
      "8   0.203384        0.909965                 False  \n",
      "6   0.184926        0.909965                 False  \n",
      "9   0.161079        0.909965                 False  \n",
      "3   0.151300        0.909965                 False  \n",
      "1   0.127264        0.909965                 False  \n",
      "7   0.119922        0.909965                 False  \n",
      "16  0.118612        0.909965                 False  \n",
      "17  0.104747        0.909965                 False  \n",
      "2   0.099649        0.909965                 False  \n",
      "18  0.091391        0.909965                 False  \n",
      "14 -0.072584        0.909965                 False  \n",
      "11  0.066541        0.909965                 False  \n",
      "0  -0.061191        0.909965                 False  \n",
      "4   0.040792        0.909965                 False  \n",
      "5  -0.039078        0.909965                 False  \n",
      "13 -0.031761        0.909965                 False  \n",
      "10 -0.023493        0.909965                 False  \n",
      "19 -0.021309        0.909965                 False  \n",
      "Correlations (normalized mapping)            feature_original       feature_normalized  r_pointbiserial  \\\n",
      "15        Right-Hippocampus        right_hippocampus        -0.134542   \n",
      "12           Right-Amygdala           right_amygdala        -0.121510   \n",
      "8              Left-Putamen             left_putamen        -0.101405   \n",
      "6          Left-Hippocampus         left_hippocampus        -0.092284   \n",
      "9             Left-Thalamus            left_thalamus        -0.080467   \n",
      "3             Left-Amygdala            left_amygdala        -0.075610   \n",
      "1                       CSF                      csf        -0.063652   \n",
      "7             Left-Pallidum            left_pallidum        -0.059993   \n",
      "16           Right-Pallidum           right_pallidum        -0.059340   \n",
      "17            Right-Putamen            right_putamen        -0.052424   \n",
      "2       Left-Accumbens-area      left_accumbens_area        -0.049879   \n",
      "18           Right-Thalamus           right_thalamus        -0.045755   \n",
      "14  Right-Cerebellum-Cortex  right_cerebellum_cortex         0.036353   \n",
      "11     Right-Accumbens-area     right_accumbens_area        -0.033330   \n",
      "0                Brain-Stem               brain_stem         0.030653   \n",
      "4              Left-Caudate             left_caudate        -0.020439   \n",
      "5    Left-Cerebellum-Cortex   left_cerebellum_cortex         0.019581   \n",
      "13            Right-Caudate            right_caudate         0.015916   \n",
      "10           Left-VentralDC           left_ventraldc         0.011773   \n",
      "19          Right-VentralDC          right_ventraldc         0.010679   \n",
      "\n",
      "     p_value  p_value_fdr_bh  significant_fdr_0.05  \n",
      "15  0.151695        0.909818                 False  \n",
      "12  0.195798        0.909818                 False  \n",
      "8   0.280878        0.909818                 False  \n",
      "6   0.326632        0.909818                 False  \n",
      "9   0.392624        0.909818                 False  \n",
      "3   0.421905        0.909818                 False  \n",
      "1   0.499157        0.909818                 False  \n",
      "7   0.524187        0.909818                 False  \n",
      "16  0.528723        0.909818                 False  \n",
      "17  0.577918        0.909818                 False  \n",
      "2   0.596540        0.909818                 False  \n",
      "18  0.627280        0.909818                 False  \n",
      "14  0.699709        0.909818                 False  \n",
      "11  0.723625        0.909818                 False  \n",
      "0   0.745031        0.909818                 False  \n",
      "4   0.828351        0.909818                 False  \n",
      "5   0.835458        0.909818                 False  \n",
      "13  0.865935        0.909818                 False  \n",
      "10  0.900620        0.909818                 False  \n",
      "19  0.909818        0.909818                 False  \n",
      "{'downloads': {'ttest_results_csv': 'ttest_yeo\\\\ttest_subcortical_normalized.csv', 'corr_results_csv': 'ttest_yeo\\\\corr_subcortical_normalized.csv', 'column_mapping_csv': 'ttest_yeo\\\\column_mapping_normalized.csv'}, 'tested_feature_count': 20}\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T10:05:48.030808Z",
     "start_time": "2025-11-10T10:05:47.751517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Let's diagnose why your columns weren't being matched and fix it.\n",
    "# We will:\n",
    "# 1) Load the merged CSV\n",
    "# 2) Build a normalization that turns names like \"Left-Accumbens-area_ses1\" into \"left_accumbens_area\"\n",
    "#    (lowercase, replace '-' and spaces with '_', collapse repeats, drop _ses1/_ses2)\n",
    "# 3) Show a mapping of original -> normalized so you can see what's happening\n",
    "# 4) Provide a selector that matches your requested base names against these normalized names\n",
    "# 5) Re-run the subcortical t-tests/correlations using the robust mapping\n",
    "\n",
    "\n",
    "DATA_FILE = Path(\"ttest_yeo/bigfile_pruned_merged.csv\")\n",
    "GROUP_COL = \"kmeans_label\"\n",
    "\n",
    "# Your base names (underscored, no session suffix)\n",
    "base_features= [\n",
    "    'bankssts_lh', 'bankssts_rh', 'caudalanteriorcingulate_lh', 'caudalanteriorcingulate_rh',\n",
    "    'caudalmiddlefrontal_lh', 'caudalmiddlefrontal_rh', 'cuneus_lh', 'cuneus_rh',\n",
    "    'entorhinal_lh', 'entorhinal_rh', 'frontalpole_lh', 'frontalpole_rh',\n",
    "    'fusiform_lh', 'fusiform_rh', 'inferiorparietal_lh', 'inferiorparietal_rh',\n",
    "    'inferiortemporal_lh', 'inferiortemporal_rh', 'insula_lh', 'insula_rh',\n",
    "    'isthmuscingulate_lh', 'isthmuscingulate_rh', 'lateraloccipital_lh', 'lateraloccipital_rh',\n",
    "    'lateralorbitofrontal_lh', 'lateralorbitofrontal_rh', 'lingual_lh', 'lingual_rh',\n",
    "    'medialorbitofrontal_lh', 'medialorbitofrontal_rh', 'middletemporal_lh', 'middletemporal_rh',\n",
    "    'paracentral_lh', 'paracentral_rh', 'parahippocampal_lh', 'parahippocampal_rh',\n",
    "    'parsopercularis_lh', 'parsopercularis_rh', 'parsorbitalis_lh', 'parsorbitalis_rh',\n",
    "    'parstriangularis_lh', 'parstriangularis_rh', 'pericalcarine_lh', 'pericalcarine_rh',\n",
    "    'postcentral_lh', 'postcentral_rh', 'posteriorcingulate_lh', 'posteriorcingulate_rh',\n",
    "    'precentral_lh', 'precentral_rh', 'precuneus_lh', 'precuneus_rh',\n",
    "    'rostralanteriorcingulate_lh', 'rostralanteriorcingulate_rh',\n",
    "    'rostralmiddlefrontal_lh', 'rostralmiddlefrontal_rh',\n",
    "    'superiorfrontal_lh', 'superiorfrontal_rh', 'superiorparietal_lh', 'superiorparietal_rh',\n",
    "    'superiortemporal_lh', 'superiortemporal_rh', 'supramarginal_lh', 'supramarginal_rh',\n",
    "    'temporalpole_lh', 'temporalpole_rh', 'transversetemporal_lh', 'transversetemporal_rh'\n",
    "]\n",
    "\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "\n",
    "if GROUP_COL not in df.columns:\n",
    "    raise ValueError(f\"{GROUP_COL} not in file.\")\n",
    "\n",
    "# Build original->normalized and normalized->list(originals) maps\n",
    "orig_cols = df.columns.tolist()\n",
    "norm_cols = [normalize_name(c) for c in orig_cols]\n",
    "\n",
    "map_orig_to_norm: Dict[str, str] = dict(zip(orig_cols, norm_cols))\n",
    "\n",
    "# Build a \"best original\" picker for each normalized name:\n",
    "# - prefer originals ending with _ses1\n",
    "# - else originals ending with _ses2\n",
    "# - else the first seen\n",
    "pref: Dict[str, str] = {}\n",
    "for orig, norm in zip(orig_cols, norm_cols):\n",
    "    if norm not in pref:\n",
    "        pref[norm] = orig\n",
    "    # prefer *_ses1\n",
    "    if re.search(r\"_ses1$\", normalize_name(orig)):\n",
    "        pref[norm] = orig\n",
    "    # if current not ses1 and new is ses1, replace; if current neither and new is ses2, consider only if no ses1 seen\n",
    "    # (already covered by above)\n",
    "\n",
    "# Make a mapping table so you can inspect\n",
    "mapping_table = pd.DataFrame({\n",
    "    \"original\": orig_cols,\n",
    "    \"normalized\": norm_cols\n",
    "}).sort_values([\"normalized\",\"original\"])\n",
    "\n",
    "print(f\"Column name mapping (original -> normalized)  {mapping_table.head(200)}\")\n",
    "\n",
    "# Now map requested base names to selected original columns using normalized keys\n",
    "selected_originals: List[str] = []\n",
    "selection_reason: List[Tuple[str,str,str]] = []  # (base, original, reason)\n",
    "for base in base_features:\n",
    "    n = normalize_name(base)\n",
    "    if n in pref:\n",
    "        selected_originals.append(pref[n])\n",
    "        selection_reason.append((base, pref[n], \"matched_normalized\"))\n",
    "    else:\n",
    "        selection_reason.append((base, \"\", \"not_found\"))\n",
    "\n",
    "selection_df = pd.DataFrame(selection_reason, columns=[\"base_name\",\"selected_original\",\"reason\"])\n",
    "print(f\"Requested base names -> selected original column  {selection_df} \")\n",
    "\n",
    "\n",
    "\n",
    "# Proceed with tests if we found any selected columns\n",
    "if len(selected_originals) == 0:\n",
    "    print(\"None of the requested base names matched available columns after normalization.\")\n",
    "else:\n",
    "    # ensure binary groups\n",
    "    g = df[GROUP_COL]\n",
    "    if g.dtype.kind not in \"biu\":\n",
    "        try:\n",
    "            g = g.astype(int)\n",
    "        except Exception:\n",
    "            uniq = sorted(g.dropna().unique().tolist())\n",
    "            if len(uniq) != 2:\n",
    "                raise ValueError(f\"{GROUP_COL} must have 2 groups, found: {uniq}\")\n",
    "            g = g.map({uniq[0]:0, uniq[1]:1})\n",
    "    df[GROUP_COL] = g\n",
    "    uniq_g = sorted(df[GROUP_COL].dropna().unique().tolist())\n",
    "    g0, g1 = uniq_g[0], uniq_g[1]\n",
    "\n",
    "    # T-tests\n",
    "    rows = []\n",
    "    for col in selected_originals:\n",
    "        x = df.loc[df[GROUP_COL]==g0, col].astype(float).dropna().values\n",
    "        y = df.loc[df[GROUP_COL]==g1, col].astype(float).dropna().values\n",
    "        t, p = welch_ttest(x, y)\n",
    "        d = cohens_d(x, y)\n",
    "        rows.append({\n",
    "            \"feature_original\": col,\n",
    "            \"feature_normalized\": normalize_name(col),\n",
    "            f\"n_{g0}\": len(x), f\"mean_{g0}\": float(np.mean(x)) if len(x) else np.nan, f\"sd_{g0}\": float(np.std(x, ddof=1)) if len(x) else np.nan,\n",
    "            f\"n_{g1}\": len(y), f\"mean_{g1}\": float(np.mean(y)) if len(y) else np.nan, f\"sd_{g1}\": float(np.std(y, ddof=1)) if len(y) else np.nan,\n",
    "            \"t_stat\": t, \"p_value\": p, \"cohens_d\": d\n",
    "        })\n",
    "    ttest_df = pd.DataFrame(rows).sort_values(\"p_value\", na_position=\"last\")\n",
    "    rej, p_adj, crit = fdr_bh(ttest_df[\"p_value\"].values)\n",
    "    ttest_df[\"p_value_fdr_bh\"] = p_adj\n",
    "    ttest_df[\"significant_fdr_0.05\"] = rej\n",
    "\n",
    "    # Correlations\n",
    "    corr_rows = []\n",
    "    gg = df[GROUP_COL].astype(float)\n",
    "    for col in selected_originals:\n",
    "        xx = df[col].astype(float)\n",
    "        m = gg.notna() & xx.notna()\n",
    "        if m.sum() < 3:\n",
    "            r, p = (np.nan, np.nan)\n",
    "        else:\n",
    "            r, p = stats.pearsonr(gg[m], xx[m])\n",
    "        corr_rows.append({\"feature_original\": col, \"feature_normalized\": normalize_name(col), \"r_pointbiserial\": r, \"p_value\": p})\n",
    "    corr_df = pd.DataFrame(corr_rows).sort_values(\"p_value\", na_position=\"last\")\n",
    "    rej_c, p_adj_c, crit_c = fdr_bh(corr_df[\"p_value\"].values)\n",
    "    corr_df[\"p_value_fdr_bh\"] = p_adj_c\n",
    "    corr_df[\"significant_fdr_0.05\"] = rej_c\n",
    "\n",
    "    # Save and display\n",
    "    t_out = Path(\"ttest_yeo/ttest_cortical_lr.csv\")\n",
    "    c_out = Path(\"ttest_yeo/corr_cortical_lr.csv\")\n",
    "    ttest_df.to_csv(t_out, index=False)\n",
    "    corr_df.to_csv(c_out, index=False)\n",
    "    mapping_table.to_csv(map_out, index=False)\n",
    "\n",
    "    print(f\"T-tests (normalized mapping) {ttest_df}\")\n",
    "    print(f\"Correlations (normalized mapping) {corr_df}\")\n",
    "\n",
    "    print({\n",
    "        \"downloads\": {\n",
    "            \"ttest_results_cortical_lr_csv\": str(t_out),\n",
    "            \"corr_results__cortical_lrcsv\": str(c_out),\n",
    "        },\n",
    "        \"tested_feature_count\": len(selected_originals)\n",
    "    })\n"
   ],
   "id": "d850c5ff0d135948",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column name mapping (original -> normalized)                        original                  normalized\n",
      "56                 bankssts_lh                 bankssts_lh\n",
      "57                 bankssts_rh                 bankssts_rh\n",
      "2                   Brain-Stem                  brain_stem\n",
      "58  caudalanteriorcingulate_lh  caudalanteriorcingulate_lh\n",
      "59  caudalanteriorcingulate_rh  caudalanteriorcingulate_rh\n",
      "..                         ...                         ...\n",
      "51                      VAN_GM                      van_gm\n",
      "52                      VIS_GM                      vis_gm\n",
      "55               WhiteSurfArea               whitesurfarea\n",
      "53            WhiteSurfArea_lh            whitesurfarea_lh\n",
      "54            WhiteSurfArea_rh            whitesurfarea_rh\n",
      "\n",
      "[130 rows x 2 columns]\n",
      "Requested base names -> selected original column                       base_name           selected_original              reason\n",
      "0                  bankssts_lh                 bankssts_lh  matched_normalized\n",
      "1                  bankssts_rh                 bankssts_rh  matched_normalized\n",
      "2   caudalanteriorcingulate_lh  caudalanteriorcingulate_lh  matched_normalized\n",
      "3   caudalanteriorcingulate_rh  caudalanteriorcingulate_rh  matched_normalized\n",
      "4       caudalmiddlefrontal_lh      caudalmiddlefrontal_lh  matched_normalized\n",
      "..                         ...                         ...                 ...\n",
      "63            supramarginal_rh            supramarginal_rh  matched_normalized\n",
      "64             temporalpole_lh             temporalpole_lh  matched_normalized\n",
      "65             temporalpole_rh             temporalpole_rh  matched_normalized\n",
      "66       transversetemporal_lh       transversetemporal_lh  matched_normalized\n",
      "67       transversetemporal_rh       transversetemporal_rh  matched_normalized\n",
      "\n",
      "[68 rows x 3 columns] \n",
      "T-tests (normalized mapping)            feature_original       feature_normalized  n_0.0      mean_0.0  \\\n",
      "19                Insula_rh                insula_rh     51   7015.960784   \n",
      "20      isthmuscingulate_lh      isthmuscingulate_lh     51   2692.372549   \n",
      "66    transversetemporal_lh    transversetemporal_lh     51   1284.509804   \n",
      "67    transversetemporal_rh    transversetemporal_rh     51    958.392157   \n",
      "33           paracentral_rh           paracentral_rh     51   3971.941176   \n",
      "..                      ...                      ...    ...           ...   \n",
      "1               bankssts_rh              bankssts_rh     51   2383.784314   \n",
      "46    posteriorcingulate_lh    posteriorcingulate_lh     51   3175.411765   \n",
      "39         parsorbitalis_rh         parsorbitalis_rh     51   3044.156863   \n",
      "30        middletemporal_lh        middletemporal_lh     51  11768.647059   \n",
      "54  rostralmiddlefrontal_lh  rostralmiddlefrontal_lh     51  16000.980392   \n",
      "\n",
      "         sd_0.0  n_1.0      mean_1.0       sd_1.0    t_stat   p_value  \\\n",
      "19   603.981091     64   6727.359375   594.270858  2.563863  0.011745   \n",
      "20   373.748095     64   2543.281250   342.182513  2.205800  0.029626   \n",
      "66   204.012487     64   1207.046875   197.532190  2.051488  0.042690   \n",
      "67   140.721012     64    908.140625   120.716889  2.024727  0.045592   \n",
      "33   600.168190     64   3767.312500   481.340980  1.979800  0.050630   \n",
      "..          ...    ...           ...          ...       ...       ...   \n",
      "1    416.884795     64   2390.250000   352.091664 -0.088441  0.929707   \n",
      "46   530.253757     64   3170.296875   419.183510  0.056284  0.955236   \n",
      "39   409.919425     64   3041.484375   305.883990  0.038749  0.969176   \n",
      "30  1373.959327     64  11765.046875  1253.957930  0.014507  0.988453   \n",
      "54  1803.007471     64  16002.343750  1858.535472 -0.003974  0.996837   \n",
      "\n",
      "    cohens_d  p_value_fdr_bh  significant_fdr_0.05  \n",
      "19  0.482138        0.548387                 False  \n",
      "20  0.418215        0.548387                 False  \n",
      "66  0.386493        0.548387                 False  \n",
      "67  0.386703        0.548387                 False  \n",
      "33  0.380937        0.548387                 False  \n",
      "..       ...             ...                   ...  \n",
      "1  -0.016921        0.987814                 False  \n",
      "46  0.010847        0.996837                 False  \n",
      "39  0.007514        0.996837                 False  \n",
      "30  0.002752        0.996837                 False  \n",
      "54 -0.000743        0.996837                 False  \n",
      "\n",
      "[68 rows x 13 columns]\n",
      "Correlations (normalized mapping)            feature_original       feature_normalized  r_pointbiserial  \\\n",
      "19                Insula_rh                insula_rh        -0.234875   \n",
      "20      isthmuscingulate_lh      isthmuscingulate_lh        -0.205140   \n",
      "67    transversetemporal_rh    transversetemporal_rh        -0.190265   \n",
      "66    transversetemporal_lh    transversetemporal_lh        -0.190165   \n",
      "33           paracentral_rh           paracentral_rh        -0.187528   \n",
      "..                      ...                      ...              ...   \n",
      "1               bankssts_rh              bankssts_rh         0.008480   \n",
      "46    posteriorcingulate_lh    posteriorcingulate_lh        -0.005436   \n",
      "39         parsorbitalis_rh         parsorbitalis_rh        -0.003766   \n",
      "30        middletemporal_lh        middletemporal_lh        -0.001379   \n",
      "54  rostralmiddlefrontal_lh  rostralmiddlefrontal_lh         0.000373   \n",
      "\n",
      "     p_value  p_value_fdr_bh  significant_fdr_0.05  \n",
      "19  0.011515        0.492476                 False  \n",
      "20  0.027857        0.492476                 False  \n",
      "67  0.041679        0.492476                 False  \n",
      "66  0.041788        0.492476                 False  \n",
      "33  0.044762        0.492476                 False  \n",
      "..       ...             ...                   ...  \n",
      "1   0.928331        0.986352                 False  \n",
      "46  0.954022        0.996847                 False  \n",
      "39  0.968141        0.996847                 False  \n",
      "30  0.988330        0.996847                 False  \n",
      "54  0.996847        0.996847                 False  \n",
      "\n",
      "[68 rows x 6 columns]\n",
      "{'downloads': {'ttest_results_cortical_lr_csv': 'ttest_yeo\\\\ttest_cortical_lr.csv', 'corr_results__cortical_lrcsv': 'ttest_yeo\\\\corr_cortical_lr.csv'}, 'tested_feature_count': 68}\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T10:05:48.149755Z",
     "start_time": "2025-11-10T10:05:48.073706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Let's diagnose why your columns weren't being matched and fix it.\n",
    "# We will:\n",
    "# 1) Load the merged CSV\n",
    "# 2) Build a normalization that turns names like \"Left-Accumbens-area_ses1\" into \"left_accumbens_area\"\n",
    "#    (lowercase, replace '-' and spaces with '_', collapse repeats, drop _ses1/_ses2)\n",
    "# 3) Show a mapping of original -> normalized so you can see what's happening\n",
    "# 4) Provide a selector that matches your requested base names against these normalized names\n",
    "# 5) Re-run the subcortical t-tests/correlations using the robust mapping\n",
    "\n",
    "\n",
    "DATA_FILE = Path(\"ttest_yeo/bigfile_pruned_merged.csv\")\n",
    "GROUP_COL = \"kmeans_label\"\n",
    "\n",
    "# Your base names (underscored, no session suffix)\n",
    "base_features= [\n",
    "    \"cingulate_lh\",\n",
    "    \"cingulate_rh\",\n",
    "    \"frontal_lh\",\n",
    "    \"frontal_rh\",\n",
    "    \"insula_lh\",\n",
    "    \"insula_rh\",\n",
    "    \"occipital_lh\",\n",
    "    \"occipital_rh\",\n",
    "    \"parietal_lh\",\n",
    "    \"parietal_rh\",\n",
    "    \"temporal_lh\",\n",
    "    \"temporal_rh\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "\n",
    "if GROUP_COL not in df.columns:\n",
    "    raise ValueError(f\"{GROUP_COL} not in file.\")\n",
    "\n",
    "# Build original->normalized and normalized->list(originals) maps\n",
    "orig_cols = df.columns.tolist()\n",
    "norm_cols = [normalize_name(c) for c in orig_cols]\n",
    "\n",
    "map_orig_to_norm: Dict[str, str] = dict(zip(orig_cols, norm_cols))\n",
    "\n",
    "# Build a \"best original\" picker for each normalized name:\n",
    "# - prefer originals ending with _ses1\n",
    "# - else originals ending with _ses2\n",
    "# - else the first seen\n",
    "pref: Dict[str, str] = {}\n",
    "for orig, norm in zip(orig_cols, norm_cols):\n",
    "    if norm not in pref:\n",
    "        pref[norm] = orig\n",
    "    # prefer *_ses1\n",
    "    if re.search(r\"_ses1$\", normalize_name(orig)):\n",
    "        pref[norm] = orig\n",
    "    # if current not ses1 and new is ses1, replace; if current neither and new is ses2, consider only if no ses1 seen\n",
    "    # (already covered by above)\n",
    "\n",
    "# Make a mapping table so you can inspect\n",
    "mapping_table = pd.DataFrame({\n",
    "    \"original\": orig_cols,\n",
    "    \"normalized\": norm_cols\n",
    "}).sort_values([\"normalized\",\"original\"])\n",
    "\n",
    "print(f\"Column name mapping (original -> normalized)  {mapping_table.head(200)}\")\n",
    "\n",
    "# Now map requested base names to selected original columns using normalized keys\n",
    "selected_originals: List[str] = []\n",
    "selection_reason: List[Tuple[str,str,str]] = []  # (base, original, reason)\n",
    "for base in base_features:\n",
    "    n = normalize_name(base)\n",
    "    if n in pref:\n",
    "        selected_originals.append(pref[n])\n",
    "        selection_reason.append((base, pref[n], \"matched_normalized\"))\n",
    "    else:\n",
    "        selection_reason.append((base, \"\", \"not_found\"))\n",
    "\n",
    "selection_df = pd.DataFrame(selection_reason, columns=[\"base_name\",\"selected_original\",\"reason\"])\n",
    "print(f\"Requested base names -> selected original column  {selection_df} \")\n",
    "\n",
    "\n",
    "\n",
    "# Proceed with tests if we found any selected columns\n",
    "if len(selected_originals) == 0:\n",
    "    print(\"None of the requested base names matched available columns after normalization.\")\n",
    "else:\n",
    "    # ensure binary groups\n",
    "    g = df[GROUP_COL]\n",
    "    if g.dtype.kind not in \"biu\":\n",
    "        try:\n",
    "            g = g.astype(int)\n",
    "        except Exception:\n",
    "            uniq = sorted(g.dropna().unique().tolist())\n",
    "            if len(uniq) != 2:\n",
    "                raise ValueError(f\"{GROUP_COL} must have 2 groups, found: {uniq}\")\n",
    "            g = g.map({uniq[0]:0, uniq[1]:1})\n",
    "    df[GROUP_COL] = g\n",
    "    uniq_g = sorted(df[GROUP_COL].dropna().unique().tolist())\n",
    "    g0, g1 = uniq_g[0], uniq_g[1]\n",
    "\n",
    "    # T-tests\n",
    "    rows = []\n",
    "    for col in selected_originals:\n",
    "        x = df.loc[df[GROUP_COL]==g0, col].astype(float).dropna().values\n",
    "        y = df.loc[df[GROUP_COL]==g1, col].astype(float).dropna().values\n",
    "        t, p = welch_ttest(x, y)\n",
    "        d = cohens_d(x, y)\n",
    "        rows.append({\n",
    "            \"feature_original\": col,\n",
    "            \"feature_normalized\": normalize_name(col),\n",
    "            f\"n_{g0}\": len(x), f\"mean_{g0}\": float(np.mean(x)) if len(x) else np.nan, f\"sd_{g0}\": float(np.std(x, ddof=1)) if len(x) else np.nan,\n",
    "            f\"n_{g1}\": len(y), f\"mean_{g1}\": float(np.mean(y)) if len(y) else np.nan, f\"sd_{g1}\": float(np.std(y, ddof=1)) if len(y) else np.nan,\n",
    "            \"t_stat\": t, \"p_value\": p, \"cohens_d\": d\n",
    "        })\n",
    "    ttest_df = pd.DataFrame(rows).sort_values(\"p_value\", na_position=\"last\")\n",
    "    rej, p_adj, crit = fdr_bh(ttest_df[\"p_value\"].values)\n",
    "    ttest_df[\"p_value_fdr_bh\"] = p_adj\n",
    "    ttest_df[\"significant_fdr_0.05\"] = rej\n",
    "\n",
    "    # Correlations\n",
    "    corr_rows = []\n",
    "    gg = df[GROUP_COL].astype(float)\n",
    "    for col in selected_originals:\n",
    "        xx = df[col].astype(float)\n",
    "        m = gg.notna() & xx.notna()\n",
    "        if m.sum() < 3:\n",
    "            r, p = (np.nan, np.nan)\n",
    "        else:\n",
    "            r, p = stats.pearsonr(gg[m], xx[m])\n",
    "        corr_rows.append({\"feature_normalized\": normalize_name(col), \"r_pointbiserial\": r, \"p_value\": p})\n",
    "    corr_df = pd.DataFrame(corr_rows).sort_values(\"p_value\", na_position=\"last\")\n",
    "    rej_c, p_adj_c, crit_c = fdr_bh(corr_df[\"p_value\"].values)\n",
    "    corr_df[\"p_value_fdr_bh\"] = p_adj_c\n",
    "    corr_df[\"significant_fdr_0.05\"] = rej_c\n",
    "\n",
    "    # Save and display\n",
    "    t_out = Path(\"ttest_yeo/ttest_lobes_lr.csv\")\n",
    "    c_out = Path(\"ttest_yeo/corr_lobes_lr.csv\")\n",
    "    ttest_df.to_csv(t_out, index=False)\n",
    "    corr_df.to_csv(c_out, index=False)\n",
    "    mapping_table.to_csv(map_out, index=False)\n",
    "\n",
    "    print(f\"T-tests (normalized mapping) {ttest_df}\")\n",
    "    print(f\"Correlations (normalized mapping) {corr_df}\")\n",
    "\n",
    "    print({\n",
    "        \"downloads\": {\n",
    "            \"ttest_results_lobes_lr.csv\": str(t_out),\n",
    "            \"corr_results__lobes_lr.csv\": str(c_out),\n",
    "        },\n",
    "        \"tested_feature_count\": len(selected_originals)\n",
    "    })\n"
   ],
   "id": "8ea7e776d349553b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column name mapping (original -> normalized)                        original                  normalized\n",
      "56                 bankssts_lh                 bankssts_lh\n",
      "57                 bankssts_rh                 bankssts_rh\n",
      "2                   Brain-Stem                  brain_stem\n",
      "58  caudalanteriorcingulate_lh  caudalanteriorcingulate_lh\n",
      "59  caudalanteriorcingulate_rh  caudalanteriorcingulate_rh\n",
      "..                         ...                         ...\n",
      "51                      VAN_GM                      van_gm\n",
      "52                      VIS_GM                      vis_gm\n",
      "55               WhiteSurfArea               whitesurfarea\n",
      "53            WhiteSurfArea_lh            whitesurfarea_lh\n",
      "54            WhiteSurfArea_rh            whitesurfarea_rh\n",
      "\n",
      "[130 rows x 2 columns]\n",
      "Requested base names -> selected original column         base_name selected_original              reason\n",
      "0   cingulate_lh      Cingulate_lh  matched_normalized\n",
      "1   cingulate_rh      Cingulate_rh  matched_normalized\n",
      "2     frontal_lh        Frontal_lh  matched_normalized\n",
      "3     frontal_rh        Frontal_rh  matched_normalized\n",
      "4      insula_lh         Insula_lh  matched_normalized\n",
      "5      insula_rh         Insula_rh  matched_normalized\n",
      "6   occipital_lh      Occipital_lh  matched_normalized\n",
      "7   occipital_rh      Occipital_rh  matched_normalized\n",
      "8    parietal_lh       Parietal_lh  matched_normalized\n",
      "9    parietal_rh       Parietal_rh  matched_normalized\n",
      "10   temporal_lh       Temporal_lh  matched_normalized\n",
      "11   temporal_rh       Temporal_rh  matched_normalized \n",
      "T-tests (normalized mapping)    feature_original feature_normalized  n_0.0      mean_0.0       sd_0.0  \\\n",
      "5         Insula_rh          insula_rh     51   7015.960784   603.981091   \n",
      "3        Frontal_rh         frontal_rh     51  88204.254902  8279.863513   \n",
      "8       Parietal_lh        parietal_lh     51  58513.921569  6027.766143   \n",
      "4         Insula_lh          insula_lh     51   7187.725490   647.617359   \n",
      "2        Frontal_lh         frontal_lh     51  90191.274510  8391.142502   \n",
      "9       Parietal_rh        parietal_rh     51  59500.039216  5889.335375   \n",
      "0      Cingulate_lh       cingulate_lh     51  10524.901961  1571.343187   \n",
      "7      Occipital_rh       occipital_rh     51  26761.901961  3192.712729   \n",
      "10      Temporal_lh        temporal_lh     51  57909.117647  4964.806585   \n",
      "1      Cingulate_rh       cingulate_rh     51   9903.176471  1377.751323   \n",
      "11      Temporal_rh        temporal_rh     51  55784.039216  4602.721247   \n",
      "6      Occipital_lh       occipital_lh     51  25257.137255  2701.026390   \n",
      "\n",
      "    n_1.0      mean_1.0       sd_1.0    t_stat   p_value  cohens_d  \\\n",
      "5      64   6727.359375   594.270858  2.563863  0.011745  0.482138   \n",
      "3      64  85770.187500  6739.788214  1.698370  0.092691  0.326258   \n",
      "8      64  56840.640625  5719.896676  1.512664  0.133377  0.285635   \n",
      "4      64   7004.718750   752.440357  1.400715  0.164057  0.258493   \n",
      "2      64  88234.109375  6817.849682  1.348355  0.180734  0.259072   \n",
      "9      64  58273.953125  5905.559741  1.107771  0.270434  0.207868   \n",
      "0      64  10226.828125  1238.337820  1.107976  0.270713  0.213593   \n",
      "7      64  26179.578125  2705.107326  1.038857  0.301425  0.198685   \n",
      "10     64  56991.703125  4467.814535  1.028781  0.306022  0.195435   \n",
      "1      64   9735.328125  1137.140609  0.700438  0.485340  0.134350   \n",
      "11     64  55223.250000  4207.540533  0.674130  0.501744  0.127836   \n",
      "6      64  24927.796875  2507.822328  0.670425  0.504080  0.126909   \n",
      "\n",
      "    p_value_fdr_bh  significant_fdr_0.05  \n",
      "5         0.140943                 False  \n",
      "3         0.408030                 False  \n",
      "8         0.408030                 False  \n",
      "4         0.408030                 False  \n",
      "2         0.408030                 False  \n",
      "9         0.408030                 False  \n",
      "0         0.408030                 False  \n",
      "7         0.408030                 False  \n",
      "10        0.408030                 False  \n",
      "1         0.504080                 False  \n",
      "11        0.504080                 False  \n",
      "6         0.504080                 False  \n",
      "Correlations (normalized mapping)    feature_normalized  r_pointbiserial   p_value  p_value_fdr_bh  \\\n",
      "5           insula_rh        -0.234875  0.011515        0.138174   \n",
      "3          frontal_rh        -0.161369  0.084908        0.400015   \n",
      "8         parietal_lh        -0.141707  0.130871        0.400015   \n",
      "2          frontal_lh        -0.128759  0.170245        0.400015   \n",
      "4           insula_lh        -0.128476  0.171194        0.400015   \n",
      "0        cingulate_lh        -0.106439  0.257559        0.400015   \n",
      "9         parietal_rh        -0.103617  0.270462        0.400015   \n",
      "7        occipital_rh        -0.099086  0.292084        0.400015   \n",
      "10        temporal_lh        -0.097480  0.300011        0.400015   \n",
      "1        cingulate_rh        -0.067180  0.475619        0.500350   \n",
      "11        temporal_rh        -0.063937  0.497233        0.500350   \n",
      "6        occipital_lh        -0.063475  0.500350        0.500350   \n",
      "\n",
      "    significant_fdr_0.05  \n",
      "5                  False  \n",
      "3                  False  \n",
      "8                  False  \n",
      "2                  False  \n",
      "4                  False  \n",
      "0                  False  \n",
      "9                  False  \n",
      "7                  False  \n",
      "10                 False  \n",
      "1                  False  \n",
      "11                 False  \n",
      "6                  False  \n",
      "{'downloads': {'ttest_results_lobes_lr.csv': 'ttest_yeo\\\\ttest_lobes_lr.csv', 'corr_results__lobes_lr.csv': 'ttest_yeo\\\\corr_lobes_lr.csv'}, 'tested_feature_count': 12}\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T10:05:48.268948Z",
     "start_time": "2025-11-10T10:05:48.198538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Let's diagnose why your columns weren't being matched and fix it.\n",
    "# We will:\n",
    "# 1) Load the merged CSV\n",
    "# 2) Build a normalization that turns names like \"Left-Accumbens-area_ses1\" into \"left_accumbens_area\"\n",
    "#    (lowercase, replace '-' and spaces with '_', collapse repeats, drop _ses1/_ses2)\n",
    "# 3) Show a mapping of original -> normalized so you can see what's happening\n",
    "# 4) Provide a selector that matches your requested base names against these normalized names\n",
    "# 5) Re-run the subcortical t-tests/correlations using the robust mapping\n",
    "\n",
    "\n",
    "DATA_FILE = Path(\"ttest_yeo/bigfile_pruned_merged.csv\")\n",
    "GROUP_COL = \"kmeans_label\"\n",
    "\n",
    "# Your base names (underscored, no session suffix)\n",
    "base_features= [\n",
    "    \"cingulate\",\n",
    "    \"frontal\",\n",
    "    \"insula\",\n",
    "    \"occipital\",\n",
    "    \"parietal\",\n",
    "    \"temporal\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "\n",
    "if GROUP_COL not in df.columns:\n",
    "    raise ValueError(f\"{GROUP_COL} not in file.\")\n",
    "\n",
    "# Build original->normalized and normalized->list(originals) maps\n",
    "orig_cols = df.columns.tolist()\n",
    "norm_cols = [normalize_name(c) for c in orig_cols]\n",
    "\n",
    "map_orig_to_norm: Dict[str, str] = dict(zip(orig_cols, norm_cols))\n",
    "\n",
    "# Build a \"best original\" picker for each normalized name:\n",
    "# - prefer originals ending with _ses1\n",
    "# - else originals ending with _ses2\n",
    "# - else the first seen\n",
    "pref: Dict[str, str] = {}\n",
    "for orig, norm in zip(orig_cols, norm_cols):\n",
    "    if norm not in pref:\n",
    "        pref[norm] = orig\n",
    "    # prefer *_ses1\n",
    "    if re.search(r\"_ses1$\", normalize_name(orig)):\n",
    "        pref[norm] = orig\n",
    "    # if current not ses1 and new is ses1, replace; if current neither and new is ses2, consider only if no ses1 seen\n",
    "    # (already covered by above)\n",
    "\n",
    "# Make a mapping table so you can inspect\n",
    "mapping_table = pd.DataFrame({\n",
    "    \"original\": orig_cols,\n",
    "    \"normalized\": norm_cols\n",
    "}).sort_values([\"normalized\",\"original\"])\n",
    "\n",
    "print(f\"Column name mapping (original -> normalized)  {mapping_table.head(200)}\")\n",
    "\n",
    "# Now map requested base names to selected original columns using normalized keys\n",
    "selected_originals: List[str] = []\n",
    "selection_reason: List[Tuple[str,str,str]] = []  # (base, original, reason)\n",
    "for base in base_features:\n",
    "    n = normalize_name(base)\n",
    "    if n in pref:\n",
    "        selected_originals.append(pref[n])\n",
    "        selection_reason.append((base, pref[n], \"matched_normalized\"))\n",
    "    else:\n",
    "        selection_reason.append((base, \"\", \"not_found\"))\n",
    "\n",
    "selection_df = pd.DataFrame(selection_reason, columns=[\"base_name\",\"selected_original\",\"reason\"])\n",
    "print(f\"Requested base names -> selected original column  {selection_df} \")\n",
    "\n",
    "\n",
    "\n",
    "# Proceed with tests if we found any selected columns\n",
    "if len(selected_originals) == 0:\n",
    "    print(\"None of the requested base names matched available columns after normalization.\")\n",
    "else:\n",
    "    # ensure binary groups\n",
    "    g = df[GROUP_COL]\n",
    "    if g.dtype.kind not in \"biu\":\n",
    "        try:\n",
    "            g = g.astype(int)\n",
    "        except Exception:\n",
    "            uniq = sorted(g.dropna().unique().tolist())\n",
    "            if len(uniq) != 2:\n",
    "                raise ValueError(f\"{GROUP_COL} must have 2 groups, found: {uniq}\")\n",
    "            g = g.map({uniq[0]:0, uniq[1]:1})\n",
    "    df[GROUP_COL] = g\n",
    "    uniq_g = sorted(df[GROUP_COL].dropna().unique().tolist())\n",
    "    g0, g1 = uniq_g[0], uniq_g[1]\n",
    "\n",
    "    # T-tests\n",
    "    rows = []\n",
    "    for col in selected_originals:\n",
    "        x = df.loc[df[GROUP_COL]==g0, col].astype(float).dropna().values\n",
    "        y = df.loc[df[GROUP_COL]==g1, col].astype(float).dropna().values\n",
    "        t, p = welch_ttest(x, y)\n",
    "        d = cohens_d(x, y)\n",
    "        rows.append({\n",
    "            \"feature_original\": col,\n",
    "            \"feature_normalized\": normalize_name(col),\n",
    "            f\"n_{g0}\": len(x), f\"mean_{g0}\": float(np.mean(x)) if len(x) else np.nan, f\"sd_{g0}\": float(np.std(x, ddof=1)) if len(x) else np.nan,\n",
    "            f\"n_{g1}\": len(y), f\"mean_{g1}\": float(np.mean(y)) if len(y) else np.nan, f\"sd_{g1}\": float(np.std(y, ddof=1)) if len(y) else np.nan,\n",
    "            \"t_stat\": t, \"p_value\": p, \"cohens_d\": d\n",
    "        })\n",
    "    ttest_df = pd.DataFrame(rows).sort_values(\"p_value\", na_position=\"last\")\n",
    "    rej, p_adj, crit = fdr_bh(ttest_df[\"p_value\"].values)\n",
    "    ttest_df[\"p_value_fdr_bh\"] = p_adj\n",
    "    ttest_df[\"significant_fdr_0.05\"] = rej\n",
    "\n",
    "    # Correlations\n",
    "    corr_rows = []\n",
    "    gg = df[GROUP_COL].astype(float)\n",
    "    for col in selected_originals:\n",
    "        xx = df[col].astype(float)\n",
    "        m = gg.notna() & xx.notna()\n",
    "        if m.sum() < 3:\n",
    "            r, p = (np.nan, np.nan)\n",
    "        else:\n",
    "            r, p = stats.pearsonr(gg[m], xx[m])\n",
    "        corr_rows.append({\"feature_normalized\": normalize_name(col), \"r_pointbiserial\": r, \"p_value\": p})\n",
    "    corr_df = pd.DataFrame(corr_rows).sort_values(\"p_value\", na_position=\"last\")\n",
    "    rej_c, p_adj_c, crit_c = fdr_bh(corr_df[\"p_value\"].values)\n",
    "    corr_df[\"p_value_fdr_bh\"] = p_adj_c\n",
    "    corr_df[\"significant_fdr_0.05\"] = rej_c\n",
    "\n",
    "    # Save and display\n",
    "    t_out = Path(\"ttest_yeo/ttest_lobes_full_brain.csv\")\n",
    "    c_out = Path(\"ttest_yeo/corr_lobes_full_brain.csv\")\n",
    "    ttest_df.to_csv(t_out, index=False)\n",
    "    corr_df.to_csv(c_out, index=False)\n",
    "    mapping_table.to_csv(map_out, index=False)\n",
    "\n",
    "    print(f\"T-tests (normalized mapping) {ttest_df}\")\n",
    "    print(f\"Correlations (normalized mapping) {corr_df}\")\n",
    "\n",
    "    print({\n",
    "        \"downloads\": {\n",
    "            \"ttest_results_lobes_full_brain.csv\": str(t_out),\n",
    "            \"corr_results__lobes_full_brain.csv\": str(c_out),\n",
    "        },\n",
    "        \"tested_feature_count\": len(selected_originals)\n",
    "    })\n"
   ],
   "id": "3e036bb56b77a726",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column name mapping (original -> normalized)                        original                  normalized\n",
      "56                 bankssts_lh                 bankssts_lh\n",
      "57                 bankssts_rh                 bankssts_rh\n",
      "2                   Brain-Stem                  brain_stem\n",
      "58  caudalanteriorcingulate_lh  caudalanteriorcingulate_lh\n",
      "59  caudalanteriorcingulate_rh  caudalanteriorcingulate_rh\n",
      "..                         ...                         ...\n",
      "51                      VAN_GM                      van_gm\n",
      "52                      VIS_GM                      vis_gm\n",
      "55               WhiteSurfArea               whitesurfarea\n",
      "53            WhiteSurfArea_lh            whitesurfarea_lh\n",
      "54            WhiteSurfArea_rh            whitesurfarea_rh\n",
      "\n",
      "[130 rows x 2 columns]\n",
      "Requested base names -> selected original column     base_name selected_original              reason\n",
      "0  cingulate         Cingulate  matched_normalized\n",
      "1    frontal           Frontal  matched_normalized\n",
      "2     insula            Insula  matched_normalized\n",
      "3  occipital         Occipital  matched_normalized\n",
      "4   parietal          Parietal  matched_normalized\n",
      "5   temporal          Temporal  matched_normalized \n",
      "T-tests (normalized mapping)   feature_original feature_normalized  n_0.0       mean_0.0        sd_0.0  \\\n",
      "2           Insula             insula     51   14203.686275   1195.996396   \n",
      "1          Frontal            frontal     51  178395.529412  16441.619120   \n",
      "4         Parietal           parietal     51  118013.960784  11581.948870   \n",
      "0        Cingulate          cingulate     51   20428.078431   2718.570778   \n",
      "3        Occipital          occipital     51   52019.039216   5655.567434   \n",
      "5         Temporal           temporal     51  113693.156863   9435.940100   \n",
      "\n",
      "   n_1.0       mean_1.0        sd_1.0    t_stat   p_value  cohens_d  \\\n",
      "2     64   13732.078125   1293.961903  2.025567  0.045219  0.376816   \n",
      "1     64  174004.296875  13393.027337  1.542614  0.126227  0.296314   \n",
      "4     64  115114.593750  11424.847955  1.341708  0.182539  0.252237   \n",
      "0     64   19962.156250   2079.770914  1.010729  0.314806  0.195467   \n",
      "3     64   51107.375000   5005.967041  0.903247  0.368549  0.171908   \n",
      "5     64  112214.953125   8507.282399  0.871545  0.385507  0.165530   \n",
      "\n",
      "   p_value_fdr_bh  significant_fdr_0.05  \n",
      "2        0.271316                 False  \n",
      "1        0.365079                 False  \n",
      "4        0.365079                 False  \n",
      "0        0.385507                 False  \n",
      "3        0.385507                 False  \n",
      "5        0.385507                 False  \n",
      "Correlations (normalized mapping)   feature_normalized  r_pointbiserial   p_value  p_value_fdr_bh  \\\n",
      "2             insula        -0.185570  0.047083        0.282497   \n",
      "1            frontal        -0.146893  0.117218        0.351654   \n",
      "4           parietal        -0.125416  0.181705        0.363411   \n",
      "0          cingulate        -0.097496  0.299933        0.379720   \n",
      "3          occipital        -0.085837  0.361700        0.379720   \n",
      "5           temporal        -0.082675  0.379720        0.379720   \n",
      "\n",
      "   significant_fdr_0.05  \n",
      "2                 False  \n",
      "1                 False  \n",
      "4                 False  \n",
      "0                 False  \n",
      "3                 False  \n",
      "5                 False  \n",
      "{'downloads': {'ttest_results_lobes_full_brain.csv': 'ttest_yeo\\\\ttest_lobes_full_brain.csv', 'corr_results__lobes_full_brain.csv': 'ttest_yeo\\\\corr_lobes_full_brain.csv'}, 'tested_feature_count': 6}\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T10:06:13.284061Z",
     "start_time": "2025-11-10T10:06:13.206770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Let's diagnose why your columns weren't being matched and fix it.\n",
    "# We will:\n",
    "# 1) Load the merged CSV\n",
    "# 2) Build a normalization that turns names like \"Left-Accumbens-area_ses1\" into \"left_accumbens_area\"\n",
    "#    (lowercase, replace '-' and spaces with '_', collapse repeats, drop _ses1/_ses2)\n",
    "# 3) Show a mapping of original -> normalized so you can see what's happening\n",
    "# 4) Provide a selector that matches your requested base names against these normalized names\n",
    "# 5) Re-run the subcortical t-tests/correlations using the robust mapping\n",
    "\n",
    "\n",
    "DATA_FILE = Path(\"ttest_yeo/bigfile_pruned_merged.csv\")\n",
    "GROUP_COL = \"kmeans_label\"\n",
    "\n",
    "# Your base names (underscored, no session suffix)\n",
    "base_features= [\n",
    "    \"DA_GM\",\n",
    "    \"VAN_GM\",\n",
    "    \"VIS_GM\",\n",
    "    \"FPN_GM\",\n",
    "    \"SMN_GM\",\n",
    "    \"DMN_GM\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "\n",
    "if GROUP_COL not in df.columns:\n",
    "    raise ValueError(f\"{GROUP_COL} not in file.\")\n",
    "\n",
    "# Build original->normalized and normalized->list(originals) maps\n",
    "orig_cols = df.columns.tolist()\n",
    "norm_cols = [normalize_name(c) for c in orig_cols]\n",
    "\n",
    "map_orig_to_norm: Dict[str, str] = dict(zip(orig_cols, norm_cols))\n",
    "\n",
    "# Build a \"best original\" picker for each normalized name:\n",
    "# - prefer originals ending with _ses1\n",
    "# - else originals ending with _ses2\n",
    "# - else the first seen\n",
    "pref: Dict[str, str] = {}\n",
    "for orig, norm in zip(orig_cols, norm_cols):\n",
    "    if norm not in pref:\n",
    "        pref[norm] = orig\n",
    "    # prefer *_ses1\n",
    "    if re.search(r\"_ses1$\", normalize_name(orig)):\n",
    "        pref[norm] = orig\n",
    "    # if current not ses1 and new is ses1, replace; if current neither and new is ses2, consider only if no ses1 seen\n",
    "    # (already covered by above)\n",
    "\n",
    "# Make a mapping table so you can inspect\n",
    "mapping_table = pd.DataFrame({\n",
    "    \"original\": orig_cols,\n",
    "    \"normalized\": norm_cols\n",
    "}).sort_values([\"normalized\",\"original\"])\n",
    "\n",
    "print(f\"Column name mapping (original -> normalized)  {mapping_table.head(200)}\")\n",
    "\n",
    "# Now map requested base names to selected original columns using normalized keys\n",
    "selected_originals: List[str] = []\n",
    "selection_reason: List[Tuple[str,str,str]] = []  # (base, original, reason)\n",
    "for base in base_features:\n",
    "    n = normalize_name(base)\n",
    "    if n in pref:\n",
    "        selected_originals.append(pref[n])\n",
    "        selection_reason.append((base, pref[n], \"matched_normalized\"))\n",
    "    else:\n",
    "        selection_reason.append((base, \"\", \"not_found\"))\n",
    "\n",
    "selection_df = pd.DataFrame(selection_reason, columns=[\"base_name\",\"selected_original\",\"reason\"])\n",
    "print(f\"Requested base names -> selected original column  {selection_df} \")\n",
    "\n",
    "\n",
    "\n",
    "# Proceed with tests if we found any selected columns\n",
    "if len(selected_originals) == 0:\n",
    "    print(\"None of the requested base names matched available columns after normalization.\")\n",
    "else:\n",
    "    # ensure binary groups\n",
    "    g = df[GROUP_COL]\n",
    "    if g.dtype.kind not in \"biu\":\n",
    "        try:\n",
    "            g = g.astype(int)\n",
    "        except Exception:\n",
    "            uniq = sorted(g.dropna().unique().tolist())\n",
    "            if len(uniq) != 2:\n",
    "                raise ValueError(f\"{GROUP_COL} must have 2 groups, found: {uniq}\")\n",
    "            g = g.map({uniq[0]:0, uniq[1]:1})\n",
    "    df[GROUP_COL] = g\n",
    "    uniq_g = sorted(df[GROUP_COL].dropna().unique().tolist())\n",
    "    g0, g1 = uniq_g[0], uniq_g[1]\n",
    "\n",
    "    # T-tests\n",
    "    rows = []\n",
    "    for col in selected_originals:\n",
    "        x = df.loc[df[GROUP_COL]==g0, col].astype(float).dropna().values\n",
    "        y = df.loc[df[GROUP_COL]==g1, col].astype(float).dropna().values\n",
    "        t, p = welch_ttest(x, y)\n",
    "        d = cohens_d(x, y)\n",
    "        rows.append({\n",
    "            \"feature_original\": col,\n",
    "            \"feature_normalized\": normalize_name(col),\n",
    "            f\"n_{g0}\": len(x), f\"mean_{g0}\": float(np.mean(x)) if len(x) else np.nan, f\"sd_{g0}\": float(np.std(x, ddof=1)) if len(x) else np.nan,\n",
    "            f\"n_{g1}\": len(y), f\"mean_{g1}\": float(np.mean(y)) if len(y) else np.nan, f\"sd_{g1}\": float(np.std(y, ddof=1)) if len(y) else np.nan,\n",
    "            \"t_stat\": t, \"p_value\": p, \"cohens_d\": d\n",
    "        })\n",
    "    ttest_df = pd.DataFrame(rows).sort_values(\"p_value\", na_position=\"last\")\n",
    "    rej, p_adj, crit = fdr_bh(ttest_df[\"p_value\"].values)\n",
    "    ttest_df[\"p_value_fdr_bh\"] = p_adj\n",
    "    ttest_df[\"significant_fdr_0.05\"] = rej\n",
    "\n",
    "    # Correlations\n",
    "    corr_rows = []\n",
    "    gg = df[GROUP_COL].astype(float)\n",
    "    for col in selected_originals:\n",
    "        xx = df[col].astype(float)\n",
    "        m = gg.notna() & xx.notna()\n",
    "        if m.sum() < 3:\n",
    "            r, p = (np.nan, np.nan)\n",
    "        else:\n",
    "            r, p = stats.pearsonr(gg[m], xx[m])\n",
    "        corr_rows.append({\"feature_normalized\": normalize_name(col), \"r_pointbiserial\": r, \"p_value\": p})\n",
    "    corr_df = pd.DataFrame(corr_rows).sort_values(\"p_value\", na_position=\"last\")\n",
    "    rej_c, p_adj_c, crit_c = fdr_bh(corr_df[\"p_value\"].values)\n",
    "    corr_df[\"p_value_fdr_bh\"] = p_adj_c\n",
    "    corr_df[\"significant_fdr_0.05\"] = rej_c\n",
    "\n",
    "    # Save and display\n",
    "    t_out = Path(\"ttest_yeo/ttest_lobes_full_brain.csv\")\n",
    "    c_out = Path(\"ttest_yeo/corr_lobes_full_brain.csv\")\n",
    "    ttest_df.to_csv(t_out, index=False)\n",
    "    corr_df.to_csv(c_out, index=False)\n",
    "    mapping_table.to_csv(map_out, index=False)\n",
    "\n",
    "    print(f\"T-tests (normalized mapping) {ttest_df}\")\n",
    "    print(f\"Correlations (normalized mapping) {corr_df}\")\n",
    "\n",
    "    print({\n",
    "        \"downloads\": {\n",
    "            \"ttest_results_networks_full_brain.csv\": str(t_out),\n",
    "            \"corr_results_networks_full_brain.csv\": str(c_out),\n",
    "        },\n",
    "        \"tested_feature_count\": len(selected_originals)\n",
    "    })\n"
   ],
   "id": "2b7ea345b921f7db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column name mapping (original -> normalized)                        original                  normalized\n",
      "56                 bankssts_lh                 bankssts_lh\n",
      "57                 bankssts_rh                 bankssts_rh\n",
      "2                   Brain-Stem                  brain_stem\n",
      "58  caudalanteriorcingulate_lh  caudalanteriorcingulate_lh\n",
      "59  caudalanteriorcingulate_rh  caudalanteriorcingulate_rh\n",
      "..                         ...                         ...\n",
      "51                      VAN_GM                      van_gm\n",
      "52                      VIS_GM                      vis_gm\n",
      "55               WhiteSurfArea               whitesurfarea\n",
      "53            WhiteSurfArea_lh            whitesurfarea_lh\n",
      "54            WhiteSurfArea_rh            whitesurfarea_rh\n",
      "\n",
      "[130 rows x 2 columns]\n",
      "Requested base names -> selected original column    base_name selected_original              reason\n",
      "0     DA_GM                             not_found\n",
      "1    VAN_GM            VAN_GM  matched_normalized\n",
      "2    VIS_GM            VIS_GM  matched_normalized\n",
      "3    FPN_GM            FPN_GM  matched_normalized\n",
      "4    SMN_GM            SMN_GM  matched_normalized\n",
      "5    DMN_GM            DMN_GM  matched_normalized \n",
      "T-tests (normalized mapping)   feature_original feature_normalized  n_0.0       mean_0.0        sd_0.0  \\\n",
      "3           SMN_GM             smn_gm     51   82047.862745   8053.320180   \n",
      "4           DMN_GM             dmn_gm     51  151428.098039  13329.231578   \n",
      "1           VIS_GM             vis_gm     51   72302.843137   6963.725797   \n",
      "2           FPN_GM             fpn_gm     51   44296.705882   4802.728942   \n",
      "0           VAN_GM             van_gm     51   57950.294118   5959.293875   \n",
      "\n",
      "   n_1.0       mean_1.0        sd_1.0    t_stat   p_value  cohens_d  \\\n",
      "3     64   79484.015625   6972.913211  1.798849  0.075077  0.343207   \n",
      "4     64  148075.328125  11785.815147  1.410010  0.161618  0.268387   \n",
      "1     64   70777.187500   6044.360591  1.236778  0.219078  0.235903   \n",
      "2     64   43389.140625   4196.799154  1.064060  0.289864  0.202806   \n",
      "0     64   56936.750000   5011.584716  0.971346  0.333777  0.185928   \n",
      "\n",
      "   p_value_fdr_bh  significant_fdr_0.05  \n",
      "3        0.333777                 False  \n",
      "4        0.333777                 False  \n",
      "1        0.333777                 False  \n",
      "2        0.333777                 False  \n",
      "0        0.333777                 False  \n",
      "Correlations (normalized mapping)   feature_normalized  r_pointbiserial   p_value  p_value_fdr_bh  \\\n",
      "3             smn_gm        -0.169516  0.070120        0.324029   \n",
      "4             dmn_gm        -0.133308  0.155521        0.324029   \n",
      "1             vis_gm        -0.117410  0.211425        0.324029   \n",
      "2             fpn_gm        -0.101120  0.282240        0.324029   \n",
      "0             van_gm        -0.092780  0.324029        0.324029   \n",
      "\n",
      "   significant_fdr_0.05  \n",
      "3                 False  \n",
      "4                 False  \n",
      "1                 False  \n",
      "2                 False  \n",
      "0                 False  \n",
      "{'downloads': {'ttest_results_networks_full_brain.csv': 'ttest_yeo\\\\ttest_lobes_full_brain.csv', 'corr_results_networks_full_brain.csv': 'ttest_yeo\\\\corr_lobes_full_brain.csv'}, 'tested_feature_count': 5}\n"
     ]
    }
   ],
   "execution_count": 63
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
